{"cells":[{"cell_type":"markdown","metadata":{"id":"lLqJy6EZtrZB"},"source":["### Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3487,"status":"ok","timestamp":1701927288392,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"},"user_tz":420},"id":"7-ewruGCyaCn","outputId":"1e15d27f-409b-44a1-df60-be34c5a73254"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"yLRvBRGgt5Zk"},"source":["#### Install Pytorch"]},{"cell_type":"code","source":["import torch\n","\n","TORCH = torch.__version__.split('+')[0]\n","CUDA = 'cu' + torch.version.cuda.replace('.', '')\n","\n","# Construct the installation command\n","install_command = f\"pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\"\n","\n","# Execute the command\n","!{install_command}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YMqZ2Vdu_how","executionInfo":{"status":"ok","timestamp":1702164457137,"user_tz":420,"elapsed":12781,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}},"outputId":"0a4701d3-8d1c-4a66-9833-09483a0352f0"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://data.pyg.org/whl/torch-2.1.0+cu118.html\n","Collecting pyg_lib\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/pyg_lib-0.3.1%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (2.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_scatter\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_scatter-2.1.2%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_sparse\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_sparse-0.6.18%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m122.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_cluster\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_cluster-1.6.3%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_spline_conv\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_spline_conv-1.2.2%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (887 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.8/887.8 kB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_sparse) (1.11.4)\n","Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch_sparse) (1.23.5)\n","Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n","Successfully installed pyg_lib-0.3.1+pt21cu118 torch_cluster-1.6.3+pt21cu118 torch_scatter-2.1.2+pt21cu118 torch_sparse-0.6.18+pt21cu118 torch_spline_conv-1.2.2+pt21cu118\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DQdlS70-zM8Y"},"outputs":[],"source":["import os\n","import torch\n","import numpy as np\n","\n","# Set a random seed for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""]},{"cell_type":"markdown","metadata":{"id":"zi7c7EMpsCBX"},"source":["### `ASLDatasetLoader` Class\n","\n","The `ASLDatasetLoader` class is designed for loading and processing the ASL dataset. Given a directory, it reads sign language data from JSON files and constructs graph representations suitable for graph-based neural networks. Crucially, the class converts JSON data into PyTorch Geometric (PyG) `Data` objects comprising `x` (node features), `edge_index` (graph connectivity), and `y' (labels) attributes.\n","\n","**Methods**:\n","\n","- `_create_sign_to_label_map`: Generates a mapping from sign names to unique labels.\n","\n","- `_read_file_data`: Reads data from a given JSON file.\n","\n","- `_augment_data`: Implements data augmentation by applying random rotation, translation, and scaling to landmarks, which can enhance the model's robustness.\n","\n","- `_create_graph_from_frame`: Constructs a PyG `Data` object from frame data, concentrating on hand and face landmarks. Edges are created between consecutive landmarks and between left and right hand landmarks. Additional features, like hand-to-face distances, are also computed.\n","\n","- `get_dataset`: Assembles the dataset, optionally incorporating data augmentation. The function outputs a list of PyG `Data` objects ready for graph neural network processing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LifBvD3D4t4C"},"outputs":[],"source":["import os\n","import torch\n","import json\n","import numpy as np\n","from torch.utils.data import Dataset\n","\n","class ASLInMemoryDataset(Dataset):\n","    def __init__(self, root, transform=None, pre_transform=None):\n","        self.root = root\n","        self.transform = transform\n","        self.pre_transform = pre_transform\n","        self.processed_dir = os.path.join(self.root, 'processed_2')  # New processed directory\n","        self.loader = ASLDatasetLoader(self.processed_dir)\n","\n","        # Check if processed data exists, if not, process and save the data\n","        if not os.path.exists(self.processed_dir) or not os.listdir(self.processed_dir):\n","            os.makedirs(self.processed_dir, exist_ok=True)\n","            self.data, self.labels = self.loader.load_data()\n","            torch.save((self.data, self.labels), os.path.join(self.processed_dir, 'processed_data_2.pt'))\n","        else:\n","            self.data, self.labels = torch.load(os.path.join(self.processed_dir, 'processed_data_2.pt'))\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        sample = self.data[idx]\n","        label = self.labels[idx]\n","\n","        if self.transform:\n","            sample = self.transform(sample)\n","\n","        return sample, label\n","\n","    def num_features(self):\n","        # Assumes all samples have the same shape\n","        return self.data[0].shape[1] * self.data[0].shape[2] if len(self.data) > 0 else 0\n","\n","    def num_classes(self):\n","        return len(set(self.labels))"]},{"cell_type":"code","source":["import os\n","import json\n","import numpy as np\n","import torch\n","\n","class ASLDatasetLoader:\n","    def __init__(self, directory_path):\n","        self.directory_path = directory_path\n","\n","    def load_data(self):\n","        data = []\n","        labels = []\n","        for filename in os.listdir(self.directory_path):\n","            file_path = os.path.join(self.directory_path, filename)\n","            with open(file_path, 'r') as file:\n","                content = json.load(file)\n","                for example in content[\"examples\"]:\n","                    for frame in example[\"frames\"]:\n","                        frame_data = self._process_frame_data(frame)\n","                        augmented_data = self._augment_data(frame_data)\n","                        data.append(augmented_data)\n","                        labels.append(content[\"sign\"])\n","\n","        return np.array(data), np.array(labels)\n","\n","    def _process_frame_data(self, frame):\n","        # Assuming each frame has landmarks data in a specific format\n","        landmarks = np.array([[landmark['x'], landmark['y']] for landmark in frame['landmarks']])\n","        return landmarks\n","\n","    def _augment_data(self, frame_data, rotation_range=10, translation_range=0.05, scaling_range=0.1, jittering_range=0.01, noise_scale=0.01, mirroring_prob=0.5):\n","        # Apply jittering\n","        jittering = np.random.uniform(-jittering_range, jittering_range, frame_data.shape)\n","        frame_data += jittering\n","\n","        # Apply random rotation\n","        theta = np.radians(np.random.uniform(-rotation_range, rotation_range))\n","        rotation_matrix = np.array([\n","            [np.cos(theta), -np.sin(theta)],\n","            [np.sin(theta),  np.cos(theta)]\n","        ])\n","        frame_data = np.dot(frame_data - frame_data.mean(axis=0), rotation_matrix)\n","\n","        # Apply random translation\n","        translation = np.random.uniform(-translation_range, translation_range, frame_data.shape[1])\n","        frame_data += translation\n","\n","        # Apply random scaling\n","        scaling_factor = np.random.uniform(1 - scaling_range, 1 + scaling_range)\n","        frame_data = frame_data * scaling_factor\n","\n","        # Apply noise injection\n","        noise = np.random.normal(0, noise_scale, frame_data.shape)\n","        frame_data += noise\n","\n","        # Apply mirroring with a probability\n","        if np.random.rand() < mirroring_prob:\n","            frame_data[:, 0] = -frame_data[:, 0]  # Mirroring on x-axis\n","\n","        return frame_data\n"],"metadata":{"id":"F2TjkBSGEs4f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xooW2sogtdL1"},"source":["### `ASLGraphClassifier` Class\n","\n","The `ASLGraphClassifier`, features deeper GCN layers and additional channels to capture intricate data patterns potentially. It takes a PyG `Data` object as input, and its forward pass emits class logits.\n","\n","**Methods**:\n","\n","- `forward`: Details the forward pass, accepting a PyG `Data` object. Two GCN layers with subsequent batch normalization and dropout layers process the input. Post global max-pooling, two linear layers coupled with dropout ensure final classification, leading to log-softmax outputs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RAslUK79VVV6"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Conv1DBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, dropout_rate):\n","        super(Conv1DBlock, self).__init__()\n","        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size//2)\n","        self.bn = nn.BatchNorm1d(out_channels)\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = F.relu(x)\n","        x = self.bn(x)\n","        x = self.dropout(x)\n","        return x\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, embed_dim, num_heads, dropout_rate, expand_ratio=2):\n","        super(TransformerBlock, self).__init__()\n","        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n","        self.linear1 = nn.Linear(embed_dim, embed_dim * expand_ratio)\n","        self.linear2 = nn.Linear(embed_dim * expand_ratio, embed_dim)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.norm1 = nn.LayerNorm(embed_dim)\n","        self.norm2 = nn.LayerNorm(embed_dim)\n","\n","    def forward(self, x):\n","        identity = x\n","        x, _ = self.attention(x, x, x)\n","        x = x + identity\n","        x = self.norm1(x)\n","\n","        identity = x\n","        x = F.relu(self.linear1(x))\n","        x = self.dropout(x)\n","        x = self.linear2(x)\n","        x = x + identity\n","        x = self.norm2(x)\n","\n","        return x\n","\n","class ASLClassifier(nn.Module):\n","    def __init__(self, max_len, num_channels, num_classes, dim=192):\n","        super(ASLClassifier, self).__init__()\n","        self.stem_conv = nn.Linear(max_len * num_channels, dim)\n","        self.stem_bn = nn.BatchNorm1d(dim)\n","\n","        # Convolutional and Transformer blocks\n","        self.layer1 = Conv1DBlock(dim, dim, kernel_size=17, dropout_rate=0.2)\n","        self.transformer1 = TransformerBlock(dim, num_heads=8, dropout_rate=0.2)\n","\n","        self.layer2 = Conv1DBlock(dim, dim, kernel_size=17, dropout_rate=0.2)\n","        self.transformer2 = TransformerBlock(dim, num_heads=8, dropout_rate=0.2)\n","\n","        # if dim == 384:  # For larger model size\n","        #     # Add additional Conv1D and Transformer layers as needed\n","\n","        self.top_conv = nn.Linear(dim, dim * 2)\n","        self.classifier = nn.Linear(dim * 2, num_classes)\n","        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n","        self.dropout = nn.Dropout(0.8)\n","\n","    def forward(self, x):\n","        x = x.view(x.size(0), -1)  # Flatten the input\n","        x = self.stem_conv(x)\n","        x = self.stem_bn(x)\n","        x = x.view(x.size(0), -1, x.size(1))  # Reshape for Conv1D\n","\n","        x = self.layer1(x)\n","        x = self.transformer1(x)\n","\n","        x = self.layer2(x)\n","        x = self.transformer2(x)\n","\n","        # if x.size(1) == 384:  # Adjust for larger model size\n","        #     # Apply additional layers if needed\n","\n","        x = self.top_conv(x)\n","        x = self.global_avg_pool(x).view(x.size(0), -1)  # Global average pooling\n","        x = self.dropout(x)\n","        x = self.classifier(x)\n","\n","        return F.log_softmax(x, dim=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RdBGsFveWcbF"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.utils.data import DataLoader\n","from torch.cuda.amp import GradScaler, autocast\n","\n","EPOCHS = 100\n","LEARNING_RATE = 0.0005\n","WEIGHT_DECAY = 5e-4\n","BATCH_SIZE = 7\n","GRADIENT_ACCUM_STEPS = 4  # For gradient accumulation\n","WORKERS = 2\n","\n","def stratified_data_split(dataset, test_size=0.2):\n","    labels = [label for _, label in dataset]\n","    train_data, test_data = train_test_split(dataset, test_size=test_size, stratify=labels, random_state=42)\n","    return train_data, test_data\n","\n","def validate(loader, model, device):\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","\n","    for data, labels in loader:\n","        data, labels = data.to(device), labels.to(device)\n","        with torch.no_grad():\n","            out = model(data)\n","        pred = out.argmax(dim=1)\n","        all_preds.append(pred.cpu())\n","        all_labels.append(labels.cpu())\n","\n","    all_preds = torch.cat(all_preds)\n","    all_labels = torch.cat(all_labels)\n","    accuracy = (all_preds == all_labels).float().mean().item()\n","\n","    metrics = {\n","        'accuracy': accuracy,\n","        'precision': precision_score(all_labels.numpy(), all_preds.numpy(), average='macro', zero_division=1),\n","        'recall': recall_score(all_labels.numpy(), all_preds.numpy(), average='macro', zero_division=1),\n","        'f1': f1_score(all_labels.numpy(), all_preds.numpy(), average='macro', zero_division=1)\n","    }\n","    return metrics\n","\n","\n","def train(train_data, val_data, num_classes, input_channels, epochs=100, learning_rate=0.0005, weight_decay=5e-4, patience=5):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = ASLClassifier(input_channels=input_channels, num_classes=num_classes).to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.7, patience=2, verbose=True)\n","    criterion = torch.nn.CrossEntropyLoss()\n","\n","    # Check if CUDA is available for mixed precision training\n","    use_amp = torch.cuda.is_available()\n","    scaler = GradScaler() if use_amp else None\n","\n","    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS)\n","    val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKERS)\n","\n","    best_val_accuracy = 0.0\n","    epochs_without_improvement = 0\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        total_loss = 0\n","        all_preds = []\n","        all_labels = []\n","\n","        for batch_idx, (data, labels) in enumerate(tqdm(train_loader, desc=\"Training\")):\n","            data, labels = data.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","\n","            with autocast(enabled=use_amp):\n","                outputs = model(data)\n","                loss = criterion(outputs, labels)\n","\n","            if use_amp:\n","                scaler.scale(loss).backward()\n","                if (batch_idx + 1) % GRADIENT_ACCUM_STEPS == 0:\n","                    scaler.step(optimizer)\n","                    scaler.update()\n","                    optimizer.zero_grad()\n","            else:\n","                loss.backward()\n","                if (batch_idx + 1) % GRADIENT_ACCUM_STEPS == 0:\n","                    optimizer.step()\n","                    optimizer.zero_grad()\n","\n","            total_loss += loss.item()\n","            preds = outputs.argmax(dim=1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","        avg_loss = total_loss / len(train_loader)\n","        train_accuracy = (np.array(all_preds) == np.array(all_labels)).mean()\n","\n","        val_metrics = validate(val_loader, model, device)\n","        scheduler.step(val_metrics['accuracy'])\n","\n","        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_metrics['accuracy']:.4f}\")\n","\n","        if val_metrics['accuracy'] > best_val_accuracy:\n","            best_val_accuracy = val_metrics['accuracy']\n","            epochs_without_improvement = 0\n","            # Save the model if needed\n","            torch.save(model.state_dict(), 'best_model.pth')\n","        else:\n","            epochs_without_improvement += 1\n","\n","        if epochs_without_improvement >= patience:\n","            print(\"Early stopping\")\n","            break\n","\n","    return model, all_preds, all_labels, train_accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EU2sza6GtOUS"},"outputs":[],"source":["directory_path = \"/content/drive/MyDrive/Colab Notebooks/DGMD E-14 Project/Datasets/processed-40-500\"\n","# Create an instance of the ASLInMemoryDataset\n","dataset = ASLInMemoryDataset(root=directory_path)"]},{"cell_type":"code","source":["# Split the dataset into training and validation subsets\n","train_data, val_data = stratified_data_split(dataset)"],"metadata":{"id":"I1Sg12EwrK8u","executionInfo":{"status":"error","timestamp":1701927557113,"user_tz":420,"elapsed":282,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}},"colab":{"base_uri":"https://localhost:8080/","height":344},"outputId":"797daa3c-7ecd-40ec-f8ae-3fe5b4f47ddc"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-63fcd78e2108>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Split the dataset into training and validation subsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstratified_data_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-30-c225b05d6e64>\u001b[0m in \u001b[0;36mstratified_data_split\u001b[0;34m(dataset, test_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstratified_data_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2563\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2236\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2237\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rr2PlwLy5M6H"},"outputs":[],"source":["# Train the model using the datasets\n","model, all_preds, all_labels, accuracy = train(train_data, val_data, dataset.num_classes(), dataset.num_features(), epochs=EPOCHS, learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"]},{"cell_type":"code","source":["dataset.sign_to_label()"],"metadata":{"id":"AlLngPoYqiDb"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sMrYkTs89Q7O"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix, classification_report\n","import numpy as np\n","\n","# Convert lists to numpy arrays for compatibility with sklearn\n","y_true = np.array(all_labels)\n","y_pred = np.array(all_preds)\n","\n","# Calculate the confusion matrix\n","cm = confusion_matrix(y_true, y_pred)\n","\n","# Visualize the confusion matrix\n","plt.figure(figsize=(10, 7))\n","sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n","plt.title(\"Confusion Matrix\")\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"True\")\n","plt.show()\n","\n","# Ensure the class names are in the correct order for target_names\n","ordered_class_names = [name for name, num in sorted((dataset.sign_to_label()).items(), key=lambda item: item[1])]\n","\n","# Per-Class Accuracy\n","class_accuracy = cm.diagonal() / cm.sum(axis=1)\n","for i, acc in enumerate(class_accuracy):\n","    class_name = ordered_class_names[i]\n","    print(f\"Accuracy for class {i} ({class_name}): {acc*100:.2f}%\")\n","\n","# Detailed classification report\n","print(\"\\nClassification Report:\\n\")\n","print(classification_report(y_true, y_pred, target_names=ordered_class_names, zero_division=1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hl5tQmtN9Syt"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","\n","def print_top_misclassified_classes(y_true, y_pred, sign_to_label, N=3, zero_division=1):\n","    \"\"\"\n","    Prints the top N classes that get misclassified the most.\n","\n","    Parameters:\n","    - y_true: Actual labels\n","    - y_pred: Predicted labels by the model\n","    - sign_to_label: Dictionary mapping class names to class numbers\n","    - N: Number of top misclassified classes to print\n","    - zero_division: Parameter for handling zero division in classification_report\n","\n","    Returns:\n","    None\n","    \"\"\"\n","\n","    # Ensure the class names are in the correct order for target_names\n","    ordered_class_names = [name for name, num in sorted(sign_to_label.items(), key=lambda item: item[1])]\n","\n","    # Generate and print classification report with class names\n","    print(\"\\nClassification Report:\\n\")\n","    print(classification_report(y_true, y_pred, target_names=ordered_class_names, zero_division=zero_division))\n","\n","    # Generate classification report as dict to find misclassified classes\n","    report = classification_report(y_true, y_pred, output_dict=True, zero_division=zero_division)\n","\n","    # Create a dictionary to store misclassification rates\n","    misclassification_rates = {}\n","\n","    # Iterate through each class in the report\n","    for class_num, metrics in report.items():\n","        if class_num.isdigit():\n","            class_name = [key for key, value in sign_to_label.items() if value == int(class_num)][0]\n","            misclassification_rates[class_name] = 1 - metrics['recall']\n","\n","    # Sort classes based on misclassification rate\n","    sorted_classes = sorted(misclassification_rates, key=misclassification_rates.get, reverse=True)\n","\n","    # Print top N misclassified classes\n","    print(f\"\\nTop {N} misclassified classes:\")\n","    for i in range(N):\n","        class_name = sorted_classes[i]\n","        print(f\"{i+1}. {class_name} - Misclassification rate: {misclassification_rates[class_name]:.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m4ZtqFxz9XlG"},"outputs":[],"source":["print_top_misclassified_classes(y_true, y_pred, dataset.sign_to_label(), N=10, zero_division=1)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1wIPH3B0fuuoSlPfXK5fGd9Em1LP-uQFJ","timestamp":1697014386160}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}