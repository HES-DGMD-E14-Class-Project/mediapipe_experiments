{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiSS4wuxZ01m",
        "outputId": "140232bf-7969-4dcd-83f2-5df0d29e6d5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n",
            "11.8\n",
            "Collecting torch-geometric-temporal\n",
            "  Downloading torch_geometric_temporal-0.54.0.tar.gz (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (4.4.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (2.0.1+cu118)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (3.0.2)\n",
            "Collecting pandas<=1.3.5 (from torch-geometric-temporal)\n",
            "  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse (from torch-geometric-temporal)\n",
            "  Using cached torch_sparse-0.6.17.tar.gz (209 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch_scatter (from torch-geometric-temporal)\n",
            "  Using cached torch_scatter-2.1.1.tar.gz (107 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch_geometric (from torch-geometric-temporal)\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (1.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.10/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2023.3.post1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torch-geometric-temporal) (3.27.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torch-geometric-temporal) (17.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (4.66.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (1.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torch-geometric-temporal) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (2023.7.22)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric->torch-geometric-temporal) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric->torch-geometric-temporal) (3.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torch-geometric-temporal) (1.3.0)\n",
            "Building wheels for collected packages: torch-geometric-temporal, torch_geometric, torch_scatter, torch_sparse\n",
            "  Building wheel for torch-geometric-temporal (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric-temporal: filename=torch_geometric_temporal-0.54.0-py3-none-any.whl size=86722 sha256=97af22abd717c2e9eefd4b19cc952aabd4cf0f8f74fed06034a1be134024c62b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/9b/b6/e15256e053f0cb49b1084a67a709db909d418386a231f0722c\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910454 sha256=fcd21c5641b699521a34756308e0eb1a367a790292debcb33e06a9a8bf3f01f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "  Building wheel for torch_scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_scatter: filename=torch_scatter-2.1.1-cp310-cp310-linux_x86_64.whl size=491866 sha256=2519bcaf8db2b366d850c13659da480a08d1081407b2941527df027e7390ec12\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/67/58/6566a3b61c6ec0f2ca0c2c324cd035ef2955601f0fb3197d5f\n",
            "  Building wheel for torch_sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_sparse: filename=torch_sparse-0.6.17-cp310-cp310-linux_x86_64.whl size=1053726 sha256=25a74b84023abc2b142367ebbdc5cefe034aa00a5b3dcc2ad57dea543346e681\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/25/e7/037b58fa47ba781444fd101a2f06c63a9d4e967ca6b910c53a\n",
            "Successfully built torch-geometric-temporal torch_geometric torch_scatter torch_sparse\n",
            "Installing collected packages: torch_scatter, torch_sparse, pandas, torch_geometric, torch-geometric-temporal\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 1.3.5 which is incompatible.\n",
            "plotnine 0.12.3 requires pandas>=1.5.0, but you have pandas 1.3.5 which is incompatible.\n",
            "xarray 2023.7.0 requires pandas>=1.4, but you have pandas 1.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-1.3.5 torch-geometric-temporal-0.54.0 torch_geometric-2.3.1 torch_scatter-2.1.1 torch_sparse-0.6.17\n"
          ]
        }
      ],
      "source": [
        "!python -c \"import torch; print(torch.__version__)\"\n",
        "!python -c \"import torch; print(torch.version.cuda)\"\n",
        "!pip install torch-geometric-temporal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric_temporal.nn.recurrent import A3TGCN2\n",
        "from torch_geometric_temporal.signal import temporal_signal_split\n",
        "\n",
        "# GPU support\n",
        "DEVICE = torch.device('cuda') # cuda\n",
        "shuffle=True\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "_9_1Hzq-pc6k"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from tqdm import tqdm\n",
        "except ImportError:\n",
        "    def tqdm(iterable):\n",
        "        return iterable\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric_temporal.nn.recurrent import TGCN\n",
        "\n",
        "from torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\n",
        "from torch_geometric_temporal.signal import temporal_signal_split\n",
        "\n",
        "loader = ChickenpoxDatasetLoader()\n",
        "\n",
        "dataset = loader.get_dataset()\n",
        "\n",
        "train_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n",
        "\n",
        "class RecurrentGCN(torch.nn.Module):\n",
        "    def __init__(self, node_features):\n",
        "        super(RecurrentGCN, self).__init__()\n",
        "        self.recurrent = TGCN(node_features, 32)\n",
        "        self.linear = torch.nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight, prev_hidden_state):\n",
        "        h = self.recurrent(x, edge_index, edge_weight, prev_hidden_state)\n",
        "        y = F.relu(h)\n",
        "        y = self.linear(y)\n",
        "        return y, h\n",
        "\n",
        "model = RecurrentGCN(node_features = 4)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in tqdm(range(50)):\n",
        "    cost = 0\n",
        "    hidden_state = None\n",
        "    for time, snapshot in enumerate(train_dataset):\n",
        "        y_hat, hidden_state = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr,hidden_state)\n",
        "        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n",
        "    cost = cost / (time+1)\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "model.eval()\n",
        "cost = 0\n",
        "hidden_state = None\n",
        "for time, snapshot in enumerate(test_dataset):\n",
        "    y_hat, hidden_state = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr, hidden_state)\n",
        "    cost = cost + torch.mean((y_hat-snapshot.y)**2)\n",
        "cost = cost / (time+1)\n",
        "cost = cost.item()\n",
        "print(\"MSE: {:.4f}\".format(cost))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdlYr4KnprOr",
        "outputId": "248581f7-ab5b-415d-891f-ed31ece959fc"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:28<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.9937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: mount google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-ewruGCyaCn",
        "outputId": "94d85b3d-a0d4-4fdf-f164-0fdf8c50cba3"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "class ASLDatasetLoader:\n",
        "    def __init__(self, directory_path):\n",
        "        self.directory_path = directory_path\n",
        "        self.sign_to_label = self._create_sign_to_label_map()\n",
        "\n",
        "    def _create_sign_to_label_map(self):\n",
        "        signs = [os.path.splitext(filename)[0] for filename in os.listdir(self.directory_path)]\n",
        "        return {sign: i for i, sign in enumerate(signs)}\n",
        "\n",
        "    def _read_file_data(self, file_path):\n",
        "        with open(file_path, 'r') as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    def _augment_data(self, frame_data, rotation_range=10, translation_range=0.05, scaling_range=0.1):\n",
        "        \"\"\"\n",
        "        Augment the frame data with random rotation, translation, and scaling.\n",
        "\n",
        "        :param frame_data: Dictionary containing frame landmarks and deltas.\n",
        "        :param rotation_range: Maximum rotation angle in degrees.\n",
        "        :param translation_range: Maximum translation as a fraction of landmark range.\n",
        "        :param scaling_range: Maximum scaling factor.\n",
        "        :return: Augmented frame data.\n",
        "        \"\"\"\n",
        "        landmarks = np.array(frame_data[\"landmarks\"])\n",
        "        centroid = np.mean(landmarks, axis=0)\n",
        "\n",
        "        # Random rotation\n",
        "        theta = np.radians(np.random.uniform(-rotation_range, rotation_range))\n",
        "        rotation_matrix = np.array([\n",
        "            [np.cos(theta), -np.sin(theta)],\n",
        "            [np.sin(theta), np.cos(theta)]\n",
        "        ])\n",
        "        landmarks = np.dot(landmarks - centroid, rotation_matrix) + centroid\n",
        "\n",
        "        # Random translation\n",
        "        max_translation = translation_range * (landmarks.max(axis=0) - landmarks.min(axis=0))\n",
        "        translations = np.random.uniform(-max_translation, max_translation)\n",
        "        landmarks += translations\n",
        "\n",
        "        # Random scaling\n",
        "        scale = np.random.uniform(1 - scaling_range, 1 + scaling_range)\n",
        "        landmarks = centroid + scale * (landmarks - centroid)\n",
        "\n",
        "        frame_data[\"landmarks\"] = landmarks.tolist()\n",
        "        return frame_data\n",
        "\n",
        "    def _create_graph_from_frame(self, sign_name, frame_data):\n",
        "        landmarks = np.array(frame_data[\"landmarks\"])\n",
        "        deltas = np.array(frame_data[\"deltas\"])\n",
        "\n",
        "        # Adjust lengths for concatenation\n",
        "        n_landmarks = len(landmarks)\n",
        "        landmarks = landmarks[:n_landmarks-1]\n",
        "        deltas = deltas[:n_landmarks-1]\n",
        "\n",
        "        # Create edges based on the number of available landmarks (or nodes)\n",
        "        edges = [[i, i+1] for i in range(len(landmarks) - 1)]\n",
        "\n",
        "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "        x = torch.tensor(np.hstack((landmarks, deltas)), dtype=torch.float)\n",
        "        y = torch.tensor([self.sign_to_label[sign_name]], dtype=torch.long)\n",
        "\n",
        "        return Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "\n",
        "    def get_dataset(self, augment=False):\n",
        "        dataset = []\n",
        "\n",
        "        for filename in os.listdir(self.directory_path):\n",
        "            sign_name = os.path.splitext(filename)[0]\n",
        "            file_path = os.path.join(self.directory_path, filename)\n",
        "            sign_data = self._read_file_data(file_path)\n",
        "\n",
        "            for frame_data in sign_data[\"frames\"]:\n",
        "                if augment:\n",
        "                  frame_data = self._augment_data(frame_data)\n",
        "                graph_data = self._create_graph_from_frame(sign_name, frame_data)\n",
        "\n",
        "                dataset.append(graph_data)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def number_of_classes(self):\n",
        "        return len(self.sign_to_label)"
      ],
      "metadata": {
        "id": "LifBvD3D4t4C"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GCNConv, global_max_pool, BatchNorm  # Notice the change in the import\n",
        "\n",
        "class GraphClassifier(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, num_classes, dropout_rate=0.5):\n",
        "        super(GraphClassifier, self).__init__()\n",
        "        self.conv1 = GCNConv(num_node_features, 128)\n",
        "        self.bn1 = BatchNorm(128)\n",
        "        self.dropout1 = torch.nn.Dropout(dropout_rate)  # Dropout after first layer\n",
        "        self.conv2 = GCNConv(128, 64)\n",
        "        self.bn2 = BatchNorm(64)\n",
        "        self.dropout2 = torch.nn.Dropout(dropout_rate)  # Dropout after second layer\n",
        "        self.fc = torch.nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        # First GCN layer\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.leaky_relu(x)  # Use LeakyReLU\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # Second GCN layer\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Global pooling across nodes\n",
        "        x = global_max_pool(x, data.batch)  # Here's the change from mean pooling to max pooling\n",
        "\n",
        "        # Final classification layer\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "XpmOnloG40DP"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def stratified_data_split(data_list, test_size=0.2):\n",
        "    # Extract labels from data list\n",
        "    labels = [data.y.item() for data in data_list]\n",
        "\n",
        "    # Use sklearn's train_test_split with stratify option\n",
        "    train_data, test_data = train_test_split(data_list, test_size=test_size, stratify=labels, random_state=42)\n",
        "\n",
        "    return train_data, test_data\n",
        "\n",
        "def train():\n",
        "    directory_path = \"/content/drive/MyDrive/Colab Notebooks/DGMD E-14 Project/Datasets/ASL\"\n",
        "    loader = ASLDatasetLoader(directory_path)\n",
        "\n",
        "    # Create the entire dataset without augmentation and then perform stratified split\n",
        "    data_list = loader.get_dataset()\n",
        "    train_dataset, test_dataset = stratified_data_split(data_list, test_size=0.2)\n",
        "\n",
        "    # Now augment only the training dataset\n",
        "    augmented_train_dataset = loader.get_dataset(augment=True)\n",
        "\n",
        "    num_classes = loader.number_of_classes()\n",
        "\n",
        "    train_labels = [data.y.item() for data in train_dataset]\n",
        "    test_labels = [data.y.item() for data in test_dataset]\n",
        "\n",
        "    print(\"Training label distribution:\", Counter(train_labels))\n",
        "    print(\"Test label distribution:\", Counter(test_labels))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = GraphClassifier(num_node_features=4, num_classes=num_classes).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=5e-4)\n",
        "\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(EPOCHS):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            batch = batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(batch)\n",
        "            loss = F.nll_loss(out, batch.y)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Check for NaN loss\n",
        "            if np.isnan(loss.item()):\n",
        "                print(\"Warning: NaN loss detected!\")\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch}, Loss: {avg_loss}\")\n",
        "\n",
        "        scheduler.step(avg_loss)\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in test_loader:\n",
        "        batch = batch.to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(batch).max(dim=1)[1]\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(batch.y.cpu().numpy())\n",
        "            correct += pred.eq(batch.y).sum().item()\n",
        "\n",
        "    print(f\"Accuracy: {correct / len(test_dataset)}\")\n",
        "    print(\"Sample predictions:\", all_preds[:20])\n",
        "    print(\"Sample true labels:\", all_labels[:20])"
      ],
      "metadata": {
        "id": "6QYi0OpiBuRK"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rr2PlwLy5M6H",
        "outputId": "c588e2a1-e418-4581-c5fe-ffac9a7dae4c"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training label distribution: Counter({0: 402, 12: 363, 10: 298, 18: 282, 3: 269, 5: 265, 6: 258, 17: 247, 14: 246, 15: 243, 16: 243, 9: 236, 2: 223, 8: 220, 4: 215, 7: 214, 13: 210, 19: 203, 1: 199, 11: 199})\n",
            "Test label distribution: Counter({0: 100, 12: 91, 10: 75, 18: 70, 3: 67, 5: 66, 6: 65, 17: 62, 14: 62, 15: 61, 16: 60, 9: 59, 2: 56, 8: 55, 4: 54, 7: 53, 13: 52, 19: 51, 11: 50, 1: 50})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 2.7679731951484197\n",
            "Epoch 1, Loss: 2.4197495210019846\n",
            "Epoch 2, Loss: 2.264846469782576\n",
            "Epoch 3, Loss: 2.16229141135759\n",
            "Epoch 4, Loss: 2.0803750685498685\n",
            "Epoch 5, Loss: 2.0108864858180664\n",
            "Epoch 6, Loss: 1.9526521118381355\n",
            "Epoch 7, Loss: 1.9114532915851739\n",
            "Epoch 8, Loss: 1.8804189178008068\n",
            "Epoch 9, Loss: 1.842461360406272\n",
            "Epoch 10, Loss: 1.8082927314541009\n",
            "Epoch 11, Loss: 1.773604042167905\n",
            "Epoch 12, Loss: 1.7454687243775477\n",
            "Epoch 13, Loss: 1.7187168243565136\n",
            "Epoch 14, Loss: 1.6867444349240652\n",
            "Epoch 15, Loss: 1.6601606690430943\n",
            "Epoch 16, Loss: 1.6658137987686108\n",
            "Epoch 17, Loss: 1.6550552641289145\n",
            "Epoch 18, Loss: 1.6048921678639665\n",
            "Epoch 19, Loss: 1.5817411828644667\n",
            "Epoch 20, Loss: 1.6001691780512846\n",
            "Epoch 21, Loss: 1.581323659872707\n",
            "Epoch 22, Loss: 1.5591718064078801\n",
            "Epoch 23, Loss: 1.5353141064885296\n",
            "Epoch 24, Loss: 1.5436589295351053\n",
            "Epoch 25, Loss: 1.5201885594597346\n",
            "Epoch 26, Loss: 1.5437640589249284\n",
            "Epoch 27, Loss: 1.5111538073684596\n",
            "Epoch 28, Loss: 1.5068559744690038\n",
            "Epoch 29, Loss: 1.5114291657375385\n",
            "Epoch 30, Loss: 1.5085199973251247\n",
            "Epoch 31, Loss: 1.5084351293648346\n",
            "Epoch 32, Loss: 1.4792693532720398\n",
            "Epoch 33, Loss: 1.5097508671917492\n",
            "Epoch 34, Loss: 1.4883954872058918\n",
            "Epoch 35, Loss: 1.4795260783992237\n",
            "Epoch 36, Loss: 1.4892710862280447\n",
            "Epoch 37, Loss: 1.4337687009497533\n",
            "Epoch 38, Loss: 1.457849843969828\n",
            "Epoch 39, Loss: 1.4574486579321608\n",
            "Epoch 40, Loss: 1.445049420942234\n",
            "Epoch 41, Loss: 1.4377293771580806\n",
            "Epoch 42, Loss: 1.460166975667205\n",
            "Epoch 43, Loss: 1.446081975215598\n",
            "Epoch 44, Loss: 1.4517331017723567\n",
            "Epoch 45, Loss: 1.4615561777277837\n",
            "Epoch 46, Loss: 1.4258601239210442\n",
            "Epoch 47, Loss: 1.4546379273450827\n",
            "Epoch 48, Loss: 1.4344049175328846\n",
            "Epoch 49, Loss: 1.4224283076539825\n",
            "Epoch 50, Loss: 1.4073843412761446\n",
            "Epoch 51, Loss: 1.434136998427065\n",
            "Epoch 52, Loss: 1.4370698574223095\n",
            "Epoch 53, Loss: 1.4117360975168929\n",
            "Epoch 54, Loss: 1.4340115246138996\n",
            "Epoch 55, Loss: 1.4276162219952933\n",
            "Epoch 56, Loss: 1.38222737025611\n",
            "Epoch 57, Loss: 1.4229990300498432\n",
            "Epoch 58, Loss: 1.4094163824485828\n",
            "Epoch 59, Loss: 1.43061269121834\n",
            "Epoch 60, Loss: 1.4272960763943345\n",
            "Epoch 61, Loss: 1.4019507743135284\n",
            "Epoch 62, Loss: 1.404385350927522\n",
            "Epoch 63, Loss: 1.4082671452926685\n",
            "Epoch 64, Loss: 1.4132282175595248\n",
            "Epoch 65, Loss: 1.4204789532136313\n",
            "Epoch 66, Loss: 1.3877825627598581\n",
            "Epoch 67, Loss: 1.3992520129378838\n",
            "Epoch 00068: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch 68, Loss: 1.3769095253340806\n",
            "Epoch 69, Loss: 1.392057862085632\n",
            "Epoch 70, Loss: 1.373160501446905\n",
            "Epoch 71, Loss: 1.3587746835207637\n",
            "Epoch 72, Loss: 1.3555200594135477\n",
            "Epoch 73, Loss: 1.345590344712704\n",
            "Epoch 74, Loss: 1.320888060934936\n",
            "Epoch 75, Loss: 1.3414861642861668\n",
            "Epoch 76, Loss: 1.3349021473262883\n",
            "Epoch 77, Loss: 1.351997223081468\n",
            "Epoch 78, Loss: 1.3546155064920835\n",
            "Epoch 79, Loss: 1.3153082871738868\n",
            "Epoch 80, Loss: 1.3516064852098875\n",
            "Epoch 81, Loss: 1.3250544471076773\n",
            "Epoch 82, Loss: 1.3599103123326846\n",
            "Epoch 83, Loss: 1.3480176318295394\n",
            "Epoch 84, Loss: 1.339942087101031\n",
            "Epoch 85, Loss: 1.3244081694114058\n",
            "Epoch 86, Loss: 1.3269209650498401\n",
            "Epoch 87, Loss: 1.3547829578194437\n",
            "Epoch 88, Loss: 1.3279923989048488\n",
            "Epoch 89, Loss: 1.3569347986692115\n",
            "Epoch 90, Loss: 1.369071128624904\n",
            "Epoch 00091: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch 91, Loss: 1.34178328174579\n",
            "Epoch 92, Loss: 1.352661450829687\n",
            "Epoch 93, Loss: 1.327056206857102\n",
            "Epoch 94, Loss: 1.3378790575492232\n",
            "Epoch 95, Loss: 1.362410448397262\n",
            "Epoch 96, Loss: 1.3810561125791525\n",
            "Epoch 97, Loss: 1.3337322926219506\n",
            "Epoch 98, Loss: 1.3317795392078688\n",
            "Epoch 99, Loss: 1.3140689062921307\n",
            "Accuracy: 0.7648927720413026\n",
            "Sample predictions: [4, 11, 7, 2, 12, 12, 11, 2, 13, 8, 12, 3, 2, 19, 0, 19, 12, 4, 0, 5]\n",
            "Sample true labels: [4, 11, 2, 2, 12, 12, 11, 11, 13, 12, 12, 17, 2, 19, 0, 17, 12, 6, 0, 5]\n"
          ]
        }
      ]
    }
  ]
}