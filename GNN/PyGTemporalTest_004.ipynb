{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiSS4wuxZ01m",
        "outputId": "c48e9145-17b7-437a-d5e4-99f2a5b95547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n",
            "11.8\n",
            "Collecting torch-geometric-temporal\n",
            "  Downloading torch_geometric_temporal-0.54.0.tar.gz (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (4.4.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (2.0.1+cu118)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (3.0.2)\n",
            "Collecting pandas<=1.3.5 (from torch-geometric-temporal)\n",
            "  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse (from torch-geometric-temporal)\n",
            "  Downloading torch_sparse-0.6.17.tar.gz (209 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch_scatter (from torch-geometric-temporal)\n",
            "  Downloading torch_scatter-2.1.1.tar.gz (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.6/107.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch_geometric (from torch-geometric-temporal)\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (1.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.10/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2023.3.post1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torch-geometric-temporal) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torch-geometric-temporal) (17.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (4.66.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (1.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torch-geometric-temporal) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (2023.7.22)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric->torch-geometric-temporal) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric->torch-geometric-temporal) (3.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torch-geometric-temporal) (1.3.0)\n",
            "Building wheels for collected packages: torch-geometric-temporal, torch_geometric, torch_scatter, torch_sparse\n",
            "  Building wheel for torch-geometric-temporal (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric-temporal: filename=torch_geometric_temporal-0.54.0-py3-none-any.whl size=86722 sha256=66dbca53e08bdaf22077873383a9ade9653c677d3305df62c26ac30fac4d951d\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/9b/b6/e15256e053f0cb49b1084a67a709db909d418386a231f0722c\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910454 sha256=986c22cd1d9bb8d178cb514611cb34c2a7b52cb3d420988cc822a2090955ad81\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "  Building wheel for torch_scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_scatter: filename=torch_scatter-2.1.1-cp310-cp310-linux_x86_64.whl size=491866 sha256=1d8be5e5ecb1f25d8344767f684234017944d406be9705b69385e171d9c172ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/67/58/6566a3b61c6ec0f2ca0c2c324cd035ef2955601f0fb3197d5f\n",
            "  Building wheel for torch_sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_sparse: filename=torch_sparse-0.6.17-cp310-cp310-linux_x86_64.whl size=1053726 sha256=cfb07c12426a62e04cf3a88dbd82bc23aee93a2a1eb20885e78c553f81b61803\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/25/e7/037b58fa47ba781444fd101a2f06c63a9d4e967ca6b910c53a\n",
            "Successfully built torch-geometric-temporal torch_geometric torch_scatter torch_sparse\n",
            "Installing collected packages: torch_scatter, torch_sparse, pandas, torch_geometric, torch-geometric-temporal\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 1.3.5 which is incompatible.\n",
            "plotnine 0.12.3 requires pandas>=1.5.0, but you have pandas 1.3.5 which is incompatible.\n",
            "xarray 2023.7.0 requires pandas>=1.4, but you have pandas 1.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-1.3.5 torch-geometric-temporal-0.54.0 torch_geometric-2.3.1 torch_scatter-2.1.1 torch_sparse-0.6.17\n"
          ]
        }
      ],
      "source": [
        "!python -c \"import torch; print(torch.__version__)\"\n",
        "!python -c \"import torch; print(torch.version.cuda)\"\n",
        "!pip install torch-geometric-temporal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: mount google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-ewruGCyaCn",
        "outputId": "c40d3574-aca7-43e0-f9c1-a8f23cf281c2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "\n",
        "class ASLDatasetLoader:\n",
        "    def __init__(self, directory_path):\n",
        "        self.directory_path = directory_path\n",
        "        self.sign_to_label = self._create_sign_to_label_map()\n",
        "\n",
        "    def _create_sign_to_label_map(self):\n",
        "        signs = [os.path.splitext(filename)[0] for filename in os.listdir(self.directory_path)]\n",
        "        return {sign: i for i, sign in enumerate(signs)}\n",
        "\n",
        "    def _read_file_data(self, file_path):\n",
        "        with open(file_path, 'r') as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    def _augment_data(self, frame_data, rotation_range=10, translation_range=0.05, scaling_range=0.1):\n",
        "        \"\"\"\n",
        "        Augment the frame data with random rotation, translation, and scaling.\n",
        "\n",
        "        :param frame_data: Dictionary containing frame landmarks and deltas.\n",
        "        :param rotation_range: Maximum rotation angle in degrees.\n",
        "        :param translation_range: Maximum translation as a fraction of landmark range.\n",
        "        :param scaling_range: Maximum scaling factor.\n",
        "        :return: Augmented frame data.\n",
        "        \"\"\"\n",
        "        landmarks = np.array(frame_data[\"landmarks\"])\n",
        "        centroid = np.mean(landmarks, axis=0)\n",
        "\n",
        "        # Random rotation\n",
        "        theta = np.radians(np.random.uniform(-rotation_range, rotation_range))\n",
        "        rotation_matrix = np.array([\n",
        "            [np.cos(theta), -np.sin(theta)],\n",
        "            [np.sin(theta), np.cos(theta)]\n",
        "        ])\n",
        "        landmarks = np.dot(landmarks - centroid, rotation_matrix) + centroid\n",
        "\n",
        "        # Random translation\n",
        "        max_translation = translation_range * (landmarks.max(axis=0) - landmarks.min(axis=0))\n",
        "        translations = np.random.uniform(-max_translation, max_translation)\n",
        "        landmarks += translations\n",
        "\n",
        "        # Random scaling\n",
        "        scale = np.random.uniform(1 - scaling_range, 1 + scaling_range)\n",
        "        landmarks = centroid + scale * (landmarks - centroid)\n",
        "\n",
        "        frame_data[\"landmarks\"] = landmarks.tolist()\n",
        "        return frame_data\n",
        "\n",
        "    def _create_graph_from_frame(self, sign_name, frame_data):\n",
        "        landmarks = np.array(frame_data[\"landmarks\"])\n",
        "        deltas = np.array(frame_data[\"deltas\"])\n",
        "\n",
        "        # Adjust lengths for concatenation\n",
        "        n_landmarks = len(landmarks)\n",
        "        landmarks = landmarks[:n_landmarks-1]\n",
        "        deltas = deltas[:n_landmarks-1]\n",
        "\n",
        "        # Create edges based on the number of available landmarks (or nodes)\n",
        "        edges = [[i, i+1] for i in range(len(landmarks) - 1)]\n",
        "\n",
        "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "        x = torch.tensor(np.hstack((landmarks, deltas)), dtype=torch.float)\n",
        "        y = torch.tensor([self.sign_to_label[sign_name]], dtype=torch.long)\n",
        "\n",
        "        return Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "\n",
        "    def get_dataset(self, augment=False):\n",
        "        dataset = []\n",
        "\n",
        "        for filename in os.listdir(self.directory_path):\n",
        "            sign_name = os.path.splitext(filename)[0]\n",
        "            file_path = os.path.join(self.directory_path, filename)\n",
        "            sign_data = self._read_file_data(file_path)\n",
        "\n",
        "            for frame_data in sign_data[\"frames\"]:\n",
        "                if augment:\n",
        "                  frame_data = self._augment_data(frame_data)\n",
        "                graph_data = self._create_graph_from_frame(sign_name, frame_data)\n",
        "\n",
        "                dataset.append(graph_data)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def number_of_classes(self):\n",
        "        return len(self.sign_to_label)"
      ],
      "metadata": {
        "id": "LifBvD3D4t4C"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GCNConv, global_max_pool, BatchNorm  # Notice the change in the import\n",
        "\n",
        "class GraphClassifier(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, num_classes, dropout_rate=0.5):\n",
        "        super(GraphClassifier, self).__init__()\n",
        "        self.conv1 = GCNConv(num_node_features, 128)\n",
        "        self.bn1 = BatchNorm(128)\n",
        "        self.dropout1 = torch.nn.Dropout(dropout_rate)  # Dropout after first layer\n",
        "        self.conv2 = GCNConv(128, 64)\n",
        "        self.bn2 = BatchNorm(64)\n",
        "        self.dropout2 = torch.nn.Dropout(dropout_rate)  # Dropout after second layer\n",
        "        self.fc = torch.nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        # First GCN layer\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.leaky_relu(x)  # Use LeakyReLU\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # Second GCN layer\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Global pooling across nodes\n",
        "        x = global_max_pool(x, data.batch)  # Here's the change from mean pooling to max pooling\n",
        "\n",
        "        # Final classification layer\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "XpmOnloG40DP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_max_pool, global_mean_pool\n",
        "\n",
        "class ExtendedGraphClassifier(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super(ExtendedGraphClassifier, self).__init__()\n",
        "        self.conv1 = GCNConv(num_features, 128)\n",
        "        self.conv2 = GCNConv(128, 256)\n",
        "        self.lin1 = torch.nn.Linear(256, 128)\n",
        "        self.lin2 = torch.nn.Linear(128, num_classes)\n",
        "        self.dropout = torch.nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = self.dropout(F.relu(self.conv1(x, edge_index)))\n",
        "        x = self.dropout(F.relu(self.conv2(x, edge_index)))\n",
        "        x = global_max_pool(x, batch)\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.lin2(x)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "metadata": {
        "id": "RAslUK79VVV6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch_geometric.loader import DataLoader\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "EPOCHS = 250\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "\n",
        "def stratified_data_split(data_list, test_size=0.2):\n",
        "    # Extract labels from data list\n",
        "    labels = [data.y.item() for data in data_list]\n",
        "\n",
        "    # Use sklearn's train_test_split with stratify option\n",
        "    train_data, test_data = train_test_split(data_list, test_size=test_size, stratify=labels, random_state=42)\n",
        "\n",
        "    return train_data, test_data\n",
        "\n",
        "\n",
        "def validate(loader, model, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        with torch.no_grad():\n",
        "            out = model(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += int((pred == data.y).sum())\n",
        "    return correct / len(loader.dataset)\n",
        "\n",
        "def train():\n",
        "    directory_path = \"/content/drive/MyDrive/Colab Notebooks/DGMD E-14 Project/Datasets/ASL\"\n",
        "    loader = ASLDatasetLoader(directory_path)\n",
        "\n",
        "    # Create the entire dataset without augmentation and then perform stratified split\n",
        "    data_list = loader.get_dataset()\n",
        "    train_dataset, test_dataset = stratified_data_split(data_list, test_size=0.2)\n",
        "\n",
        "    # Now augment only the training dataset\n",
        "    augmented_train_dataset = loader.get_dataset(augment=True)\n",
        "\n",
        "    num_classes = loader.number_of_classes()\n",
        "\n",
        "    train_labels = [data.y.item() for data in train_dataset]\n",
        "    test_labels = [data.y.item() for data in test_dataset]\n",
        "\n",
        "    print(\"Training label distribution:\", Counter(train_labels))\n",
        "    print(\"Test label distribution:\", Counter(test_labels))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = ExtendedGraphClassifier(num_features=4, num_classes=num_classes).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=5e-4)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.7, patience=5, verbose=True)\n",
        "\n",
        "    max_epochs_without_improvement = 20\n",
        "    epochs_without_improvement = 0\n",
        "    best_val_accuracy = 0\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(EPOCHS):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            batch = batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(batch)\n",
        "            loss = F.nll_loss(out, batch.y)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Check for NaN loss\n",
        "            if np.isnan(loss.item()):\n",
        "                print(\"Warning: NaN loss detected!\")\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch}, Loss: {avg_loss}\")\n",
        "\n",
        "        val_accuracy = validate(test_loader, model, device)\n",
        "        scheduler.step(val_accuracy)\n",
        "\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "\n",
        "        if epochs_without_improvement >= max_epochs_without_improvement:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in test_loader:\n",
        "        batch = batch.to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(batch).max(dim=1)[1]\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(batch.y.cpu().numpy())\n",
        "            correct += pred.eq(batch.y).sum().item()\n",
        "\n",
        "    print(f\"Accuracy: {correct / len(test_dataset)}\")\n",
        "    print(\"Sample predictions:\", all_preds[:20])\n",
        "    print(\"Sample true labels:\", all_labels[:20])"
      ],
      "metadata": {
        "id": "RdBGsFveWcbF"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rr2PlwLy5M6H",
        "outputId": "9f7a1eb2-fc6a-44af-f9b2-146618dcf76f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training label distribution: Counter({15: 402, 7: 363, 9: 298, 3: 282, 13: 269, 16: 265, 19: 258, 2: 247, 8: 246, 4: 243, 5: 243, 10: 236, 14: 223, 18: 220, 17: 215, 12: 214, 1: 210, 0: 203, 11: 199, 6: 199})\n",
            "Test label distribution: Counter({15: 100, 7: 91, 9: 75, 3: 70, 13: 67, 16: 66, 19: 65, 2: 62, 8: 62, 4: 61, 5: 60, 10: 59, 14: 56, 18: 55, 17: 54, 12: 53, 1: 52, 0: 51, 6: 50, 11: 50})\n",
            "Epoch 0, Loss: 2.9783951134621343\n",
            "Epoch 1, Loss: 2.7325360246851473\n",
            "Epoch 2, Loss: 2.5616929244391526\n",
            "Epoch 3, Loss: 2.5553990859019606\n",
            "Epoch 4, Loss: 2.530746081207372\n",
            "Epoch 5, Loss: 2.523849629148652\n",
            "Epoch 6, Loss: 2.511371390729011\n",
            "Epoch 7, Loss: 2.52093997484521\n",
            "Epoch 8, Loss: 2.5140357289133193\n",
            "Epoch 9, Loss: 2.4862496596348436\n",
            "Epoch 10, Loss: 2.482298280619368\n",
            "Epoch 11, Loss: 2.4806352280363253\n",
            "Epoch 12, Loss: 2.4762717953211144\n",
            "Epoch 13, Loss: 2.4677184608918203\n",
            "Epoch 14, Loss: 2.4751703890064096\n",
            "Epoch 00015: reducing learning rate of group 0 to 7.0000e-04.\n",
            "Epoch 15, Loss: 2.456407485128958\n",
            "Epoch 16, Loss: 2.4453239169301866\n",
            "Epoch 17, Loss: 2.438696298418166\n",
            "Epoch 18, Loss: 2.432797121096261\n",
            "Epoch 19, Loss: 2.4383116975615295\n",
            "Epoch 20, Loss: 2.4246965646743774\n",
            "Epoch 00021: reducing learning rate of group 0 to 4.9000e-04.\n",
            "Epoch 21, Loss: 2.419799837885024\n",
            "Epoch 22, Loss: 2.4186382391784766\n",
            "Epoch 23, Loss: 2.4114928502070754\n",
            "Epoch 24, Loss: 2.402425479285325\n",
            "Epoch 25, Loss: 2.4045787959159175\n",
            "Epoch 26, Loss: 2.3917934140072594\n",
            "Epoch 27, Loss: 2.3836515372312523\n",
            "Epoch 28, Loss: 2.3748800482931016\n",
            "Epoch 29, Loss: 2.3552892675882653\n",
            "Epoch 30, Loss: 2.329807974115203\n",
            "Epoch 31, Loss: 2.2907532046112835\n",
            "Epoch 32, Loss: 2.2683438153206548\n",
            "Epoch 33, Loss: 2.2549216603930993\n",
            "Epoch 34, Loss: 2.2394388174708886\n",
            "Epoch 35, Loss: 2.2253529278537894\n",
            "Epoch 36, Loss: 2.219677061974248\n",
            "Epoch 37, Loss: 2.2107799385167377\n",
            "Epoch 38, Loss: 2.2119563350194618\n",
            "Epoch 39, Loss: 2.2041166032416912\n",
            "Epoch 40, Loss: 2.201167306568049\n",
            "Epoch 41, Loss: 2.197786534888835\n",
            "Epoch 42, Loss: 2.1945344737813444\n",
            "Epoch 43, Loss: 2.1826090352444707\n",
            "Epoch 44, Loss: 2.181368857999391\n",
            "Epoch 45, Loss: 2.177445667453959\n",
            "Epoch 46, Loss: 2.1736228375495235\n",
            "Epoch 47, Loss: 2.170296201977549\n",
            "Epoch 48, Loss: 2.16904520384873\n",
            "Epoch 49, Loss: 2.16580028850821\n",
            "Epoch 50, Loss: 2.1659821883032593\n",
            "Epoch 51, Loss: 2.1592060327529907\n",
            "Epoch 52, Loss: 2.157475861567485\n",
            "Epoch 53, Loss: 2.1560631809355337\n",
            "Epoch 54, Loss: 2.137122814413867\n",
            "Epoch 55, Loss: 2.1405407544932786\n",
            "Epoch 56, Loss: 2.129964559893065\n",
            "Epoch 57, Loss: 2.127826271177847\n",
            "Epoch 58, Loss: 2.118164923372148\n",
            "Epoch 59, Loss: 2.1136421327349506\n",
            "Epoch 60, Loss: 2.10375454757787\n",
            "Epoch 61, Loss: 2.0923081598704374\n",
            "Epoch 62, Loss: 2.0813256230535386\n",
            "Epoch 63, Loss: 2.071601653400856\n",
            "Epoch 64, Loss: 2.0578643279739572\n",
            "Epoch 65, Loss: 2.0435627736622775\n",
            "Epoch 66, Loss: 2.027738548532317\n",
            "Epoch 67, Loss: 2.007218412206143\n",
            "Epoch 68, Loss: 1.983860580227043\n",
            "Epoch 69, Loss: 1.9673477222647848\n",
            "Epoch 70, Loss: 1.9496622553354577\n",
            "Epoch 71, Loss: 1.9323157170150853\n",
            "Epoch 72, Loss: 1.9183779215510888\n",
            "Epoch 73, Loss: 1.9019224258917797\n",
            "Epoch 74, Loss: 1.8866181645212294\n",
            "Epoch 75, Loss: 1.8680148486849628\n",
            "Epoch 76, Loss: 1.8496912981890425\n",
            "Epoch 77, Loss: 1.8417614600326442\n",
            "Epoch 78, Loss: 1.8290155115006845\n",
            "Epoch 79, Loss: 1.8224403609203388\n",
            "Epoch 80, Loss: 1.8076819121083127\n",
            "Epoch 81, Loss: 1.8011993652657619\n",
            "Epoch 82, Loss: 1.7971784593183784\n",
            "Epoch 83, Loss: 1.7853082478800906\n",
            "Epoch 84, Loss: 1.770729803586308\n",
            "Epoch 85, Loss: 1.76428768227372\n",
            "Epoch 86, Loss: 1.7499329560919652\n",
            "Epoch 87, Loss: 1.7383564582353905\n",
            "Epoch 88, Loss: 1.7414306206039236\n",
            "Epoch 89, Loss: 1.7264624932144261\n",
            "Epoch 90, Loss: 1.7161193791824052\n",
            "Epoch 91, Loss: 1.7046050074734265\n",
            "Epoch 92, Loss: 1.702603800387322\n",
            "Epoch 93, Loss: 1.6886507008649125\n",
            "Epoch 94, Loss: 1.6810834958583494\n",
            "Epoch 95, Loss: 1.6751800345469126\n",
            "Epoch 96, Loss: 1.667703087571301\n",
            "Epoch 97, Loss: 1.659184702589542\n",
            "Epoch 98, Loss: 1.6486192504061927\n",
            "Epoch 99, Loss: 1.6385784639587886\n",
            "Epoch 100, Loss: 1.632887680319291\n",
            "Epoch 101, Loss: 1.615586242343806\n",
            "Epoch 102, Loss: 1.6077887672412245\n",
            "Epoch 103, Loss: 1.5968160297297225\n",
            "Epoch 104, Loss: 1.5947201915934115\n",
            "Epoch 105, Loss: 1.582065012636064\n",
            "Epoch 106, Loss: 1.5760957802398294\n",
            "Epoch 107, Loss: 1.5673078600364396\n",
            "Epoch 108, Loss: 1.563718527178221\n",
            "Epoch 109, Loss: 1.5668437511106081\n",
            "Epoch 110, Loss: 1.5533281557167633\n",
            "Epoch 00111: reducing learning rate of group 0 to 3.4300e-04.\n",
            "Epoch 111, Loss: 1.536724766598472\n",
            "Epoch 112, Loss: 1.5327465013612676\n",
            "Epoch 113, Loss: 1.5308055507985852\n",
            "Epoch 114, Loss: 1.5261585116386414\n",
            "Epoch 115, Loss: 1.5128907614116427\n",
            "Epoch 116, Loss: 1.5170089625105072\n",
            "Epoch 117, Loss: 1.504779235471653\n",
            "Epoch 118, Loss: 1.5033608614643919\n",
            "Epoch 119, Loss: 1.5032871373092072\n",
            "Epoch 00120: reducing learning rate of group 0 to 2.4010e-04.\n",
            "Epoch 120, Loss: 1.4908266210857826\n",
            "Epoch 121, Loss: 1.4850001644484605\n",
            "Epoch 122, Loss: 1.481770954554594\n",
            "Epoch 123, Loss: 1.4808166072338442\n",
            "Epoch 124, Loss: 1.480002981952474\n",
            "Epoch 125, Loss: 1.476097060155265\n",
            "Epoch 126, Loss: 1.4741318655919424\n",
            "Epoch 127, Loss: 1.4724082030072998\n",
            "Epoch 128, Loss: 1.466612242445161\n",
            "Epoch 129, Loss: 1.4653687873218633\n",
            "Epoch 130, Loss: 1.4590858183329618\n",
            "Epoch 131, Loss: 1.4595922993708261\n",
            "Epoch 132, Loss: 1.453995046736319\n",
            "Epoch 133, Loss: 1.4494967275782475\n",
            "Epoch 00134: reducing learning rate of group 0 to 1.6807e-04.\n",
            "Epoch 134, Loss: 1.4434832889067977\n",
            "Epoch 135, Loss: 1.4437649664999563\n",
            "Epoch 136, Loss: 1.4407989650587492\n",
            "Epoch 137, Loss: 1.4388564500627639\n",
            "Epoch 138, Loss: 1.435397277149973\n",
            "Epoch 139, Loss: 1.4343968693968616\n",
            "Epoch 140, Loss: 1.4328155087519296\n",
            "Epoch 141, Loss: 1.4308177957051917\n",
            "Epoch 142, Loss: 1.427087660831741\n",
            "Epoch 143, Loss: 1.4257946172847022\n",
            "Epoch 144, Loss: 1.4258662356606013\n",
            "Epoch 145, Loss: 1.4232553832138641\n",
            "Epoch 146, Loss: 1.4199788857109938\n",
            "Epoch 147, Loss: 1.4174291763124587\n",
            "Epoch 148, Loss: 1.415795996973786\n",
            "Epoch 149, Loss: 1.4147943118705024\n",
            "Epoch 150, Loss: 1.4134979285771334\n",
            "Epoch 151, Loss: 1.4101629513728469\n",
            "Epoch 00152: reducing learning rate of group 0 to 1.1765e-04.\n",
            "Epoch 152, Loss: 1.4048852814903743\n",
            "Epoch 153, Loss: 1.4082998795599877\n",
            "Epoch 154, Loss: 1.4027516826798645\n",
            "Epoch 155, Loss: 1.3986632590052448\n",
            "Epoch 156, Loss: 1.3995145102844964\n",
            "Epoch 157, Loss: 1.4000009162516533\n",
            "Epoch 158, Loss: 1.3957999854148189\n",
            "Epoch 159, Loss: 1.3909607314610783\n",
            "Epoch 160, Loss: 1.3904378576369225\n",
            "Epoch 161, Loss: 1.3873806946639773\n",
            "Epoch 162, Loss: 1.3864619954477382\n",
            "Epoch 163, Loss: 1.3864463130130043\n",
            "Epoch 164, Loss: 1.392304726039307\n",
            "Epoch 165, Loss: 1.3870779781401912\n",
            "Epoch 166, Loss: 1.3847155465355403\n",
            "Epoch 167, Loss: 1.3801187107080146\n",
            "Epoch 168, Loss: 1.3793260839921009\n",
            "Epoch 169, Loss: 1.3779370384880258\n",
            "Epoch 170, Loss: 1.3765562161614624\n",
            "Epoch 171, Loss: 1.375743162028397\n",
            "Epoch 172, Loss: 1.3753031665765787\n",
            "Epoch 173, Loss: 1.3724982021730157\n",
            "Epoch 174, Loss: 1.3759932487825803\n",
            "Epoch 175, Loss: 1.3740216138996655\n",
            "Epoch 176, Loss: 1.3703167004675805\n",
            "Epoch 177, Loss: 1.3683318624013587\n",
            "Epoch 00178: reducing learning rate of group 0 to 8.2354e-05.\n",
            "Epoch 178, Loss: 1.3645713400991657\n",
            "Epoch 179, Loss: 1.3642523824414121\n",
            "Epoch 180, Loss: 1.363977068964439\n",
            "Epoch 181, Loss: 1.3640561130227922\n",
            "Epoch 182, Loss: 1.3631217785273926\n",
            "Epoch 183, Loss: 1.3615249542495873\n",
            "Epoch 184, Loss: 1.3592164037348349\n",
            "Epoch 185, Loss: 1.3574298401422138\n",
            "Epoch 186, Loss: 1.355523884673662\n",
            "Epoch 187, Loss: 1.3555069196073315\n",
            "Epoch 00188: reducing learning rate of group 0 to 5.7648e-05.\n",
            "Epoch 188, Loss: 1.3519381650640994\n",
            "Epoch 189, Loss: 1.3515465519850767\n",
            "Epoch 190, Loss: 1.3471450447281705\n",
            "Epoch 191, Loss: 1.3491507398931286\n",
            "Epoch 192, Loss: 1.349284200728694\n",
            "Epoch 193, Loss: 1.3480582501314864\n",
            "Epoch 194, Loss: 1.345569283524646\n",
            "Epoch 00195: reducing learning rate of group 0 to 4.0354e-05.\n",
            "Epoch 195, Loss: 1.3458811123914356\n",
            "Epoch 196, Loss: 1.3443779926511306\n",
            "Epoch 197, Loss: 1.342763087417506\n",
            "Epoch 198, Loss: 1.3428017255626148\n",
            "Epoch 199, Loss: 1.344274133821077\n",
            "Epoch 200, Loss: 1.3415514921085745\n",
            "Epoch 201, Loss: 1.3419995651214938\n",
            "Epoch 202, Loss: 1.3437950422492209\n",
            "Epoch 203, Loss: 1.339905721100071\n",
            "Epoch 204, Loss: 1.3409912397589865\n",
            "Epoch 205, Loss: 1.3380940903591205\n",
            "Epoch 00206: reducing learning rate of group 0 to 2.8248e-05.\n",
            "Epoch 206, Loss: 1.339062036592749\n",
            "Epoch 207, Loss: 1.33677082310749\n",
            "Epoch 208, Loss: 1.3386154778396027\n",
            "Epoch 209, Loss: 1.3362195469156097\n",
            "Epoch 210, Loss: 1.3350161023532288\n",
            "Epoch 211, Loss: 1.33521701753894\n",
            "Epoch 00212: reducing learning rate of group 0 to 1.9773e-05.\n",
            "Epoch 212, Loss: 1.3359871702858164\n",
            "Epoch 213, Loss: 1.3353302395796474\n",
            "Epoch 214, Loss: 1.3358447298973422\n",
            "Epoch 215, Loss: 1.3339924374713172\n",
            "Epoch 216, Loss: 1.3346120451070085\n",
            "Epoch 217, Loss: 1.3335741924334177\n",
            "Epoch 00218: reducing learning rate of group 0 to 1.3841e-05.\n",
            "Epoch 218, Loss: 1.3346481794797922\n",
            "Epoch 219, Loss: 1.332706972013546\n",
            "Early stopping triggered.\n",
            "Accuracy: 0.5949166004765687\n",
            "Sample predictions: [9, 16, 9, 19, 16, 12, 0, 13, 15, 2, 19, 18, 1, 6, 12, 3, 8, 8, 3, 15]\n",
            "Sample true labels: [9, 16, 9, 19, 0, 12, 16, 13, 15, 2, 6, 2, 3, 6, 12, 3, 8, 13, 2, 15]\n"
          ]
        }
      ]
    }
  ]
}