{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Setup"],"metadata":{"id":"lLqJy6EZtrZB"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7-ewruGCyaCn","outputId":"eeac660c-9a71-4471-ab74-54e278e8f9c5","executionInfo":{"status":"ok","timestamp":1697014132651,"user_tz":-60,"elapsed":21700,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["#### Install Pytorch Geometric Temporal"],"metadata":{"id":"yLRvBRGgt5Zk"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oiSS4wuxZ01m","outputId":"ba523748-d765-485a-f81b-516fa4f3eaed"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.0.1+cu118\n","11.8\n","Requirement already satisfied: torch-geometric-temporal in /usr/local/lib/python3.10/dist-packages (0.54.0)\n","Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (4.4.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (2.0.1+cu118)\n","Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (3.0.2)\n","Requirement already satisfied: pandas<=1.3.5 in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (1.3.5)\n","Requirement already satisfied: torch-sparse in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (0.6.18)\n","Requirement already satisfied: torch-scatter in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (2.1.2)\n","Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (2.3.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (1.23.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (1.16.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (3.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.10/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2023.3.post1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (1.12)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torch-geometric-temporal) (3.27.6)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torch-geometric-temporal) (17.0.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric->torch-geometric-temporal) (4.66.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric->torch-geometric-temporal) (1.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric->torch-geometric-temporal) (2.31.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric->torch-geometric-temporal) (3.1.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric->torch-geometric-temporal) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric->torch-geometric-temporal) (5.9.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torch-geometric-temporal) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (2023.7.22)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric->torch-geometric-temporal) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric->torch-geometric-temporal) (3.2.0)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torch-geometric-temporal) (1.3.0)\n"]}],"source":["!python -c \"import torch; print(torch.__version__)\"\n","!python -c \"import torch; print(torch.version.cuda)\"\n","!source /content/drive/MyDrive/dgmd-e-14-colab-env/bin/activate; pip install torch-geometric-temporal"]},{"cell_type":"markdown","source":["### `ASLDatasetLoader` Class\n","\n","The `ASLDatasetLoader` class is designed for loading and processing the ASL dataset. Given a directory, it reads sign language data from JSON files and constructs graph representations suitable for graph-based neural networks. Crucially, the class converts JSON data into PyTorch Geometric (PyG) `Data` objects comprising `x` (node features), `edge_index` (graph connectivity), and `y' (labels) attributes.\n","\n","**Methods**:\n","\n","- `_create_sign_to_label_map`: Generates a mapping from sign names to unique labels.\n","\n","- `_read_file_data`: Reads data from a given JSON file.\n","\n","- `_augment_data`: Implements data augmentation by applying random rotation, translation, and scaling to landmarks, which can enhance the model's robustness.\n","\n","- `_create_graph_from_frame`: Constructs a PyG `Data` object from frame data, concentrating on hand and face landmarks. Edges are created between consecutive landmarks and between left and right hand landmarks. Additional features, like hand-to-face distances, are also computed.\n","\n","- `get_dataset`: Assembles the dataset, optionally incorporating data augmentation. The function outputs a list of PyG `Data` objects ready for graph neural network processing."],"metadata":{"id":"zi7c7EMpsCBX"}},{"cell_type":"code","source":["import torch\n","import os\n","import json\n","import numpy as np\n","from torch_geometric.data import Data\n","\n","HAND_TO_FACE_THRESHOLD = 0.05\n","DATA_DIRECTORY = \"/content/drive/MyDrive/Colab Notebooks/DGMD E-14 Project/Datasets/ASL\"\n","\n","class ASLDatasetLoader:\n","    def __init__(self, directory_path):\n","        self.directory_path = directory_path\n","        self.sign_to_label = self._create_sign_to_label_map()\n","\n","    def _create_sign_to_label_map(self):\n","        signs = [os.path.splitext(filename)[0] for filename in os.listdir(self.directory_path)]\n","        return {sign: i for i, sign in enumerate(signs)}\n","\n","    def _read_file_data(self, file_path):\n","        with open(file_path, 'r') as f:\n","            return json.load(f)\n","\n","    def _augment_data(self, frame_data, rotation_range=10, translation_range=0.05, scaling_range=0.1):\n","        \"\"\"\n","        Augment the frame data with random rotation, translation, and scaling.\n","\n","        :param frame_data: Dictionary containing frame landmarks and deltas.\n","        :param rotation_range: Maximum rotation angle in degrees.\n","        :param translation_range: Maximum translation as a fraction of landmark range.\n","        :param scaling_range: Maximum scaling factor.\n","        :return: Augmented frame data.\n","        \"\"\"\n","        landmarks = np.array(frame_data[\"landmarks\"])\n","        centroid = np.mean(landmarks, axis=0)\n","\n","        # Random rotation\n","        theta = np.radians(np.random.uniform(-rotation_range, rotation_range))\n","        rotation_matrix = np.array([\n","            [np.cos(theta), -np.sin(theta)],\n","            [np.sin(theta), np.cos(theta)]\n","        ])\n","        landmarks = np.dot(landmarks - centroid, rotation_matrix) + centroid\n","\n","        # Random translation\n","        max_translation = translation_range * (landmarks.max(axis=0) - landmarks.min(axis=0))\n","        translations = np.random.uniform(-max_translation, max_translation)\n","        landmarks += translations\n","\n","        # Random scaling\n","        scale = np.random.uniform(1 - scaling_range, 1 + scaling_range)\n","        landmarks = centroid + scale * (landmarks - centroid)\n","\n","        frame_data[\"landmarks\"] = landmarks.tolist()\n","        return frame_data\n","\n","    def _create_graph_from_frame(self, sign_name, frame_data, landmark_types):\n","        left_hand_indices = [i for i, t in enumerate(landmark_types) if t == \"L\"]\n","        right_hand_indices = [i for i, t in enumerate(landmark_types) if t == \"R\"]\n","        face_indices = [i for i, t in enumerate(landmark_types) if t == \"F\"]\n","\n","        landmarks = np.array(frame_data[\"landmarks\"])\n","        deltas = np.array(frame_data[\"deltas\"])\n","\n","        # Create weights based on landmark importance\n","        weights = [2 if t == \"L\" or t == \"R\" else 1 for t in landmark_types]\n","\n","        # Create edges based on the number of available landmarks (or nodes)\n","        edges = [[i, i + 1] for i in range(len(landmarks) - 1)]\n","\n","        # Add edges between the left and right hand landmarks\n","        for i in left_hand_indices:\n","            for j in right_hand_indices:\n","                edges.append([i, j])\n","\n","        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n","\n","        # Compute additional features like hand-to-face and hand-to-body distances\n","        hand_to_face_contact = []\n","        for idx, ltype in enumerate(landmark_types):\n","            if ltype in [\"L\", \"R\"] and any(t == \"F\" for t in landmark_types):\n","                min_distance = min([np.linalg.norm(landmarks[idx] - landmarks[j]) for j, t in enumerate(landmark_types) if t == \"F\"])\n","                hand_to_face_contact.append(1 if min_distance < HAND_TO_FACE_THRESHOLD else 0)\n","            else:\n","                hand_to_face_contact.append(0)\n","\n","        # Reshape the 1D arrays to 2D for concatenation\n","        weights_2d = np.array(weights)[:, np.newaxis]\n","        hand_to_face_contact_2d = np.array(hand_to_face_contact)[:, np.newaxis]\n","\n","        # Concatenate landmarks, deltas, importance weights, and hand-to-face contact features\n","        x = torch.tensor(np.hstack((landmarks, deltas, weights_2d, hand_to_face_contact_2d)), dtype=torch.float)\n","        y = torch.tensor([self.sign_to_label[sign_name]], dtype=torch.long)\n","\n","        return Data(x=x, edge_index=edge_index, y=y)\n","\n","\n","    def get_dataset(self, augment=False):\n","        dataset = []\n","\n","        for filename in os.listdir(self.directory_path):\n","            sign_name = os.path.splitext(filename)[0]\n","            file_path = os.path.join(self.directory_path, filename)\n","            sign_data = self._read_file_data(file_path)\n","\n","            for frame_data in sign_data[\"frames\"]:\n","                landmark_types = sign_data.get(\"landmark_types\", [\"F\", \"L\", \"P\", \"R\"])  # defaulting to all types\n","\n","                if augment:\n","                  frame_data = self._augment_data(frame_data)\n","                graph_data = self._create_graph_from_frame(sign_name, frame_data, landmark_types)\n","\n","                dataset.append(graph_data)\n","\n","        return dataset\n","\n","    def number_of_classes(self):\n","        return len(self.sign_to_label)"],"metadata":{"id":"LifBvD3D4t4C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"FfO5VLnQjCtk"}},{"cell_type":"code","source":["import imgaug.augmenters as iaa\n","\n","class ASLDataset(torch.utils.data.Dataset):\n","    def __init__(self, directory_path, augment=False):\n","        self.loader = ASLDatasetLoader(directory_path)\n","        self.data_list = self.loader.get_dataset()\n","        self.augment = augment\n","\n","        # Define the augmentation pipeline\n","        self.aug_pipeline = iaa.Sequential([\n","            iaa.Affine(translate_percent={\"x\": (-0.05, 0.05), \"y\": (-0.05, 0.05)},  # Random translation\n","                       rotate=(-10, 10),  # Random rotation\n","                       scale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)}),  # Random scaling\n","            iaa.AdditiveGaussianNoise(scale=(0, 0.05*100))  # Add Gaussian Noise\n","        ])\n","\n","    def __len__(self):\n","        return len(self.data_list)\n","\n","    def __getitem__(self, idx):\n","        data = self.data_list[idx]\n","        if self.augment:\n","            landmarks = np.array(data.x[:, :2].tolist())  # Extract landmarks from feature\n","            augmented_landmarks = self.aug_pipeline(images=[landmarks])[0]\n","            data.x[:, :2] = torch.tensor(augmented_landmarks, dtype=torch.float)\n","        return data\n","\n","    def number_of_classes(self):\n","        return len(self.loader.sign_to_label)\n"],"metadata":{"id":"cdHg9zt6jARp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### `ASLGraphClassifier` Class\n","\n","The `ASLGraphClassifier`, features deeper GCN layers and additional channels to capture intricate data patterns potentially. It takes a PyG `Data` object as input, and its forward pass emits class logits.\n","\n","**Methods**:\n","\n","- `forward`: Details the forward pass, accepting a PyG `Data` object. Two GCN layers with subsequent batch normalization and dropout layers process the input. Post global max-pooling, two linear layers coupled with dropout ensure final classification, leading to log-softmax outputs."],"metadata":{"id":"xooW2sogtdL1"}},{"cell_type":"code","source":["import torch.nn.functional as F\n","from torch_geometric.nn import GCNConv, global_max_pool, global_mean_pool, BatchNorm\n","\n","class ASLGraphClassifier(torch.nn.Module):\n","    def __init__(self, num_node_features, num_classes):  # <- Added num_node_features\n","        super(ASLGraphClassifier, self).__init__()\n","        self.conv1 = GCNConv(num_node_features, 256)  # Increased channels\n","        self.bn1 = torch.nn.BatchNorm1d(256)    # Batch normalization layer\n","        self.conv2 = GCNConv(256, 512)          # Increased channels\n","        self.bn2 = torch.nn.BatchNorm1d(512)    # Batch normalization layer\n","        self.lin1 = torch.nn.Linear(512, 256)\n","        self.lin2 = torch.nn.Linear(256, num_classes)\n","        self.dropout = torch.nn.Dropout(p=0.5)\n","\n","    def forward(self, data):\n","        x, edge_index, batch = data.x, data.edge_index, data.batch\n","        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n","        x = self.dropout(x)\n","        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n","        x = self.dropout(x)\n","        x = global_max_pool(x, batch)\n","        x = F.relu(self.lin1(x))\n","        x = self.dropout(x)\n","        x = self.lin2(x)\n","        return F.log_softmax(x, dim=1)"],"metadata":{"id":"RAslUK79VVV6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch_geometric.loader import DataLoader\n","from collections import Counter\n","import random\n","\n","EPOCHS = 1\n","LEARNING_RATE = 0.001\n","NUM_WORKERS = 2\n","\n","def stratified_data_split(data, test_size=0.2):\n","    labels = [d.y.item() for d in data]\n","    train_data, test_data, _, _ = train_test_split(data, labels, test_size=test_size, stratify=labels, random_state=42)\n","    return train_data, test_data\n","\n","def evaluate(model, loader, device, detailed=False):\n","    model.eval()\n","    correct = 0\n","    all_preds = []\n","    all_labels = []\n","\n","    for batch in loader:\n","        batch = batch.to(device)\n","        with torch.no_grad():\n","            pred = model(batch).max(dim=1)[1]\n","            all_preds.extend(pred.cpu().numpy())\n","            all_labels.extend(batch.y.cpu().numpy())\n","            correct += pred.eq(batch.y).sum().item()\n","\n","    accuracy = correct / len(loader.dataset)\n","\n","    if detailed:\n","        return accuracy, all_preds, all_labels\n","    else:\n","        return accuracy\n","\n","\n","def train():\n","    loader = ASLDatasetLoader(DATA_DIRECTORY)\n","\n","    # Splitting data using the stratified_data_split method\n","    train_dataset, val_dataset = stratified_data_split(loader.get_dataset(augment=True), test_size=0.2)\n","\n","    num_classes = loader.number_of_classes()\n","\n","    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=NUM_WORKERS)\n","    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=NUM_WORKERS)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = ASLGraphClassifier(num_node_features=6, num_classes=num_classes).to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=5e-4)\n","\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n","\n","    for epoch in range(EPOCHS):\n","        model.train()\n","        total_loss = 0\n","        for batch in train_loader:\n","            batch = batch.to(device)\n","            optimizer.zero_grad()\n","            out = model(batch)\n","            loss = F.nll_loss(out, batch.y)\n","            loss.backward()\n","\n","            # Gradient clipping\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","\n","            optimizer.step()\n","            total_loss += loss.item()\n","\n","            # Check for NaN loss\n","            if np.isnan(loss.item()):\n","                print(\"Warning: NaN loss detected!\")\n","\n","        avg_loss = total_loss / len(train_loader)\n","        val_accuracy = evaluate(model, val_loader, device)\n","\n","        print(f\"Epoch {epoch}, Loss: {avg_loss}, Validation Accuracy: {val_accuracy:.4f}\")\n","\n","        scheduler.step(avg_loss)\n","\n","    # Evaluating on validation set to get sample predictions\n","    _, all_preds, all_labels = evaluate(model, val_loader, device, detailed=True)\n","    print(\"Sample predictions:\", all_preds[:20])\n","    print(\"Sample true labels:\", all_labels[:20])"],"metadata":{"id":"RdBGsFveWcbF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rr2PlwLy5M6H","outputId":"88132e30-f7b7-4350-9ce3-19fe46af0b5a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Loss: 5.126807749170425, Validation Accuracy: 0.0710\n","Sample predictions: [216, 216, 121, 113, 150, 2, 82, 82, 145, 93, 215, 192, 145, 216, 133, 160, 216, 216, 147, 189]\n","Sample true labels: [88, 21, 173, 38, 175, 64, 171, 41, 72, 88, 196, 145, 119, 212, 1, 195, 163, 12, 16, 16]\n"]}]}]}