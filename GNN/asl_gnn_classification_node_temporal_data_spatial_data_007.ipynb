{"cells":[{"cell_type":"markdown","metadata":{"id":"lLqJy6EZtrZB"},"source":["### Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"7-ewruGCyaCn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702157898141,"user_tz":420,"elapsed":35953,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}},"outputId":"b42b62ef-5bd2-4a0b-860c-065880e080a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"yLRvBRGgt5Zk"},"source":["#### Install Pytorch Geometric Temporal"]},{"cell_type":"code","source":["!pip install torch_geometric"],"metadata":{"id":"QRkM-wvTVBMw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702157905463,"user_tz":420,"elapsed":7323,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}},"outputId":"0e01b06e-0cee-4f77-8924-8af6d52ac4b7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch_geometric\n","  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m0.9/1.0 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.11.17)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n","Installing collected packages: torch_geometric\n","Successfully installed torch_geometric-2.4.0\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"oiSS4wuxZ01m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702157920120,"user_tz":420,"elapsed":14659,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}},"outputId":"5e3a48e7-e7bc-4498-a08a-e3959b434165"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://data.pyg.org/whl/torch-2.1.0+cu118.html\n","Collecting pyg_lib\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/pyg_lib-0.3.1%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (2.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_scatter\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_scatter-2.1.2%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_sparse\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_sparse-0.6.18%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_cluster\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_cluster-1.6.3%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_spline_conv\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_spline_conv-1.2.2%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (887 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.8/887.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_sparse) (1.11.4)\n","Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch_sparse) (1.23.5)\n","Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n","Successfully installed pyg_lib-0.3.1+pt21cu118 torch_cluster-1.6.3+pt21cu118 torch_scatter-2.1.2+pt21cu118 torch_sparse-0.6.18+pt21cu118 torch_spline_conv-1.2.2+pt21cu118\n"]}],"source":["import torch\n","\n","TORCH = torch.__version__.split('+')[0]\n","CUDA = 'cu' + torch.version.cuda.replace('.', '')\n","\n","# Construct the installation command\n","install_command = f\"pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\"\n","\n","# Execute the command\n","!{install_command}"]},{"cell_type":"code","source":["import numpy as np\n","import os\n","\n","# Set a random seed for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""],"metadata":{"id":"upFSRna0XWWE","executionInfo":{"status":"ok","timestamp":1702157920120,"user_tz":420,"elapsed":3,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zi7c7EMpsCBX"},"source":["### `ASLDatasetLoader` Class\n","\n","The `ASLDatasetLoader` class is designed for loading and processing the ASL dataset. Given a directory, it reads sign language data from JSON files and constructs graph representations suitable for graph-based neural networks. Crucially, the class converts JSON data into PyTorch Geometric (PyG) `Data` objects comprising `x` (node features), `edge_index` (graph connectivity), and `y' (labels) attributes.\n","\n","**Methods**:\n","\n","- `_create_sign_to_label_map`: Generates a mapping from sign names to unique labels.\n","\n","- `_read_file_data`: Reads data from a given JSON file.\n","\n","- `_augment_data`: Implements data augmentation by applying random rotation, translation, and scaling to landmarks, which can enhance the model's robustness.\n","\n","- `_create_graph_from_frame`: Constructs a PyG `Data` object from frame data, concentrating on hand and face landmarks. Edges are created between consecutive landmarks and between left and right hand landmarks. Additional features, like hand-to-face distances, are also computed.\n","\n","- `get_dataset`: Assembles the dataset, optionally incorporating data augmentation. The function outputs a list of PyG `Data` objects ready for graph neural network processing."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"LifBvD3D4t4C","executionInfo":{"status":"ok","timestamp":1702157923646,"user_tz":420,"elapsed":3528,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"outputs":[],"source":["import json\n","from torch_geometric.data import Data\n","\n","class ASLDatasetLoader:\n","    # Define natural connections as class attributes\n","    HAND_CONNECTIONS = frozenset([\n","      # Left hand palm\n","      (\"left_hand-0\", \"left_hand-1\"),\n","      (\"left_hand-0\", \"left_hand-5\"),\n","      (\"left_hand-9\", \"left_hand-13\"),\n","      (\"left_hand-13\", \"left_hand-17\"),\n","      (\"left_hand-5\", \"left_hand-9\"),\n","      (\"left_hand-0\", \"left_hand-17\"),\n","      # Left hand thumb\n","      (\"left_hand-1\", \"left_hand-2\"),\n","      (\"left_hand-2\", \"left_hand-3\"),\n","      (\"left_hand-3\", \"left_hand-4\"),\n","      # Left hand index finger\n","      (\"left_hand-5\", \"left_hand-6\"),\n","      (\"left_hand-6\", \"left_hand-7\"),\n","      (\"left_hand-7\", \"left_hand-8\"),\n","      # Left hand middle finger\n","      (\"left_hand-9\", \"left_hand-10\"),\n","      (\"left_hand-10\", \"left_hand-11\"),\n","      (\"left_hand-11\", \"left_hand-12\"),\n","      # Left hand ring finger\n","      (\"left_hand-13\", \"left_hand-14\"),\n","      (\"left_hand-14\", \"left_hand-15\"),\n","      (\"left_hand-15\", \"left_hand-16\"),\n","      # Left hand pinky\n","      (\"left_hand-17\", \"left_hand-18\"),\n","      (\"left_hand-18\", \"left_hand-19\"),\n","      (\"left_hand-19\", \"left_hand-20\"),\n","      # Right hand palm\n","      (\"right_hand-0\", \"right_hand-1\"),\n","      (\"right_hand-0\", \"right_hand-5\"),\n","      (\"right_hand-9\", \"right_hand-13\"),\n","      (\"right_hand-13\", \"right_hand-17\"),\n","      (\"right_hand-5\", \"right_hand-9\"),\n","      (\"right_hand-0\", \"right_hand-17\"),\n","      # Right hand thumb\n","      (\"right_hand-1\", \"right_hand-2\"),\n","      (\"right_hand-2\", \"right_hand-3\"),\n","      (\"right_hand-3\", \"right_hand-4\"),\n","      # Right hand index finger\n","      (\"right_hand-5\", \"right_hand-6\"),\n","      (\"right_hand-6\", \"right_hand-7\"),\n","      (\"right_hand-7\", \"right_hand-8\"),\n","      # Right hand middle finger\n","      (\"right_hand-9\", \"right_hand-10\"),\n","      (\"right_hand-10\", \"right_hand-11\"),\n","      (\"right_hand-11\", \"right_hand-12\"),\n","      # Right hand ring finger\n","      (\"right_hand-13\", \"right_hand-14\"),\n","      (\"right_hand-14\", \"right_hand-15\"),\n","      (\"right_hand-15\", \"right_hand-16\"),\n","      # Right hand pinky\n","      (\"right_hand-17\", \"right_hand-18\"),\n","      (\"right_hand-18\", \"right_hand-19\"),\n","      (\"right_hand-19\", \"right_hand-20\"),\n","    ])\n","\n","    POSE_CONNECTIONS = frozenset([\n","      (\"pose-0\", \"pose-1\"),\n","      (\"pose-1\", \"pose-2\"),\n","      (\"pose-2\", \"pose-3\"),\n","      (\"pose-3\", \"pose-7\"),\n","      (\"pose-0\", \"pose-4\"),\n","      (\"pose-4\", \"pose-5\"),\n","      (\"pose-5\", \"pose-6\"),\n","      (\"pose-6\", \"pose-8\"),\n","      (\"pose-9\", \"pose-10\"),\n","      (\"pose-11\", \"pose-12\"),\n","      (\"pose-11\", \"pose-13\"),\n","      (\"pose-13\", \"pose-15\"),\n","      (\"pose-15\", \"pose-17\"),\n","      (\"pose-12\", \"pose-14\"),\n","      (\"pose-14\", \"pose-16\"),\n","      (\"pose-16\", \"pose-18\"),\n","      (\"pose-11\", \"pose-23\"),\n","      (\"pose-12\", \"pose-24\"),\n","      (\"pose-23\", \"pose-24\"),\n","    ])\n","\n","    FACE_CONNECTIONS = frozenset([\n","      # Connections for FACEMESH_LIPS using available landmarks\n","      (\"face-61\", \"face-146\"), (\"face-146\", \"face-91\"), (\"face-91\", \"face-181\"),\n","      (\"face-181\", \"face-84\"), (\"face-84\", \"face-17\"), (\"face-17\", \"face-314\"),\n","      (\"face-314\", \"face-405\"), (\"face-405\", \"face-321\"), (\"face-321\", \"face-375\"),\n","      (\"face-375\", \"face-291\"), (\"face-78\", \"face-95\"), (\"face-95\", \"face-88\"),\n","      (\"face-88\", \"face-178\"), (\"face-178\", \"face-87\"), (\"face-87\", \"face-14\"),\n","      (\"face-14\", \"face-317\"), (\"face-317\", \"face-402\"), (\"face-402\", \"face-318\"),\n","      (\"face-318\", \"face-324\"), (\"face-324\", \"face-308\"),\n","\n","      # Connections for FACEMESH_LEFT_EYE using available landmarks\n","      (\"face-263\", \"face-249\"), (\"face-388\", \"face-387\"), (\"face-387\", \"face-386\"),\n","      (\"face-386\", \"face-385\"), (\"face-385\", \"face-384\"), (\"face-384\", \"face-398\"),\n","\n","      # Connections for FACEMESH_LEFT_EYEBROW using available landmarks\n","      (\"face-276\", \"face-283\"), (\"face-300\", \"face-293\"), (\"face-293\", \"face-334\"),\n","      (\"face-334\", \"face-296\"), (\"face-296\", \"face-336\"),\n","\n","      # Connections for FACEMESH_RIGHT_EYE using available landmarks\n","      (\"face-33\", \"face-7\"), (\"face-246\", \"face-161\"), (\"face-161\", \"face-160\"),\n","      (\"face-160\", \"face-159\"), (\"face-159\", \"face-158\"), (\"face-158\", \"face-157\"),\n","      (\"face-157\", \"face-173\"),\n","\n","      # Connections for FACEMESH_RIGHT_EYEBROW using available landmarks\n","      (\"face-46\", \"face-53\"), (\"face-70\", \"face-63\"), (\"face-63\", \"face-105\"),\n","      (\"face-105\", \"face-66\"), (\"face-66\", \"face-107\"),\n","\n","      # Connections for FACEMESH_FACE_OVAL using available landmarks\n","      (\"face-10\", \"face-338\"), (\"face-338\", \"face-297\"), (\"face-297\", \"face-332\"),\n","      (\"face-332\", \"face-284\"), (\"face-284\", \"face-251\"), (\"face-251\", \"face-389\"),\n","      (\"face-389\", \"face-356\"), (\"face-356\", \"face-454\"), (\"face-454\", \"face-323\"),\n","      (\"face-323\", \"face-361\"), (\"face-361\", \"face-288\"), (\"face-288\", \"face-397\"),\n","      (\"face-397\", \"face-365\"), (\"face-365\", \"face-379\"), (\"face-379\", \"face-378\"),\n","      (\"face-378\", \"face-400\"), (\"face-400\", \"face-377\"), (\"face-377\", \"face-152\"),\n","      (\"face-152\", \"face-148\"), (\"face-148\", \"face-176\"), (\"face-176\", \"face-149\"),\n","      (\"face-149\", \"face-150\"), (\"face-150\", \"face-136\"), (\"face-136\", \"face-172\"),\n","      (\"face-172\", \"face-58\"), (\"face-58\", \"face-132\"), (\"face-132\", \"face-93\"),\n","      (\"face-93\", \"face-234\"), (\"face-234\", \"face-127\"), (\"face-127\", \"face-162\"),\n","      (\"face-162\", \"face-21\"), (\"face-21\", \"face-54\"), (\"face-54\", \"face-103\"),\n","      (\"face-103\", \"face-67\"), (\"face-67\", \"face-109\"), (\"face-109\", \"face-10\"),\n","    ])\n","\n","    def __init__(self, directory_path, min_examples_per_class=2, max_files=-1):\n","        self.directory_path = directory_path\n","        self.min_examples_per_class = min_examples_per_class\n","        self.max_files = max_files\n","        self.sign_to_label = self._create_sign_to_label_map()\n","\n","    def _create_sign_to_label_map(self):\n","        signs = [os.path.splitext(filename)[0] for filename in os.listdir(self.directory_path)]\n","        return {sign: i for i, sign in enumerate(signs)}\n","\n","    def _read_file_data(self, file_path):\n","        with open(file_path, 'r') as f:\n","            data = json.load(f)\n","\n","        sign_name = data[\"sign\"]\n","        num_examples = len(data[\"examples\"])\n","        print(f\"Loaded sign '{sign_name}' with {num_examples} examples\")\n","\n","        return data if num_examples >= self.min_examples_per_class else None\n","\n","    def _augment_data(self, frame_data, rotation_range=10, translation_range=0.05, scaling_range=0.1, jittering_range=0.01, noise_scale=0.01, mirroring=False):\n","        \"\"\"\n","        Augment the frame data with various techniques including rotation, translation, scaling, jittering, noise injection, and mirroring.\n","\n","        :param frame_data: Dictionary containing frame landmarks and deltas.\n","        :param rotation_range: Maximum rotation angle in degrees.\n","        :param translation_range: Maximum translation as a fraction of landmark range.\n","        :param scaling_range: Maximum scaling factor.\n","        :param jittering_range: Range for jittering.\n","        :param noise_scale: Scale of the random noise to be added.\n","        :param mirroring: Whether to mirror the landmarks (simulate opposite hand).\n","        :return: Augmented frame data.\n","        \"\"\"\n","        # Extract landmarks\n","        landmarks = np.array([[landmark['x'], landmark['y']] for landmark in frame_data['landmarks']])\n","        centroid = np.mean(landmarks, axis=0)\n","\n","        # Jittering\n","        jittering = np.random.uniform(-jittering_range, jittering_range, landmarks.shape)\n","        landmarks += jittering\n","\n","        # Random rotation\n","        theta = np.radians(np.random.uniform(-rotation_range, rotation_range))\n","        rotation_matrix = np.array([\n","            [np.cos(theta), -np.sin(theta)],\n","            [np.sin(theta), np.cos(theta)]\n","        ])\n","        landmarks = np.dot(landmarks - centroid, rotation_matrix) + centroid\n","\n","        # Random translation\n","        max_translation = translation_range * (landmarks.max(axis=0) - landmarks.min(axis=0))\n","        translations = np.random.uniform(-max_translation, max_translation, size=landmarks.shape[1])\n","        landmarks += translations\n","\n","        # Random scaling\n","        scale = np.random.uniform(1 - scaling_range, 1 + scaling_range)\n","        landmarks = centroid + scale * (landmarks - centroid)\n","\n","        # Noise Injection\n","        noise = np.random.normal(0, noise_scale, landmarks.shape)\n","        landmarks += noise\n","\n","        # Mirroring (if applicable)\n","        if mirroring:\n","            landmarks[:, 0] = -landmarks[:, 0] + 2 * centroid[0]  # Reflect x-coordinates\n","\n","        # Update the landmarks in frame_data\n","        for i, landmark in enumerate(frame_data['landmarks']):\n","            landmark['x'], landmark['y'] = landmarks[i]\n","\n","        return frame_data\n","\n","\n","    def _create_graph_from_frame(self, sign_name, sign_data):\n","        graphs = []\n","\n","        for example in sign_data[\"examples\"]:\n","            all_features = []  # Combined list for landmarks, velocities, and accelerations\n","            edges = []\n","\n","            for frame in example[\"frames\"]:\n","                for landmark_data in frame[\"landmarks\"]:\n","                    # Extract spatial coordinates\n","                    landmark_features = [landmark_data[\"x\"], landmark_data[\"y\"]]\n","\n","                    # Extract temporal data (velocity and acceleration)\n","                    temporal_data = next((item for item in frame[\"temporal\"] if item[\"landmark\"] == landmark_data[\"landmark\"]), None)\n","                    if temporal_data:\n","                        velocity = [temporal_data[\"velocity\"][\"x\"], temporal_data[\"velocity\"][\"y\"]]\n","                        acceleration = [temporal_data[\"acceleration\"][\"x\"], temporal_data[\"acceleration\"][\"y\"]]\n","                    else:\n","                        velocity = [0, 0]\n","                        acceleration = [0, 0]\n","\n","                    # Combine spatial and temporal features\n","                    combined_features = landmark_features + velocity + acceleration\n","                    all_features.append(combined_features)\n","\n","                # Add spatial edges within the frame using natural connections\n","                for i in range(len(frame[\"landmarks\"])):\n","                    for j in range(len(frame[\"landmarks\"])):\n","                        if i != j:\n","                            connection = (frame[\"landmarks\"][i][\"landmark\"], frame[\"landmarks\"][j][\"landmark\"])\n","                            if connection in self.HAND_CONNECTIONS or \\\n","                              connection in self.POSE_CONNECTIONS or \\\n","                              connection in self.FACE_CONNECTIONS:\n","                                edges.append([len(all_features) - len(frame[\"landmarks\"]) + i,\n","                                              len(all_features) - len(frame[\"landmarks\"]) + j])\n","\n","            # Add temporal edges between frames within each example\n","            for i in range(len(example[\"frames\"]) - 1):\n","                for j in range(len(frame[\"landmarks\"])):\n","                    start_index = i * len(frame[\"landmarks\"]) + j\n","                    end_index = (i + 1) * len(frame[\"landmarks\"]) + j\n","                    edges.append([start_index, end_index])\n","\n","            # Create the graph\n","            x = torch.tensor(all_features, dtype=torch.float)\n","            edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n","            y = torch.tensor([self.sign_to_label[sign_name]], dtype=torch.long)\n","\n","            # Append the graph for the current example to the list of graphs\n","            graphs.append(Data(x=x, edge_index=edge_index, y=y))\n","\n","        return graphs\n","\n","    def get_dataset(self, augment=False):\n","        dataset = []\n","        file_count = 0\n","\n","        for filename in os.listdir(self.directory_path):\n","            if 0 <= self.max_files <= file_count:\n","                break  # Stop if max_files limit is reached\n","\n","            sign_name = os.path.splitext(filename)[0]\n","            file_path = os.path.join(self.directory_path, filename)\n","            sign_data = self._read_file_data(file_path)\n","            file_count += 1\n","\n","            if sign_data is None:\n","                print(f\"Skipping sign '{sign_name}' due to insufficient examples\")\n","                continue\n","\n","            # Retrieve a list of graphs, one for each example\n","            graphs = self._create_graph_from_frame(sign_name, sign_data)\n","\n","            # Debugging: Check the number of graphs created for the current sign\n","            print(f\"Sign '{sign_name}': Created {len(graphs)} graphs\")\n","\n","            dataset.extend(graphs)  # Extend the dataset with the list of graphs\n","\n","        return dataset\n","\n","    def number_of_classes(self):\n","        return len(self.sign_to_label)"]},{"cell_type":"markdown","metadata":{"id":"xooW2sogtdL1"},"source":["### `ASLGraphClassifier` Class\n","\n","The `ASLGraphClassifier`, features deeper GCN layers and additional channels to capture intricate data patterns potentially. It takes a PyG `Data` object as input, and its forward pass emits class logits.\n","\n","**Methods**:\n","\n","- `forward`: Details the forward pass, accepting a PyG `Data` object. Two GCN layers with subsequent batch normalization and dropout layers process the input. Post global max-pooling, two linear layers coupled with dropout ensure final classification, leading to log-softmax outputs."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"RAslUK79VVV6","executionInfo":{"status":"ok","timestamp":1702157923647,"user_tz":420,"elapsed":5,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"outputs":[],"source":["import torch.nn.functional as F\n","from torch_geometric.nn import GCNConv, BatchNorm, global_max_pool, LayerNorm\n","\n","class ASLGraphClassifier(torch.nn.Module):\n","    def __init__(self, num_features, num_classes, dropout_rate=0.7):\n","        super(ASLGraphClassifier, self).__init__()\n","        self.conv1 = GCNConv(num_features, 512)\n","        self.bn1 = BatchNorm(512)\n","        self.conv2 = GCNConv(512, 1024)\n","        self.bn2 = BatchNorm(1024)\n","        self.ln1 = LayerNorm(1024)  # Layer normalization\n","        self.lin1 = torch.nn.Linear(1024, 512)\n","        self.ln2 = LayerNorm(512)  # Layer normalization\n","        self.lin2 = torch.nn.Linear(512, num_classes)\n","\n","        self.dropout = torch.nn.Dropout(p=dropout_rate)\n","\n","    def forward(self, data):\n","        x, edge_index, batch = data.x, data.edge_index, data.batch\n","\n","        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n","        x = self.dropout(x)\n","        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n","        x = self.ln1(x)  # Apply layer normalization\n","        x = self.dropout(x)\n","\n","        x = global_max_pool(x, batch)\n","\n","        x = F.relu(self.lin1(x))\n","        x = self.ln2(x)  # Apply layer normalization\n","        x = self.dropout(x)\n","        x = self.lin2(x)\n","        return F.log_softmax(x, dim=1)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"RdBGsFveWcbF","executionInfo":{"status":"ok","timestamp":1702157924838,"user_tz":420,"elapsed":1195,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from torch_geometric.loader import DataLoader\n","from sklearn.model_selection import StratifiedKFold\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","def validate(loader, model, device):\n","    model.eval()\n","    correct = 0\n","    for data in loader:\n","        data = data.to(device)\n","        with torch.no_grad():\n","            out = model(data)\n","        pred = out.argmax(dim=1)\n","        correct += int((pred == data.y).sum())\n","    return correct / len(loader.dataset)\n","\n","def train(loader, EPOCHS=100, LEARNING_RATE=0.00005, K_FOLDS=5, AUGMENT=True):\n","    dataset = loader.get_dataset()\n","    if AUGMENT:\n","        augmented_dataset = loader.get_dataset(augment=True)\n","        dataset.extend(augmented_dataset)  # Combine original and augmented datasets\n","\n","    labels = [data.y.item() for data in dataset]  # Extract labels for stratification\n","\n","    # Stratified K-Fold Cross-validation\n","    kfold = StratifiedKFold(n_splits=K_FOLDS, shuffle=True)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    num_features = dataset[0].x.size(1)\n","    num_classes = loader.number_of_classes()\n","\n","    all_fold_preds = []\n","    all_fold_labels = []\n","    fold_accuracies = []\n","\n","    for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset, labels)):\n","        print(f\"FOLD {fold}\")\n","        print(\"--------------------------------\")\n","\n","        train_dataset = [dataset[i] for i in train_ids]\n","        test_dataset = [dataset[i] for i in test_ids]\n","\n","        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","        model = ASLGraphClassifier(num_features, num_classes, dropout_rate=0.5).to(device)\n","        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=5e-4)\n","        scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.7, patience=10, verbose=True)\n","\n","        best_val_accuracy = 0\n","        epochs_without_improvement = 0\n","        max_epochs_without_improvement = 20\n","\n","        for epoch in range(EPOCHS):\n","            model.train()\n","            total_loss = 0\n","            correct_train = 0\n","            total_train = 0\n","\n","            for batch in train_loader:\n","                batch = batch.to(device)\n","                optimizer.zero_grad()\n","                out = model(batch)\n","                loss = F.nll_loss(out, batch.y)\n","                loss.backward()\n","                optimizer.step()\n","\n","                total_loss += loss.item()\n","                pred_train = out.argmax(dim=1)\n","                correct_train += int((pred_train == batch.y).sum())\n","                total_train += batch.y.size(0)\n","\n","            train_accuracy = correct_train / total_train\n","            val_accuracy = validate(test_loader, model, device)\n","\n","            print(f\"Epoch {epoch}, Loss: {total_loss / len(train_loader):.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n","\n","            scheduler.step(val_accuracy)\n","\n","            if val_accuracy > best_val_accuracy:\n","                best_val_accuracy = val_accuracy\n","                epochs_without_improvement = 0\n","            else:\n","                epochs_without_improvement += 1\n","\n","            if epochs_without_improvement >= max_epochs_without_improvement:\n","                print(\"Early stopping triggered.\")\n","                break\n","\n","        model.eval()\n","        fold_preds, fold_labels, correct = validate(test_loader, model, device)\n","\n","        fold_accuracy = correct / len(test_ids)\n","        fold_accuracies.append(fold_accuracy)\n","        all_fold_preds.extend(fold_preds)\n","        all_fold_labels.extend(fold_labels)\n","\n","        print(f\"Accuracy for fold {fold}: {fold_accuracy}\\n\")\n","\n","    overall_accuracy = sum(fold_accuracies) / len(fold_accuracies)\n","    print(f\"Overall accuracy across all folds: {overall_accuracy}\")\n","\n","    print(\"Sample predictions:\", all_fold_preds[:20])\n","    print(\"Sample true labels:\", all_fold_labels[:20])\n","\n","    return model, all_fold_preds, all_fold_labels, overall_accuracy\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"rr2PlwLy5M6H","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1702168961446,"user_tz":420,"elapsed":11036612,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}},"outputId":"2aa04eee-f74b-4fd5-c1c9-a70d47a7111a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded sign 'TV' with 385 examples\n","Sign 'TV': Created 385 graphs\n","Loaded sign 'after' with 347 examples\n","Sign 'after': Created 347 graphs\n","Loaded sign 'airplane' with 393 examples\n","Sign 'airplane': Created 393 graphs\n","Loaded sign 'all' with 386 examples\n","Sign 'all': Created 386 graphs\n","Loaded sign 'alligator' with 390 examples\n","Sign 'alligator': Created 390 graphs\n","Loaded sign 'bird' with 404 examples\n","Sign 'bird': Created 404 graphs\n","Loaded sign 'callonphone' with 385 examples\n","Sign 'callonphone': Created 385 graphs\n","Loaded sign 'cry' with 390 examples\n","Sign 'cry': Created 390 graphs\n","Loaded sign 'dad' with 378 examples\n","Sign 'dad': Created 378 graphs\n","Loaded sign 'dance' with 312 examples\n","Sign 'dance': Created 312 graphs\n","Loaded sign 'dog' with 380 examples\n","Sign 'dog': Created 380 graphs\n","Loaded sign 'drink' with 400 examples\n","Sign 'drink': Created 400 graphs\n","Loaded sign 'duck' with 405 examples\n","Sign 'duck': Created 405 graphs\n","Loaded sign 'elephant' with 382 examples\n","Sign 'elephant': Created 382 graphs\n","Loaded sign 'eye' with 395 examples\n","Sign 'eye': Created 395 graphs\n","Loaded sign 'feet' with 372 examples\n","Sign 'feet': Created 372 graphs\n","Loaded sign 'finger' with 379 examples\n","Sign 'finger': Created 379 graphs\n","Loaded sign 'flower' with 396 examples\n","Sign 'flower': Created 396 graphs\n","Loaded sign 'food' with 392 examples\n","Sign 'food': Created 392 graphs\n","Loaded sign 'face' with 370 examples\n","Sign 'face': Created 370 graphs\n","Loaded sign 'TV' with 385 examples\n","Sign 'TV': Created 385 graphs\n","Loaded sign 'after' with 347 examples\n","Sign 'after': Created 347 graphs\n","Loaded sign 'airplane' with 393 examples\n","Sign 'airplane': Created 393 graphs\n","Loaded sign 'all' with 386 examples\n","Sign 'all': Created 386 graphs\n","Loaded sign 'alligator' with 390 examples\n","Sign 'alligator': Created 390 graphs\n","Loaded sign 'bird' with 404 examples\n","Sign 'bird': Created 404 graphs\n","Loaded sign 'callonphone' with 385 examples\n","Sign 'callonphone': Created 385 graphs\n","Loaded sign 'cry' with 390 examples\n","Sign 'cry': Created 390 graphs\n","Loaded sign 'dad' with 378 examples\n","Sign 'dad': Created 378 graphs\n","Loaded sign 'dance' with 312 examples\n","Sign 'dance': Created 312 graphs\n","Loaded sign 'dog' with 380 examples\n","Sign 'dog': Created 380 graphs\n","Loaded sign 'drink' with 400 examples\n","Sign 'drink': Created 400 graphs\n","Loaded sign 'duck' with 405 examples\n","Sign 'duck': Created 405 graphs\n","Loaded sign 'elephant' with 382 examples\n","Sign 'elephant': Created 382 graphs\n","Loaded sign 'eye' with 395 examples\n","Sign 'eye': Created 395 graphs\n","Loaded sign 'feet' with 372 examples\n","Sign 'feet': Created 372 graphs\n","Loaded sign 'finger' with 379 examples\n","Sign 'finger': Created 379 graphs\n","Loaded sign 'flower' with 396 examples\n","Sign 'flower': Created 396 graphs\n","Loaded sign 'food' with 392 examples\n","Sign 'food': Created 392 graphs\n","Loaded sign 'face' with 370 examples\n","Sign 'face': Created 370 graphs\n","FOLD 0\n","--------------------------------\n","Epoch 0, Loss: 3.1420, Training Accuracy: 0.0524, Validation Accuracy: 0.0801\n","Epoch 1, Loss: 3.0591, Training Accuracy: 0.0638, Validation Accuracy: 0.0638\n","Epoch 2, Loss: 2.9736, Training Accuracy: 0.0771, Validation Accuracy: 0.0922\n","Epoch 3, Loss: 2.8946, Training Accuracy: 0.1061, Validation Accuracy: 0.1014\n","Epoch 4, Loss: 2.8184, Training Accuracy: 0.1200, Validation Accuracy: 0.1197\n","Epoch 5, Loss: 2.7548, Training Accuracy: 0.1412, Validation Accuracy: 0.1161\n","Epoch 6, Loss: 2.7059, Training Accuracy: 0.1560, Validation Accuracy: 0.1292\n","Epoch 7, Loss: 2.6642, Training Accuracy: 0.1603, Validation Accuracy: 0.1325\n","Epoch 8, Loss: 2.6163, Training Accuracy: 0.1798, Validation Accuracy: 0.1361\n","Epoch 9, Loss: 2.5726, Training Accuracy: 0.1854, Validation Accuracy: 0.1511\n","Epoch 10, Loss: 2.5363, Training Accuracy: 0.2007, Validation Accuracy: 0.1655\n","Epoch 11, Loss: 2.5125, Training Accuracy: 0.2095, Validation Accuracy: 0.1685\n","Epoch 12, Loss: 2.4783, Training Accuracy: 0.2227, Validation Accuracy: 0.1766\n","Epoch 13, Loss: 2.4671, Training Accuracy: 0.2254, Validation Accuracy: 0.1694\n","Epoch 14, Loss: 2.4521, Training Accuracy: 0.2236, Validation Accuracy: 0.1878\n","Epoch 15, Loss: 2.4091, Training Accuracy: 0.2433, Validation Accuracy: 0.2103\n","Epoch 16, Loss: 2.3967, Training Accuracy: 0.2418, Validation Accuracy: 0.2116\n","Epoch 17, Loss: 2.3744, Training Accuracy: 0.2515, Validation Accuracy: 0.2152\n","Epoch 18, Loss: 2.3604, Training Accuracy: 0.2520, Validation Accuracy: 0.2133\n","Epoch 19, Loss: 2.3492, Training Accuracy: 0.2638, Validation Accuracy: 0.1976\n","Epoch 20, Loss: 2.3345, Training Accuracy: 0.2652, Validation Accuracy: 0.2156\n","Epoch 21, Loss: 2.3173, Training Accuracy: 0.2753, Validation Accuracy: 0.2228\n","Epoch 22, Loss: 2.3044, Training Accuracy: 0.2753, Validation Accuracy: 0.2133\n","Epoch 23, Loss: 2.2962, Training Accuracy: 0.2813, Validation Accuracy: 0.2277\n","Epoch 24, Loss: 2.2764, Training Accuracy: 0.2838, Validation Accuracy: 0.2175\n","Epoch 25, Loss: 2.2673, Training Accuracy: 0.2879, Validation Accuracy: 0.2080\n","Epoch 26, Loss: 2.2371, Training Accuracy: 0.2992, Validation Accuracy: 0.2211\n","Epoch 27, Loss: 2.2550, Training Accuracy: 0.2899, Validation Accuracy: 0.2162\n","Epoch 28, Loss: 2.2591, Training Accuracy: 0.2849, Validation Accuracy: 0.2270\n","Epoch 29, Loss: 2.2469, Training Accuracy: 0.2961, Validation Accuracy: 0.2130\n","Epoch 30, Loss: 2.2464, Training Accuracy: 0.2930, Validation Accuracy: 0.2391\n","Epoch 31, Loss: 2.2481, Training Accuracy: 0.2944, Validation Accuracy: 0.2300\n","Epoch 32, Loss: 2.2350, Training Accuracy: 0.2956, Validation Accuracy: 0.2149\n","Epoch 33, Loss: 2.2548, Training Accuracy: 0.2854, Validation Accuracy: 0.2123\n","Epoch 34, Loss: 2.2123, Training Accuracy: 0.3076, Validation Accuracy: 0.2022\n","Epoch 35, Loss: 2.2087, Training Accuracy: 0.3010, Validation Accuracy: 0.2424\n","Epoch 36, Loss: 2.1981, Training Accuracy: 0.3108, Validation Accuracy: 0.2287\n","Epoch 37, Loss: 2.1919, Training Accuracy: 0.3039, Validation Accuracy: 0.2084\n","Epoch 38, Loss: 2.1931, Training Accuracy: 0.3045, Validation Accuracy: 0.2159\n","Epoch 39, Loss: 2.1806, Training Accuracy: 0.3175, Validation Accuracy: 0.2136\n","Epoch 40, Loss: 2.1689, Training Accuracy: 0.3180, Validation Accuracy: 0.2476\n","Epoch 41, Loss: 2.1565, Training Accuracy: 0.3222, Validation Accuracy: 0.2169\n","Epoch 42, Loss: 2.1704, Training Accuracy: 0.3114, Validation Accuracy: 0.2388\n","Epoch 43, Loss: 2.1637, Training Accuracy: 0.3243, Validation Accuracy: 0.2365\n","Epoch 44, Loss: 2.1676, Training Accuracy: 0.3148, Validation Accuracy: 0.2381\n","Epoch 45, Loss: 2.1426, Training Accuracy: 0.3256, Validation Accuracy: 0.2362\n","Epoch 46, Loss: 2.1600, Training Accuracy: 0.3180, Validation Accuracy: 0.2211\n","Epoch 47, Loss: 2.1559, Training Accuracy: 0.3166, Validation Accuracy: 0.2457\n","Epoch 48, Loss: 2.1363, Training Accuracy: 0.3287, Validation Accuracy: 0.2421\n","Epoch 49, Loss: 2.1306, Training Accuracy: 0.3250, Validation Accuracy: 0.2359\n","Epoch 50, Loss: 2.1286, Training Accuracy: 0.3278, Validation Accuracy: 0.2365\n","Epoch 51, Loss: 2.1353, Training Accuracy: 0.3292, Validation Accuracy: 0.2355\n","Epoch 00052: reducing learning rate of group 0 to 3.5000e-05.\n","Epoch 52, Loss: 2.0889, Training Accuracy: 0.3374, Validation Accuracy: 0.2339\n","Epoch 53, Loss: 2.0905, Training Accuracy: 0.3420, Validation Accuracy: 0.2427\n","Epoch 54, Loss: 2.0750, Training Accuracy: 0.3444, Validation Accuracy: 0.2211\n","Epoch 55, Loss: 2.0685, Training Accuracy: 0.3535, Validation Accuracy: 0.2381\n","Epoch 56, Loss: 2.0873, Training Accuracy: 0.3426, Validation Accuracy: 0.2375\n","Epoch 57, Loss: 2.0552, Training Accuracy: 0.3553, Validation Accuracy: 0.2368\n","Epoch 58, Loss: 2.0553, Training Accuracy: 0.3535, Validation Accuracy: 0.2588\n","Epoch 59, Loss: 2.0516, Training Accuracy: 0.3597, Validation Accuracy: 0.2228\n","Epoch 60, Loss: 2.0450, Training Accuracy: 0.3538, Validation Accuracy: 0.2502\n","Epoch 61, Loss: 2.0442, Training Accuracy: 0.3578, Validation Accuracy: 0.2588\n","Epoch 62, Loss: 2.0255, Training Accuracy: 0.3658, Validation Accuracy: 0.2653\n","Epoch 63, Loss: 2.0303, Training Accuracy: 0.3611, Validation Accuracy: 0.2309\n","Epoch 64, Loss: 2.0155, Training Accuracy: 0.3637, Validation Accuracy: 0.2486\n","Epoch 65, Loss: 2.0315, Training Accuracy: 0.3587, Validation Accuracy: 0.2715\n","Epoch 66, Loss: 2.0035, Training Accuracy: 0.3643, Validation Accuracy: 0.2565\n","Epoch 67, Loss: 2.0086, Training Accuracy: 0.3664, Validation Accuracy: 0.2568\n","Epoch 68, Loss: 2.0337, Training Accuracy: 0.3672, Validation Accuracy: 0.2588\n","Epoch 69, Loss: 2.0172, Training Accuracy: 0.3647, Validation Accuracy: 0.2571\n","Epoch 70, Loss: 2.0290, Training Accuracy: 0.3657, Validation Accuracy: 0.2558\n","Epoch 71, Loss: 2.0395, Training Accuracy: 0.3524, Validation Accuracy: 0.2578\n","Epoch 72, Loss: 2.0263, Training Accuracy: 0.3634, Validation Accuracy: 0.2352\n","Epoch 73, Loss: 2.0374, Training Accuracy: 0.3618, Validation Accuracy: 0.2398\n","Epoch 74, Loss: 2.0332, Training Accuracy: 0.3564, Validation Accuracy: 0.2529\n","Epoch 75, Loss: 2.0104, Training Accuracy: 0.3593, Validation Accuracy: 0.2588\n","Epoch 76, Loss: 2.0228, Training Accuracy: 0.3642, Validation Accuracy: 0.2686\n","Epoch 00077: reducing learning rate of group 0 to 2.4500e-05.\n","Epoch 77, Loss: 1.9839, Training Accuracy: 0.3755, Validation Accuracy: 0.2689\n","Epoch 78, Loss: 1.9866, Training Accuracy: 0.3702, Validation Accuracy: 0.2594\n","Epoch 79, Loss: 1.9718, Training Accuracy: 0.3779, Validation Accuracy: 0.2614\n","Epoch 80, Loss: 1.9821, Training Accuracy: 0.3771, Validation Accuracy: 0.2535\n","Epoch 81, Loss: 1.9759, Training Accuracy: 0.3777, Validation Accuracy: 0.2614\n","Epoch 82, Loss: 1.9766, Training Accuracy: 0.3769, Validation Accuracy: 0.2496\n","Epoch 83, Loss: 1.9427, Training Accuracy: 0.3857, Validation Accuracy: 0.2712\n","Epoch 84, Loss: 1.9488, Training Accuracy: 0.3866, Validation Accuracy: 0.2617\n","Epoch 85, Loss: 1.9642, Training Accuracy: 0.3796, Validation Accuracy: 0.2604\n","Early stopping triggered.\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-ba00d0a7b2b2>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mASLDatasetLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_FILES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-7-323c854d0a57>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(loader, EPOCHS, LEARNING_RATE, K_FOLDS, AUGMENT)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mfold_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mfold_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable float object"]}],"source":["MAX_FILES = 20\n","directory_path = \"/content/drive/MyDrive/Colab Notebooks/DGMD E-14 Project/Datasets/processed-40-500\"\n","loader = ASLDatasetLoader(directory_path, max_files=MAX_FILES)\n","\n","model, all_preds, all_labels, accuracy = train(loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sMrYkTs89Q7O","executionInfo":{"status":"aborted","timestamp":1702168961446,"user_tz":420,"elapsed":12,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix, classification_report\n","import numpy as np\n","\n","# Convert lists to numpy arrays for compatibility with sklearn\n","y_true = np.array(all_labels)\n","y_pred = np.array(all_preds)\n","\n","# Calculate the confusion matrix\n","cm = confusion_matrix(y_true, y_pred)\n","\n","# Visualize the confusion matrix\n","plt.figure(figsize=(10, 7))\n","sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n","plt.title(\"Confusion Matrix\")\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"True\")\n","plt.show()\n","\n","# Ensure the class names are in the correct order for target_names\n","ordered_class_names = [name for name, num in sorted(loader.sign_to_label.items(), key=lambda item: item[1])]\n","\n","# Per-Class Accuracy\n","class_accuracy = cm.diagonal() / cm.sum(axis=1)\n","for i, acc in enumerate(class_accuracy):\n","    class_name = ordered_class_names[i]\n","    print(f\"Accuracy for class {i} ({class_name}): {acc*100:.2f}%\")\n","\n","# Detailed classification report\n","print(\"\\nClassification Report:\\n\")\n","print(classification_report(y_true, y_pred, target_names=ordered_class_names, zero_division=1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hl5tQmtN9Syt","executionInfo":{"status":"aborted","timestamp":1702168961446,"user_tz":420,"elapsed":12,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"outputs":[],"source":["from sklearn.metrics import classification_report\n","\n","def print_top_misclassified_classes(y_true, y_pred, sign_to_label, N=3, zero_division=1):\n","    \"\"\"\n","    Prints the top N classes that get misclassified the most.\n","\n","    Parameters:\n","    - y_true: Actual labels\n","    - y_pred: Predicted labels by the model\n","    - sign_to_label: Dictionary mapping class names to class numbers\n","    - N: Number of top misclassified classes to print\n","    - zero_division: Parameter for handling zero division in classification_report\n","\n","    Returns:\n","    None\n","    \"\"\"\n","\n","    # Ensure the class names are in the correct order for target_names\n","    ordered_class_names = [name for name, num in sorted(sign_to_label.items(), key=lambda item: item[1])]\n","\n","    # Generate and print classification report with class names\n","    print(\"\\nClassification Report:\\n\")\n","    print(classification_report(y_true, y_pred, target_names=ordered_class_names, zero_division=zero_division))\n","\n","    # Generate classification report as dict to find misclassified classes\n","    report = classification_report(y_true, y_pred, output_dict=True, zero_division=zero_division)\n","\n","    # Create a dictionary to store misclassification rates\n","    misclassification_rates = {}\n","\n","    # Iterate through each class in the report\n","    for class_num, metrics in report.items():\n","        if class_num.isdigit():\n","            class_name = [key for key, value in sign_to_label.items() if value == int(class_num)][0]\n","            misclassification_rates[class_name] = 1 - metrics['recall']\n","\n","    # Sort classes based on misclassification rate\n","    sorted_classes = sorted(misclassification_rates, key=misclassification_rates.get, reverse=True)\n","\n","    # Print top N misclassified classes\n","    print(f\"\\nTop {N} misclassified classes:\")\n","    for i in range(N):\n","        class_name = sorted_classes[i]\n","        print(f\"{i+1}. {class_name} - Misclassification rate: {misclassification_rates[class_name]:.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m4ZtqFxz9XlG","executionInfo":{"status":"aborted","timestamp":1702168961447,"user_tz":420,"elapsed":12,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"outputs":[],"source":["N = 10 if MAX_FILES > 50 else MAX_FILES\n","print_top_misclassified_classes(y_true, y_pred, loader.sign_to_label, N=N, zero_division=1)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1wIPH3B0fuuoSlPfXK5fGd9Em1LP-uQFJ","timestamp":1697014386160}],"machine_shape":"hm","gpuType":"V100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}