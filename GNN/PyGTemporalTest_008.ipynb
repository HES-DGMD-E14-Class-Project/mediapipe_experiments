{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "lLqJy6EZtrZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-ewruGCyaCn",
        "outputId": "ab3b1f34-cedb-43e3-de9c-f65b2de748c3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Google-drive virtual environment using `virtualenv`"
      ],
      "metadata": {
        "id": "re09tb8stvBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Install Pytorch Geometric Temporal"
      ],
      "metadata": {
        "id": "yLRvBRGgt5Zk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiSS4wuxZ01m",
        "outputId": "6c790104-961b-4b10-b5e5-7519bc77f890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n",
            "11.8\n",
            "Collecting torch-geometric-temporal\n",
            "  Downloading torch_geometric_temporal-0.54.0.tar.gz (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (4.4.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (2.0.1+cu118)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (3.0.2)\n",
            "Collecting pandas<=1.3.5 (from torch-geometric-temporal)\n",
            "  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse (from torch-geometric-temporal)\n",
            "  Downloading torch_sparse-0.6.18.tar.gz (209 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch_scatter (from torch-geometric-temporal)\n",
            "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch_geometric (from torch-geometric-temporal)\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (1.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.10/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2023.3.post1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torch-geometric-temporal) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torch-geometric-temporal) (17.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (4.66.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (1.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torch-geometric-temporal) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (2023.7.22)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric->torch-geometric-temporal) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric->torch-geometric-temporal) (3.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torch-geometric-temporal) (1.3.0)\n",
            "Building wheels for collected packages: torch-geometric-temporal, torch_geometric, torch_scatter, torch_sparse\n",
            "  Building wheel for torch-geometric-temporal (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric-temporal: filename=torch_geometric_temporal-0.54.0-py3-none-any.whl size=86722 sha256=5bcb3b5c77bd85bc4c23b2ce1e553f72270e8bfd678d81c1511d6d0a5d155ac7\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/9b/b6/e15256e053f0cb49b1084a67a709db909d418386a231f0722c\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910454 sha256=c2bee5a3c6c0592fb923f3fde41dfde564c379a40ab4f93db53e98568d384b45\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "  Building wheel for torch_scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_scatter: filename=torch_scatter-2.1.2-cp310-cp310-linux_x86_64.whl size=494366 sha256=21d7e46434c0798b7ea68c1eb407ce9a61068562405b2c5fc3d590c069b6de59\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/f1/2b/3b46d54b134259f58c8363568569053248040859b1a145b3ce\n",
            "  Building wheel for torch_sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_sparse: filename=torch_sparse-0.6.18-cp310-cp310-linux_x86_64.whl size=1054272 sha256=92f4d4299cce720bd28c0f798d2b161b26c4f7d8a2b5a0cda41a776657902418\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/dd/0f/a6a16f9f3b0236733d257b4b4ea91b548b984a341ed3b8f38c\n",
            "Successfully built torch-geometric-temporal torch_geometric torch_scatter torch_sparse\n",
            "Installing collected packages: torch_scatter, torch_sparse, pandas, torch_geometric, torch-geometric-temporal\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 1.3.5 which is incompatible.\n",
            "plotnine 0.12.3 requires pandas>=1.5.0, but you have pandas 1.3.5 which is incompatible.\n",
            "xarray 2023.7.0 requires pandas>=1.4, but you have pandas 1.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-1.3.5 torch-geometric-temporal-0.54.0 torch_geometric-2.3.1 torch_scatter-2.1.2 torch_sparse-0.6.18\n"
          ]
        }
      ],
      "source": [
        "!python -c \"import torch; print(torch.__version__)\"\n",
        "!python -c \"import torch; print(torch.version.cuda)\"\n",
        "!pip install torch-geometric-temporal"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `ASLDatasetLoader` Class\n",
        "\n",
        "The `ASLDatasetLoader` class is designed for loading and processing the ASL dataset. Given a directory, it reads sign language data from JSON files and constructs graph representations suitable for graph-based neural networks. Crucially, the class converts JSON data into PyTorch Geometric (PyG) `Data` objects comprising `x` (node features), `edge_index` (graph connectivity), and `y' (labels) attributes.\n",
        "\n",
        "**Methods**:\n",
        "\n",
        "- `_create_sign_to_label_map`: Generates a mapping from sign names to unique labels.\n",
        "\n",
        "- `_read_file_data`: Reads data from a given JSON file.\n",
        "\n",
        "- `_augment_data`: Implements data augmentation by applying random rotation, translation, and scaling to landmarks, which can enhance the model's robustness.\n",
        "\n",
        "- `_create_graph_from_frame`: Constructs a PyG `Data` object from frame data, concentrating on hand and face landmarks. Edges are created between consecutive landmarks and between left and right hand landmarks. Additional features, like hand-to-face distances, are also computed.\n",
        "\n",
        "- `get_dataset`: Assembles the dataset, optionally incorporating data augmentation. The function outputs a list of PyG `Data` objects ready for graph neural network processing."
      ],
      "metadata": {
        "id": "zi7c7EMpsCBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "HAND_TO_FACE_THRESHOLD = 0.05\n",
        "\n",
        "class ASLDatasetLoader:\n",
        "    def __init__(self, directory_path):\n",
        "        self.directory_path = directory_path\n",
        "        self.sign_to_label = self._create_sign_to_label_map()\n",
        "\n",
        "    def _create_sign_to_label_map(self):\n",
        "        signs = [os.path.splitext(filename)[0] for filename in os.listdir(self.directory_path)]\n",
        "        return {sign: i for i, sign in enumerate(signs)}\n",
        "\n",
        "    def _read_file_data(self, file_path):\n",
        "        with open(file_path, 'r') as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    def _augment_data(self, frame_data, rotation_range=10, translation_range=0.05, scaling_range=0.1):\n",
        "        \"\"\"\n",
        "        Augment the frame data with random rotation, translation, and scaling.\n",
        "\n",
        "        :param frame_data: Dictionary containing frame landmarks and deltas.\n",
        "        :param rotation_range: Maximum rotation angle in degrees.\n",
        "        :param translation_range: Maximum translation as a fraction of landmark range.\n",
        "        :param scaling_range: Maximum scaling factor.\n",
        "        :return: Augmented frame data.\n",
        "        \"\"\"\n",
        "        landmarks = np.array(frame_data[\"landmarks\"])\n",
        "        centroid = np.mean(landmarks, axis=0)\n",
        "\n",
        "        # Random rotation\n",
        "        theta = np.radians(np.random.uniform(-rotation_range, rotation_range))\n",
        "        rotation_matrix = np.array([\n",
        "            [np.cos(theta), -np.sin(theta)],\n",
        "            [np.sin(theta), np.cos(theta)]\n",
        "        ])\n",
        "        landmarks = np.dot(landmarks - centroid, rotation_matrix) + centroid\n",
        "\n",
        "        # Random translation\n",
        "        max_translation = translation_range * (landmarks.max(axis=0) - landmarks.min(axis=0))\n",
        "        translations = np.random.uniform(-max_translation, max_translation)\n",
        "        landmarks += translations\n",
        "\n",
        "        # Random scaling\n",
        "        scale = np.random.uniform(1 - scaling_range, 1 + scaling_range)\n",
        "        landmarks = centroid + scale * (landmarks - centroid)\n",
        "\n",
        "        frame_data[\"landmarks\"] = landmarks.tolist()\n",
        "        return frame_data\n",
        "\n",
        "    def _calculate_dominant_hand(self, sign_data):\n",
        "        \"\"\"\n",
        "        Determine the dominant hand in a sign language data sample.\n",
        "\n",
        "        This function analyzes the motion of both hands throughout the frames in\n",
        "        a given sign language data sample. The dominant hand is determined based\n",
        "        on the average magnitude and frequency of motion. The hand with the\n",
        "        higher average magnitude or higher motion event frequency is considered\n",
        "        dominant.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            A string indicating the dominant hand. Possible return values are\n",
        "            \"left\", \"right\", or \"ambiguous\" if no clear dominant hand can be\n",
        "            determined.\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        The function assumes that the order of landmarks and deltas is consistent\n",
        "        across frames and that hands' landmarks are distinguishable in the\n",
        "        landmark data (e.g., by their order or a separate landmark type identifier).\n",
        "\n",
        "        The decision criterion for dominant hand detection is heuristic and may\n",
        "        require adjustment based on empirical results and specific use case needs.\n",
        "        \"\"\"\n",
        "        left_hand_motion = 0\n",
        "        right_hand_motion = 0\n",
        "        left_hand_motion_events = 0\n",
        "        right_hand_motion_events = 0\n",
        "\n",
        "        for frame_data in sign_data[\"frames\"]:\n",
        "            landmarks = np.array(frame_data[\"landmarks\"])\n",
        "            deltas = np.array(frame_data[\"deltas\"])\n",
        "            landmark_types = frame_data[\"landmark_types\"]\n",
        "\n",
        "            for delta, ltype in zip(deltas, landmark_types):\n",
        "                motion_magnitude = np.linalg.norm(delta)\n",
        "\n",
        "                if ltype == \"L\":\n",
        "                    left_hand_motion += motion_magnitude\n",
        "                    if motion_magnitude > 0.5:  # Threshold may need adjustment\n",
        "                        left_hand_motion_events += 1\n",
        "\n",
        "                elif ltype == \"R\":\n",
        "                    right_hand_motion += motion_magnitude\n",
        "                    if motion_magnitude > 0.5:  # Threshold may need adjustment\n",
        "                        right_hand_motion_events += 1\n",
        "\n",
        "        # Combine motion magnitude and motion events to determine the dominant hand\n",
        "        # Weights (0.5 and 0.5) might need adjustment based on empirical observation\n",
        "        left_hand_score = 0.5 * left_hand_motion + 0.5 * left_hand_motion_events\n",
        "        right_hand_score = 0.5 * right_hand_motion + 0.5 * right_hand_motion_events\n",
        "\n",
        "        return \"left\" if left_hand_score > right_hand_score else \"right\"\n",
        "\n",
        "\n",
        "    def _create_graph_from_frame(self, sign_name, frame_data, sign_data, landmark_types):\n",
        "        # Calculate dominant hand\n",
        "        dominant_hand = self._calculate_dominant_hand(sign_data)\n",
        "\n",
        "        # Extract landmark and delta information\n",
        "        landmarks = np.array(frame_data[\"landmarks\"])\n",
        "        deltas = np.array(frame_data[\"deltas\"])\n",
        "\n",
        "        # Add dominant hand information to node features\n",
        "        dominant_hand_feature = [\n",
        "            1 if ((t == \"L\" and dominant_hand == \"left\") or (t == \"R\" and dominant_hand == \"right\")) else 0\n",
        "            for t in landmark_types\n",
        "        ]\n",
        "        dominant_hand_feature_2d = np.array(dominant_hand_feature)[:, np.newaxis]\n",
        "\n",
        "        # Compute additional features like hand-to-face and hand-to-body distances\n",
        "        # ... (If you have additional feature creation logic, add here)\n",
        "        hand_to_face_contact = [0] * len(landmark_types) # replace this line with actual feature creation if used\n",
        "        hand_to_face_contact_2d = np.array(hand_to_face_contact)[:, np.newaxis]\n",
        "\n",
        "        # Create weights based on landmark importance\n",
        "        weights = [2 if t == \"L\" or t == \"R\" else 1 for t in landmark_types]\n",
        "        weights_2d = np.array(weights)[:, np.newaxis]\n",
        "\n",
        "        # Concatenate landmarks, deltas, importance weights, hand-to-face contact features, and dominant hand feature\n",
        "        x = torch.tensor(np.hstack((landmarks, deltas, weights_2d, hand_to_face_contact_2d, dominant_hand_feature_2d)), dtype=torch.float)\n",
        "        y = torch.tensor([self.sign_to_label[sign_name]], dtype=torch.long)\n",
        "\n",
        "        # Create edges based on the number of available landmarks (or nodes)\n",
        "        # You might have specific logic to determine edges based on landmark types or spatial proximity\n",
        "        edges = [[i, i + 1] for i in range(len(landmarks) - 1)]\n",
        "\n",
        "        # Add edges between the left and right hand landmarks\n",
        "        for i, t1 in enumerate(landmark_types):\n",
        "            for j, t2 in enumerate(landmark_types):\n",
        "                if t1 in [\"L\", \"R\"] and t2 in [\"L\", \"R\"] and i != j:\n",
        "                    edges.append([i, j])\n",
        "\n",
        "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "\n",
        "        return Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "    def get_dataset(self, augment=False):\n",
        "      dataset = []\n",
        "\n",
        "      for filename in os.listdir(self.directory_path):\n",
        "          sign_name = os.path.splitext(filename)[0]\n",
        "          file_path = os.path.join(self.directory_path, filename)\n",
        "          sign_data = self._read_file_data(file_path)\n",
        "\n",
        "          for frame_data in sign_data[\"frames\"]:\n",
        "              landmark_types = sign_data.get(\"landmark_types\", [\"F\", \"L\", \"P\", \"R\"])  # defaulting to all types\n",
        "\n",
        "              if augment:\n",
        "                  frame_data = self._augment_data(frame_data)\n",
        "              graph_data = self._create_graph_from_frame(sign_name, frame_data, sign_data, landmark_types)\n",
        "\n",
        "              dataset.append(graph_data)\n",
        "\n",
        "      return dataset\n",
        "\n",
        "    def number_of_classes(self):\n",
        "        return len(self.sign_to_label)"
      ],
      "metadata": {
        "id": "LifBvD3D4t4C"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `ASLGraphClassifier` Class\n",
        "\n",
        "The `ASLGraphClassifier`, features deeper GCN layers and additional channels to capture intricate data patterns potentially. It takes a PyG `Data` object as input, and its forward pass emits class logits.\n",
        "\n",
        "**Methods**:\n",
        "\n",
        "- `forward`: Details the forward pass, accepting a PyG `Data` object. Two GCN layers with subsequent batch normalization and dropout layers process the input. Post global max-pooling, two linear layers coupled with dropout ensure final classification, leading to log-softmax outputs."
      ],
      "metadata": {
        "id": "xooW2sogtdL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_max_pool, global_mean_pool\n",
        "\n",
        "class ASLGraphClassifier(torch.nn.Module):\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super(ASLGraphClassifier, self).__init__()\n",
        "        self.conv1 = GCNConv(num_features, 256)  # Increased channels\n",
        "        self.bn1 = torch.nn.BatchNorm1d(256)    # Batch normalization layer\n",
        "        self.conv2 = GCNConv(256, 512)          # Increased channels\n",
        "        self.bn2 = torch.nn.BatchNorm1d(512)    # Batch normalization layer\n",
        "        self.lin1 = torch.nn.Linear(512, 256)\n",
        "        self.lin2 = torch.nn.Linear(256, num_classes)\n",
        "        self.dropout = torch.nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
        "        x = self.dropout(x)\n",
        "        x = global_max_pool(x, batch)\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.lin2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "RAslUK79VVV6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch_geometric.loader import DataLoader\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "EPOCHS = 200\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "\n",
        "def stratified_data_split(data_list, test_size=0.2):\n",
        "    \"\"\"\n",
        "    This function splits a dataset into training and testing subsets, preserving\n",
        "    the class distribution by leveraging the stratification capabilities of\n",
        "    `train_test_split` from `sklearn`. Stratification helps with potential class\n",
        "    imbalances.\n",
        "    \"\"\"\n",
        "    # Extract labels from data list\n",
        "    labels = [data.y.item() for data in data_list]\n",
        "\n",
        "    # Use sklearn's train_test_split with stratify option\n",
        "    train_data, test_data = train_test_split(data_list, test_size=test_size, stratify=labels, random_state=42)\n",
        "\n",
        "    return train_data, test_data\n",
        "\n",
        "\n",
        "def validate(loader, model, device):\n",
        "    \"\"\"\n",
        "    Used to evaluate the model on validation/test data, computing accuracy as a\n",
        "    performance metric, and offering insights into the model's efficacy.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        with torch.no_grad():\n",
        "            out = model(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += int((pred == data.y).sum())\n",
        "    return correct / len(loader.dataset)\n",
        "\n",
        "def train():\n",
        "    \"\"\"\n",
        "    The `train` function establishes the training loop for the graph-based\n",
        "    neural network. It enacts typical training loop tasks like logging\n",
        "    epoch-wise loss, validation, and early stopping.\n",
        "\n",
        "    The function also harnesses schedulers, regularization techniques, and\n",
        "    gradient clipping to ensure smooth and optimal training.\n",
        "    \"\"\"\n",
        "    directory_path = \"/content/drive/MyDrive/Colab Notebooks/DGMD E-14 Project/Datasets/ASL\"\n",
        "    loader = ASLDatasetLoader(directory_path)\n",
        "\n",
        "    # Create the entire dataset without augmentation and then perform stratified split\n",
        "    data_list = loader.get_dataset()\n",
        "    train_dataset, test_dataset = stratified_data_split(data_list, test_size=0.2)\n",
        "\n",
        "    # Now augment only the training dataset\n",
        "    augmented_train_dataset = loader.get_dataset(augment=True)\n",
        "\n",
        "    num_classes = loader.number_of_classes()\n",
        "\n",
        "    train_labels = [data.y.item() for data in train_dataset]\n",
        "    test_labels = [data.y.item() for data in test_dataset]\n",
        "\n",
        "    print(\"Training label distribution:\", Counter(train_labels))\n",
        "    print(\"Test label distribution:\", Counter(test_labels))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    num_features = train_dataset[0].x.size(1)\n",
        "    model = ASLGraphClassifier(num_features=num_features, num_classes=num_classes).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=5e-4)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.7, patience=5, verbose=True)\n",
        "\n",
        "    max_epochs_without_improvement = 20\n",
        "    epochs_without_improvement = 0\n",
        "    best_val_accuracy = 0\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(EPOCHS):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            batch = batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(batch)\n",
        "            loss = F.nll_loss(out, batch.y)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Check for NaN loss\n",
        "            if np.isnan(loss.item()):\n",
        "                print(\"Warning: NaN loss detected!\")\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch}, Loss: {avg_loss}\")\n",
        "\n",
        "        val_accuracy = validate(test_loader, model, device)\n",
        "        scheduler.step(val_accuracy)\n",
        "\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "\n",
        "        if epochs_without_improvement >= max_epochs_without_improvement:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in test_loader:\n",
        "        batch = batch.to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(batch).max(dim=1)[1]\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(batch.y.cpu().numpy())\n",
        "            correct += pred.eq(batch.y).sum().item()\n",
        "\n",
        "    print(f\"Accuracy: {correct / len(test_dataset)}\")\n",
        "    print(\"Sample predictions:\", all_preds[:20])\n",
        "    print(\"Sample true labels:\", all_labels[:20])"
      ],
      "metadata": {
        "id": "RdBGsFveWcbF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rr2PlwLy5M6H",
        "outputId": "13fe246b-f4c5-4cc0-c574-b9340036c343"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training label distribution: Counter({190: 438, 145: 430, 88: 430, 93: 420, 208: 402, 121: 400, 209: 363, 76: 346, 56: 338, 92: 338, 180: 332, 34: 330, 16: 324, 172: 323, 182: 316, 31: 310, 24: 309, 195: 308, 183: 307, 192: 306, 148: 304, 215: 298, 10: 297, 150: 297, 130: 296, 29: 295, 113: 295, 151: 295, 79: 294, 143: 293, 149: 292, 44: 290, 64: 289, 106: 288, 89: 286, 210: 282, 1: 279, 147: 279, 169: 278, 107: 278, 191: 278, 52: 277, 114: 277, 13: 276, 77: 275, 98: 274, 4: 274, 19: 274, 170: 273, 205: 273, 81: 271, 206: 270, 154: 270, 199: 270, 46: 270, 212: 269, 65: 269, 32: 268, 2: 266, 211: 265, 55: 265, 36: 265, 152: 264, 7: 263, 14: 263, 35: 263, 60: 262, 73: 262, 139: 262, 119: 262, 204: 261, 133: 261, 94: 261, 43: 260, 216: 258, 68: 258, 66: 257, 178: 256, 125: 255, 30: 255, 138: 255, 179: 254, 157: 254, 59: 253, 33: 253, 181: 252, 47: 251, 53: 250, 126: 250, 120: 250, 186: 250, 129: 249, 15: 249, 142: 247, 219: 247, 40: 247, 62: 247, 164: 247, 41: 247, 162: 246, 102: 246, 213: 246, 124: 244, 100: 244, 72: 243, 20: 243, 217: 243, 203: 242, 218: 242, 207: 242, 96: 241, 137: 241, 166: 241, 174: 240, 161: 240, 91: 239, 134: 239, 104: 238, 135: 238, 202: 238, 184: 238, 158: 237, 194: 237, 214: 236, 197: 236, 165: 236, 168: 235, 18: 235, 122: 234, 67: 234, 85: 234, 45: 234, 78: 234, 37: 234, 42: 234, 185: 234, 82: 233, 6: 233, 187: 231, 111: 230, 176: 230, 144: 230, 26: 229, 198: 229, 127: 229, 69: 229, 167: 227, 153: 226, 84: 226, 90: 226, 70: 226, 0: 225, 140: 224, 156: 224, 163: 224, 118: 223, 22: 223, 141: 223, 220: 223, 38: 222, 58: 222, 5: 222, 71: 222, 87: 222, 200: 222, 101: 222, 12: 221, 222: 220, 57: 219, 95: 219, 11: 218, 17: 218, 25: 218, 175: 218, 8: 217, 49: 216, 109: 215, 225: 215, 146: 214, 223: 214, 115: 214, 99: 213, 48: 212, 27: 211, 50: 211, 171: 210, 221: 210, 123: 209, 54: 209, 132: 209, 51: 209, 21: 208, 160: 208, 75: 208, 39: 207, 201: 206, 189: 206, 188: 206, 9: 206, 105: 205, 177: 204, 136: 204, 112: 204, 86: 204, 226: 203, 23: 202, 196: 202, 108: 202, 116: 202, 155: 201, 80: 201, 224: 199, 173: 199, 227: 199, 83: 198, 128: 198, 117: 198, 61: 197, 131: 197, 74: 197, 63: 196, 193: 195, 28: 194, 3: 193, 110: 193, 103: 188, 97: 185, 159: 182})\n",
            "Test label distribution: Counter({190: 109, 145: 108, 88: 107, 93: 105, 121: 100, 208: 100, 209: 91, 76: 86, 92: 85, 56: 84, 34: 83, 180: 83, 16: 81, 172: 81, 182: 79, 31: 78, 195: 77, 192: 77, 24: 77, 183: 77, 148: 76, 215: 75, 113: 74, 130: 74, 151: 74, 10: 74, 79: 74, 150: 74, 29: 74, 143: 73, 44: 73, 149: 73, 64: 72, 106: 72, 89: 72, 1: 70, 147: 70, 210: 70, 169: 70, 107: 70, 77: 69, 191: 69, 19: 69, 13: 69, 114: 69, 52: 69, 205: 68, 170: 68, 98: 68, 154: 68, 81: 68, 4: 68, 212: 67, 65: 67, 206: 67, 32: 67, 2: 67, 199: 67, 46: 67, 14: 66, 73: 66, 139: 66, 55: 66, 35: 66, 60: 66, 7: 66, 211: 66, 152: 66, 36: 66, 119: 65, 216: 65, 133: 65, 204: 65, 43: 65, 94: 65, 178: 64, 68: 64, 125: 64, 66: 64, 138: 64, 157: 64, 30: 64, 33: 63, 186: 63, 181: 63, 47: 63, 120: 63, 179: 63, 53: 63, 59: 63, 41: 62, 219: 62, 142: 62, 164: 62, 15: 62, 40: 62, 129: 62, 213: 62, 126: 62, 102: 62, 62: 62, 162: 62, 72: 61, 124: 61, 20: 61, 217: 61, 203: 61, 100: 61, 207: 61, 218: 61, 134: 60, 91: 60, 96: 60, 166: 60, 161: 60, 137: 60, 184: 60, 174: 60, 165: 59, 122: 59, 18: 59, 158: 59, 194: 59, 67: 59, 202: 59, 135: 59, 104: 59, 214: 59, 42: 59, 168: 59, 85: 59, 197: 59, 6: 58, 37: 58, 187: 58, 78: 58, 82: 58, 185: 58, 45: 58, 167: 57, 198: 57, 144: 57, 176: 57, 111: 57, 127: 57, 153: 57, 69: 57, 26: 57, 38: 56, 163: 56, 0: 56, 84: 56, 58: 56, 156: 56, 118: 56, 87: 56, 141: 56, 140: 56, 22: 56, 70: 56, 200: 56, 220: 56, 90: 56, 175: 55, 12: 55, 222: 55, 95: 55, 71: 55, 101: 55, 5: 55, 17: 55, 57: 55, 49: 54, 25: 54, 146: 54, 11: 54, 109: 54, 8: 54, 225: 54, 171: 53, 115: 53, 27: 53, 50: 53, 99: 53, 223: 53, 48: 53, 21: 52, 51: 52, 75: 52, 39: 52, 9: 52, 132: 52, 160: 52, 54: 52, 123: 52, 221: 52, 196: 51, 177: 51, 226: 51, 112: 51, 105: 51, 201: 51, 86: 51, 136: 51, 189: 51, 188: 51, 116: 51, 108: 51, 173: 50, 224: 50, 155: 50, 128: 50, 23: 50, 227: 50, 80: 50, 74: 49, 63: 49, 131: 49, 28: 49, 117: 49, 61: 49, 83: 49, 110: 48, 3: 48, 193: 48, 103: 47, 97: 46, 159: 45})\n",
            "Epoch 0, Loss: 4.631016983950898\n",
            "Epoch 1, Loss: 3.3574174254173483\n",
            "Epoch 2, Loss: 3.004709682109193\n",
            "Epoch 3, Loss: 2.770490141523416\n",
            "Epoch 4, Loss: 2.6372143824446477\n",
            "Epoch 5, Loss: 2.5533200558937583\n",
            "Epoch 6, Loss: 2.476183716824067\n",
            "Epoch 7, Loss: 2.4345988208193488\n",
            "Epoch 8, Loss: 2.3847283393497993\n",
            "Epoch 9, Loss: 2.340844018993776\n",
            "Epoch 10, Loss: 2.3140834821004237\n",
            "Epoch 11, Loss: 2.2880562608488395\n",
            "Epoch 12, Loss: 2.2489216205832783\n",
            "Epoch 13, Loss: 2.226747690789919\n",
            "Epoch 14, Loss: 2.190625554690046\n",
            "Epoch 15, Loss: 2.16269722999926\n",
            "Epoch 16, Loss: 2.1260683762024244\n",
            "Epoch 17, Loss: 2.0531478101153082\n",
            "Epoch 18, Loss: 1.9459900206458696\n",
            "Epoch 19, Loss: 1.8860628882886863\n",
            "Epoch 20, Loss: 1.848669774043634\n",
            "Epoch 21, Loss: 1.825163872480258\n",
            "Epoch 22, Loss: 1.816063911883862\n",
            "Epoch 23, Loss: 1.7988672733037638\n",
            "Epoch 24, Loss: 1.7898182726929512\n",
            "Epoch 25, Loss: 1.7788428376785852\n",
            "Epoch 26, Loss: 1.77161519713216\n",
            "Epoch 27, Loss: 1.7562562455436324\n",
            "Epoch 28, Loss: 1.749736795413299\n",
            "Epoch 29, Loss: 1.745809462903786\n",
            "Epoch 30, Loss: 1.733276866742408\n",
            "Epoch 31, Loss: 1.7329503336173542\n",
            "Epoch 32, Loss: 1.7207437963017227\n",
            "Epoch 33, Loss: 1.709353632370945\n",
            "Epoch 34, Loss: 1.7007725209519393\n",
            "Epoch 35, Loss: 1.7056028453526964\n",
            "Epoch 36, Loss: 1.6848186136032484\n",
            "Epoch 37, Loss: 1.6935134094896322\n",
            "Epoch 00038: reducing learning rate of group 0 to 7.0000e-04.\n",
            "Epoch 38, Loss: 1.5763634860616562\n",
            "Epoch 39, Loss: 1.54736564140546\n",
            "Epoch 40, Loss: 1.5432733161378083\n",
            "Epoch 41, Loss: 1.5309686137416953\n",
            "Epoch 42, Loss: 1.5261085479021208\n",
            "Epoch 43, Loss: 1.5261779138423979\n",
            "Epoch 44, Loss: 1.5242261911838624\n",
            "Epoch 45, Loss: 1.5148445110681836\n",
            "Epoch 46, Loss: 1.5120117830597433\n",
            "Epoch 47, Loss: 1.5020620166952499\n",
            "Epoch 48, Loss: 1.495692391640584\n",
            "Epoch 49, Loss: 1.496608827862613\n",
            "Epoch 50, Loss: 1.4942675293131376\n",
            "Epoch 51, Loss: 1.484597606621241\n",
            "Epoch 52, Loss: 1.4879075483640394\n",
            "Epoch 53, Loss: 1.4691958735044361\n",
            "Epoch 54, Loss: 1.4708165489705116\n",
            "Epoch 55, Loss: 1.4731250288188962\n",
            "Epoch 56, Loss: 1.4731428923410321\n",
            "Epoch 57, Loss: 1.463691746024073\n",
            "Epoch 58, Loss: 1.458868831816802\n",
            "Epoch 59, Loss: 1.4543694929171929\n",
            "Epoch 60, Loss: 1.447113712045313\n",
            "Epoch 61, Loss: 1.449546401915343\n",
            "Epoch 62, Loss: 1.4524846220676135\n",
            "Epoch 63, Loss: 1.4517364200732588\n",
            "Epoch 64, Loss: 1.441289547168219\n",
            "Epoch 65, Loss: 1.437570704099891\n",
            "Epoch 66, Loss: 1.4374621916059585\n",
            "Epoch 67, Loss: 1.4314099952435506\n",
            "Epoch 68, Loss: 1.4294343698098793\n",
            "Epoch 69, Loss: 1.4265302562027042\n",
            "Epoch 00070: reducing learning rate of group 0 to 4.9000e-04.\n",
            "Epoch 70, Loss: 1.3504705053140456\n",
            "Epoch 71, Loss: 1.3241765941017298\n",
            "Epoch 72, Loss: 1.3218696393716274\n",
            "Epoch 73, Loss: 1.3167900911567576\n",
            "Epoch 74, Loss: 1.3049200755401495\n",
            "Epoch 75, Loss: 1.3012199956168569\n",
            "Epoch 76, Loss: 1.3010325535147422\n",
            "Epoch 77, Loss: 1.2957637595766887\n",
            "Epoch 00078: reducing learning rate of group 0 to 3.4300e-04.\n",
            "Epoch 78, Loss: 1.2300187501780802\n",
            "Epoch 79, Loss: 1.22104654421179\n",
            "Epoch 80, Loss: 1.2113716822435194\n",
            "Epoch 81, Loss: 1.2098333060505833\n",
            "Epoch 82, Loss: 1.2043403049164205\n",
            "Epoch 83, Loss: 1.2019500224890511\n",
            "Epoch 84, Loss: 1.2019188902527516\n",
            "Epoch 85, Loss: 1.1949918969144664\n",
            "Epoch 00086: reducing learning rate of group 0 to 2.4010e-04.\n",
            "Epoch 86, Loss: 1.1517334871208647\n",
            "Epoch 87, Loss: 1.1420986968403413\n",
            "Epoch 88, Loss: 1.1354008542320138\n",
            "Epoch 89, Loss: 1.1328074022210433\n",
            "Epoch 90, Loss: 1.1300317123069525\n",
            "Epoch 91, Loss: 1.1270420861607686\n",
            "Epoch 92, Loss: 1.1257292767623803\n",
            "Epoch 93, Loss: 1.120893041304045\n",
            "Epoch 94, Loss: 1.1215454080157332\n",
            "Epoch 95, Loss: 1.1175648317977844\n",
            "Epoch 96, Loss: 1.11726663664261\n",
            "Epoch 97, Loss: 1.1146621858582262\n",
            "Epoch 98, Loss: 1.1149188631136897\n",
            "Epoch 99, Loss: 1.1119415463699855\n",
            "Epoch 100, Loss: 1.1102506893627526\n",
            "Epoch 101, Loss: 1.1077668025357652\n",
            "Epoch 102, Loss: 1.1105951365274873\n",
            "Epoch 103, Loss: 1.1067330964805715\n",
            "Epoch 00104: reducing learning rate of group 0 to 1.6807e-04.\n",
            "Epoch 104, Loss: 1.0726437062815986\n",
            "Epoch 105, Loss: 1.0665356830679582\n",
            "Epoch 106, Loss: 1.065748977213509\n",
            "Epoch 107, Loss: 1.0639796977928975\n",
            "Epoch 108, Loss: 1.06041726103144\n",
            "Epoch 109, Loss: 1.0609879257785337\n",
            "Epoch 110, Loss: 1.0582735829606424\n",
            "Epoch 111, Loss: 1.0560210138677137\n",
            "Epoch 112, Loss: 1.0536863169281043\n",
            "Epoch 113, Loss: 1.052617260690869\n",
            "Epoch 00114: reducing learning rate of group 0 to 1.1765e-04.\n",
            "Epoch 114, Loss: 1.0281634426332342\n",
            "Epoch 115, Loss: 1.02419711412985\n",
            "Epoch 116, Loss: 1.0230120577292547\n",
            "Epoch 117, Loss: 1.0208042631170833\n",
            "Epoch 118, Loss: 1.0193310758779441\n",
            "Epoch 119, Loss: 1.0163367137203507\n",
            "Epoch 120, Loss: 1.0163891265817715\n",
            "Epoch 121, Loss: 1.014732869902551\n",
            "Epoch 00122: reducing learning rate of group 0 to 8.2354e-05.\n",
            "Epoch 122, Loss: 0.9960674927572555\n",
            "Epoch 123, Loss: 0.9930332067455698\n",
            "Epoch 124, Loss: 0.9928700398045835\n",
            "Epoch 125, Loss: 0.9900584102249091\n",
            "Epoch 126, Loss: 0.9903984880763753\n",
            "Epoch 127, Loss: 0.988678008974238\n",
            "Epoch 128, Loss: 0.9876827576078311\n",
            "Epoch 129, Loss: 0.98818501565902\n",
            "Epoch 130, Loss: 0.9860562131372567\n",
            "Epoch 131, Loss: 0.9843510460510286\n",
            "Epoch 132, Loss: 0.9832300099695153\n",
            "Epoch 133, Loss: 0.9824983869684005\n",
            "Epoch 134, Loss: 0.9823405288963383\n",
            "Epoch 135, Loss: 0.9826853466969571\n",
            "Epoch 136, Loss: 0.9821166597403489\n",
            "Epoch 137, Loss: 0.9798954519964355\n",
            "Epoch 138, Loss: 0.9801586226411353\n",
            "Epoch 139, Loss: 0.9785750468426139\n",
            "Epoch 140, Loss: 0.9782676651510124\n",
            "Epoch 141, Loss: 0.9767808611954356\n",
            "Epoch 142, Loss: 0.976959148582801\n",
            "Epoch 143, Loss: 0.9763505953573898\n",
            "Epoch 144, Loss: 0.9760485224674005\n",
            "Epoch 145, Loss: 0.9745089607949583\n",
            "Epoch 146, Loss: 0.9727915458629388\n",
            "Epoch 147, Loss: 0.9729440529793417\n",
            "Epoch 148, Loss: 0.9724992961112151\n",
            "Epoch 149, Loss: 0.9724876873578008\n",
            "Epoch 00150: reducing learning rate of group 0 to 5.7648e-05.\n",
            "Epoch 150, Loss: 0.9598070604864452\n",
            "Epoch 151, Loss: 0.9576185855032158\n",
            "Epoch 152, Loss: 0.9574658888702942\n",
            "Epoch 153, Loss: 0.9564944922351891\n",
            "Epoch 154, Loss: 0.9551793834302871\n",
            "Epoch 155, Loss: 0.955393851796225\n",
            "Epoch 156, Loss: 0.9539367261746049\n",
            "Epoch 157, Loss: 0.9539026137060664\n",
            "Epoch 158, Loss: 0.9526166770836514\n",
            "Epoch 159, Loss: 0.9528511594507668\n",
            "Epoch 160, Loss: 0.95172425828567\n",
            "Epoch 161, Loss: 0.9505722138216642\n",
            "Epoch 162, Loss: 0.9509781741577646\n",
            "Epoch 163, Loss: 0.9504523917418426\n",
            "Epoch 164, Loss: 0.9496556234218386\n",
            "Epoch 165, Loss: 0.9488065325224353\n",
            "Epoch 00166: reducing learning rate of group 0 to 4.0354e-05.\n",
            "Epoch 166, Loss: 0.9392297424077315\n",
            "Epoch 167, Loss: 0.9386680372215004\n",
            "Epoch 168, Loss: 0.9386721182549357\n",
            "Epoch 169, Loss: 0.9378478783863806\n",
            "Epoch 170, Loss: 0.9372928487857407\n",
            "Epoch 171, Loss: 0.9365510374459084\n",
            "Epoch 172, Loss: 0.9357990481838676\n",
            "Epoch 173, Loss: 0.9352890727110733\n",
            "Epoch 174, Loss: 0.9349406783456926\n",
            "Epoch 175, Loss: 0.9341026479110147\n",
            "Epoch 00176: reducing learning rate of group 0 to 2.8248e-05.\n",
            "Epoch 176, Loss: 0.9272859421306785\n",
            "Epoch 177, Loss: 0.9262831885554843\n",
            "Epoch 178, Loss: 0.925721932221509\n",
            "Epoch 179, Loss: 0.9248953442799716\n",
            "Epoch 180, Loss: 0.9244591644899378\n",
            "Epoch 181, Loss: 0.9243385254125148\n",
            "Epoch 182, Loss: 0.9246623792734474\n",
            "Epoch 00183: reducing learning rate of group 0 to 1.9773e-05.\n",
            "Epoch 183, Loss: 0.9186842747154645\n",
            "Epoch 184, Loss: 0.9183206685615218\n",
            "Epoch 185, Loss: 0.9180729956293026\n",
            "Epoch 186, Loss: 0.917529339476671\n",
            "Epoch 187, Loss: 0.917135221348315\n",
            "Epoch 188, Loss: 0.917367578209086\n",
            "Epoch 189, Loss: 0.9171178852986778\n",
            "Epoch 190, Loss: 0.916343849183879\n",
            "Epoch 191, Loss: 0.915905595549347\n",
            "Epoch 192, Loss: 0.9158412784523104\n",
            "Epoch 193, Loss: 0.9160895951550805\n",
            "Epoch 194, Loss: 0.9158005366077671\n",
            "Epoch 195, Loss: 0.9154642469326969\n",
            "Epoch 196, Loss: 0.915133034876684\n",
            "Epoch 197, Loss: 0.914790118921954\n",
            "Epoch 198, Loss: 0.9139361423432389\n",
            "Epoch 199, Loss: 0.9142991248528746\n",
            "Epoch 00200: reducing learning rate of group 0 to 1.3841e-05.\n",
            "Accuracy: 0.7544305584974935\n",
            "Sample predictions: [88, 163, 173, 172, 175, 64, 171, 41, 72, 31, 196, 109, 119, 212, 1, 195, 163, 12, 16, 16]\n",
            "Sample true labels: [88, 21, 173, 38, 175, 64, 171, 41, 72, 88, 196, 145, 119, 212, 1, 195, 163, 12, 16, 16]\n"
          ]
        }
      ]
    }
  ]
}