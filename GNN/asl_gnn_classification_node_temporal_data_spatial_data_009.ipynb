{"cells":[{"cell_type":"markdown","metadata":{"id":"lLqJy6EZtrZB"},"source":["### Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29456,"status":"ok","timestamp":1702178342261,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"},"user_tz":420},"id":"7-ewruGCyaCn","outputId":"a854dfbe-3df7-4f0f-c1e8-53c8f43ab08d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"yLRvBRGgt5Zk"},"source":["#### Install Pytorch Geometric"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oiSS4wuxZ01m","executionInfo":{"status":"ok","timestamp":1702178348641,"user_tz":420,"elapsed":6385,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}},"outputId":"f0fdee20-896a-4dc0-ed10-986514fe8d7c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch_geometric\n","  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.11.17)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n","Installing collected packages: torch_geometric\n","Successfully installed torch_geometric-2.4.0\n"]}],"source":["!pip install torch_geometric"]},{"cell_type":"code","source":["import torch\n","\n","TORCH = torch.__version__.split('+')[0]\n","CUDA = 'cu' + torch.version.cuda.replace('.', '')\n","\n","# Construct the installation command\n","install_command = f\"pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\"\n","\n","# Execute the command\n","!{install_command}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YMqZ2Vdu_how","executionInfo":{"status":"ok","timestamp":1702178361970,"user_tz":420,"elapsed":13332,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}},"outputId":"8a1d38eb-0518-49c6-b724-732a39a13ad5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://data.pyg.org/whl/torch-2.1.0+cu118.html\n","Collecting pyg_lib\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/pyg_lib-0.3.1%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (2.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_scatter\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_scatter-2.1.2%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_sparse\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_sparse-0.6.18%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_cluster\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_cluster-1.6.3%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_spline_conv\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_spline_conv-1.2.2%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (887 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.8/887.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_sparse) (1.11.4)\n","Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch_sparse) (1.23.5)\n","Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n","Successfully installed pyg_lib-0.3.1+pt21cu118 torch_cluster-1.6.3+pt21cu118 torch_scatter-2.1.2+pt21cu118 torch_sparse-0.6.18+pt21cu118 torch_spline_conv-1.2.2+pt21cu118\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"DQdlS70-zM8Y","executionInfo":{"status":"ok","timestamp":1702178361970,"user_tz":420,"elapsed":16,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"outputs":[],"source":["import os\n","import torch\n","import numpy as np\n","\n","# Set a random seed for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""]},{"cell_type":"markdown","metadata":{"id":"zi7c7EMpsCBX"},"source":["### `ASLDatasetLoader` Class\n","\n","The `ASLDatasetLoader` class is designed for loading and processing the ASL dataset. Given a directory, it reads sign language data from JSON files and constructs graph representations suitable for graph-based neural networks. Crucially, the class converts JSON data into PyTorch Geometric (PyG) `Data` objects comprising `x` (node features), `edge_index` (graph connectivity), and `y' (labels) attributes.\n","\n","**Methods**:\n","\n","- `_create_sign_to_label_map`: Generates a mapping from sign names to unique labels.\n","\n","- `_read_file_data`: Reads data from a given JSON file.\n","\n","- `_augment_data`: Implements data augmentation by applying random rotation, translation, and scaling to landmarks, which can enhance the model's robustness.\n","\n","- `_create_graph_from_frame`: Constructs a PyG `Data` object from frame data, concentrating on hand and face landmarks. Edges are created between consecutive landmarks and between left and right hand landmarks. Additional features, like hand-to-face distances, are also computed.\n","\n","- `get_dataset`: Assembles the dataset, optionally incorporating data augmentation. The function outputs a list of PyG `Data` objects ready for graph neural network processing."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"LifBvD3D4t4C","executionInfo":{"status":"ok","timestamp":1702178364424,"user_tz":420,"elapsed":2469,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"outputs":[],"source":["import json\n","from itertools import repeat\n","from torch_geometric.data import InMemoryDataset, Data\n","from tqdm.notebook import tqdm\n","from torch_geometric.data import Data\n","from torch_geometric.utils import add_self_loops, remove_self_loops\n","\n","class ASLInMemoryDataset(InMemoryDataset):\n","    def __init__(self, root, transform=None, pre_transform=None):\n","        super(ASLInMemoryDataset, self).__init__(root, transform, pre_transform)\n","        self.data, self.slices = torch.load(self.processed_paths[0])\n","\n","    @property\n","    def raw_file_names(self):\n","        return [os.path.splitext(filename)[0] for filename in os.listdir(self.raw_dir)]\n","\n","    @property\n","    def processed_file_names(self):\n","        return ['processed_data.pt']\n","\n","    def download(self):\n","        # We're using local files, so no need to implement download logic\n","        pass\n","\n","    def process(self):\n","        data_list = []\n","\n","        loader = ASLDatasetLoader(self.raw_dir)\n","        self._sign_to_label = loader.sign_to_label\n","        dataset = loader.get_dataset()\n","\n","        # If you want to add a progress bar, use tqdm here\n","        # dataset = tqdm(dataset, desc=\"Processing dataset\", leave=False)\n","\n","        data_list.extend(dataset)\n","\n","        if self.pre_filter is not None:\n","            data_list = [data for data in data_list if self.pre_filter(data)]\n","\n","        if self.pre_transform is not None:\n","            data_list = [self.pre_transform(data) for data in data_list]\n","\n","        data, slices = self.collate(data_list)\n","        torch.save((data, slices), self.processed_paths[0])\n","\n","    def len(self):\n","        return len(self._data.y)\n","\n","    def get(self, idx):\n","        data = Data()\n","        for key in self._data.keys():\n","            item, slices = self._data[key], self.slices[key]\n","            s = list(repeat(slice(None), item.dim()))\n","            s[self._data.__cat_dim__(key, item)] = slice(slices[idx], slices[idx + 1])\n","            data[key] = item[s]\n","        return data\n","\n","    def sign_to_label(self):\n","        if self._sign_to_label is None:\n","            loader = ASLDatasetLoader(self.raw_dir)\n","            self._sign_to_label = loader.sign_to_label\n","        # Return mapping\n","        return self._sign_to_label"]},{"cell_type":"code","source":["import json\n","from torch_geometric.data import Data\n","\n","class ASLDatasetLoader:\n","    # Define natural connections as class attributes\n","    HAND_CONNECTIONS = frozenset([\n","      # Left hand palm\n","      (\"left_hand-0\", \"left_hand-1\"),\n","      (\"left_hand-0\", \"left_hand-5\"),\n","      (\"left_hand-9\", \"left_hand-13\"),\n","      (\"left_hand-13\", \"left_hand-17\"),\n","      (\"left_hand-5\", \"left_hand-9\"),\n","      (\"left_hand-0\", \"left_hand-17\"),\n","      # Left hand thumb\n","      (\"left_hand-1\", \"left_hand-2\"),\n","      (\"left_hand-2\", \"left_hand-3\"),\n","      (\"left_hand-3\", \"left_hand-4\"),\n","      # Left hand index finger\n","      (\"left_hand-5\", \"left_hand-6\"),\n","      (\"left_hand-6\", \"left_hand-7\"),\n","      (\"left_hand-7\", \"left_hand-8\"),\n","      # Left hand middle finger\n","      (\"left_hand-9\", \"left_hand-10\"),\n","      (\"left_hand-10\", \"left_hand-11\"),\n","      (\"left_hand-11\", \"left_hand-12\"),\n","      # Left hand ring finger\n","      (\"left_hand-13\", \"left_hand-14\"),\n","      (\"left_hand-14\", \"left_hand-15\"),\n","      (\"left_hand-15\", \"left_hand-16\"),\n","      # Left hand pinky\n","      (\"left_hand-17\", \"left_hand-18\"),\n","      (\"left_hand-18\", \"left_hand-19\"),\n","      (\"left_hand-19\", \"left_hand-20\"),\n","      # Right hand palm\n","      (\"right_hand-0\", \"right_hand-1\"),\n","      (\"right_hand-0\", \"right_hand-5\"),\n","      (\"right_hand-9\", \"right_hand-13\"),\n","      (\"right_hand-13\", \"right_hand-17\"),\n","      (\"right_hand-5\", \"right_hand-9\"),\n","      (\"right_hand-0\", \"right_hand-17\"),\n","      # Right hand thumb\n","      (\"right_hand-1\", \"right_hand-2\"),\n","      (\"right_hand-2\", \"right_hand-3\"),\n","      (\"right_hand-3\", \"right_hand-4\"),\n","      # Right hand index finger\n","      (\"right_hand-5\", \"right_hand-6\"),\n","      (\"right_hand-6\", \"right_hand-7\"),\n","      (\"right_hand-7\", \"right_hand-8\"),\n","      # Right hand middle finger\n","      (\"right_hand-9\", \"right_hand-10\"),\n","      (\"right_hand-10\", \"right_hand-11\"),\n","      (\"right_hand-11\", \"right_hand-12\"),\n","      # Right hand ring finger\n","      (\"right_hand-13\", \"right_hand-14\"),\n","      (\"right_hand-14\", \"right_hand-15\"),\n","      (\"right_hand-15\", \"right_hand-16\"),\n","      # Right hand pinky\n","      (\"right_hand-17\", \"right_hand-18\"),\n","      (\"right_hand-18\", \"right_hand-19\"),\n","      (\"right_hand-19\", \"right_hand-20\"),\n","    ])\n","\n","    POSE_CONNECTIONS = frozenset([\n","      (\"pose-0\", \"pose-1\"),\n","      (\"pose-1\", \"pose-2\"),\n","      (\"pose-2\", \"pose-3\"),\n","      (\"pose-3\", \"pose-7\"),\n","      (\"pose-0\", \"pose-4\"),\n","      (\"pose-4\", \"pose-5\"),\n","      (\"pose-5\", \"pose-6\"),\n","      (\"pose-6\", \"pose-8\"),\n","      (\"pose-9\", \"pose-10\"),\n","      (\"pose-11\", \"pose-12\"),\n","      (\"pose-11\", \"pose-13\"),\n","      (\"pose-13\", \"pose-15\"),\n","      (\"pose-15\", \"pose-17\"),\n","      (\"pose-12\", \"pose-14\"),\n","      (\"pose-14\", \"pose-16\"),\n","      (\"pose-16\", \"pose-18\"),\n","      (\"pose-11\", \"pose-23\"),\n","      (\"pose-12\", \"pose-24\"),\n","      (\"pose-23\", \"pose-24\"),\n","    ])\n","\n","    FACE_CONNECTIONS = frozenset([\n","      # Connections for FACEMESH_LIPS using available landmarks\n","      (\"face-61\", \"face-146\"), (\"face-146\", \"face-91\"), (\"face-91\", \"face-181\"),\n","      (\"face-181\", \"face-84\"), (\"face-84\", \"face-17\"), (\"face-17\", \"face-314\"),\n","      (\"face-314\", \"face-405\"), (\"face-405\", \"face-321\"), (\"face-321\", \"face-375\"),\n","      (\"face-375\", \"face-291\"), (\"face-78\", \"face-95\"), (\"face-95\", \"face-88\"),\n","      (\"face-88\", \"face-178\"), (\"face-178\", \"face-87\"), (\"face-87\", \"face-14\"),\n","      (\"face-14\", \"face-317\"), (\"face-317\", \"face-402\"), (\"face-402\", \"face-318\"),\n","      (\"face-318\", \"face-324\"), (\"face-324\", \"face-308\"),\n","\n","      # Connections for FACEMESH_LEFT_EYE using available landmarks\n","      (\"face-263\", \"face-249\"), (\"face-388\", \"face-387\"), (\"face-387\", \"face-386\"),\n","      (\"face-386\", \"face-385\"), (\"face-385\", \"face-384\"), (\"face-384\", \"face-398\"),\n","\n","      # Connections for FACEMESH_LEFT_EYEBROW using available landmarks\n","      (\"face-276\", \"face-283\"), (\"face-300\", \"face-293\"), (\"face-293\", \"face-334\"),\n","      (\"face-334\", \"face-296\"), (\"face-296\", \"face-336\"),\n","\n","      # Connections for FACEMESH_RIGHT_EYE using available landmarks\n","      (\"face-33\", \"face-7\"), (\"face-246\", \"face-161\"), (\"face-161\", \"face-160\"),\n","      (\"face-160\", \"face-159\"), (\"face-159\", \"face-158\"), (\"face-158\", \"face-157\"),\n","      (\"face-157\", \"face-173\"),\n","\n","      # Connections for FACEMESH_RIGHT_EYEBROW using available landmarks\n","      (\"face-46\", \"face-53\"), (\"face-70\", \"face-63\"), (\"face-63\", \"face-105\"),\n","      (\"face-105\", \"face-66\"), (\"face-66\", \"face-107\"),\n","\n","      # Connections for FACEMESH_FACE_OVAL using available landmarks\n","      (\"face-10\", \"face-338\"), (\"face-338\", \"face-297\"), (\"face-297\", \"face-332\"),\n","      (\"face-332\", \"face-284\"), (\"face-284\", \"face-251\"), (\"face-251\", \"face-389\"),\n","      (\"face-389\", \"face-356\"), (\"face-356\", \"face-454\"), (\"face-454\", \"face-323\"),\n","      (\"face-323\", \"face-361\"), (\"face-361\", \"face-288\"), (\"face-288\", \"face-397\"),\n","      (\"face-397\", \"face-365\"), (\"face-365\", \"face-379\"), (\"face-379\", \"face-378\"),\n","      (\"face-378\", \"face-400\"), (\"face-400\", \"face-377\"), (\"face-377\", \"face-152\"),\n","      (\"face-152\", \"face-148\"), (\"face-148\", \"face-176\"), (\"face-176\", \"face-149\"),\n","      (\"face-149\", \"face-150\"), (\"face-150\", \"face-136\"), (\"face-136\", \"face-172\"),\n","      (\"face-172\", \"face-58\"), (\"face-58\", \"face-132\"), (\"face-132\", \"face-93\"),\n","      (\"face-93\", \"face-234\"), (\"face-234\", \"face-127\"), (\"face-127\", \"face-162\"),\n","      (\"face-162\", \"face-21\"), (\"face-21\", \"face-54\"), (\"face-54\", \"face-103\"),\n","      (\"face-103\", \"face-67\"), (\"face-67\", \"face-109\"), (\"face-109\", \"face-10\"),\n","    ])\n","\n","    def __init__(self, directory_path, min_examples_per_class=2, max_files=-1):\n","        self.directory_path = directory_path\n","        self.min_examples_per_class = min_examples_per_class\n","        self.max_files = max_files\n","        self.sign_to_label = self._create_sign_to_label_map()\n","\n","    def _create_sign_to_label_map(self):\n","        signs = [os.path.splitext(filename)[0] for filename in os.listdir(self.directory_path)]\n","        return {sign: i for i, sign in enumerate(signs)}\n","\n","    def _read_file_data(self, file_path):\n","        with open(file_path, 'r') as f:\n","            data = json.load(f)\n","\n","        sign_name = data[\"sign\"]\n","        num_examples = len(data[\"examples\"])\n","        print(f\"Loaded sign '{sign_name}' with {num_examples} examples\")\n","\n","        return data if num_examples >= self.min_examples_per_class else None\n","\n","    def _augment_data(self, frame_data, rotation_range=10, translation_range=0.05, scaling_range=0.15, jittering_range=0.01, noise_scale=0.01, mirroring=True):\n","        \"\"\"\n","        Augment the frame data with various techniques including rotation, translation, scaling, jittering, noise injection, and mirroring.\n","\n","        :param frame_data: Dictionary containing frame landmarks and deltas.\n","        :param rotation_range: Maximum rotation angle in degrees.\n","        :param translation_range: Maximum translation as a fraction of landmark range.\n","        :param scaling_range: Maximum scaling factor.\n","        :param jittering_range: Range for jittering.\n","        :param noise_scale: Scale of the random noise to be added.\n","        :param mirroring: Whether to mirror the landmarks (simulate opposite hand).\n","        :return: Augmented frame data.\n","        \"\"\"\n","        # Extract landmarks\n","        landmarks = np.array([[landmark['x'], landmark['y']] for landmark in frame_data['landmarks']])\n","        centroid = np.mean(landmarks, axis=0)\n","\n","        # Jittering\n","        jittering = np.random.uniform(-jittering_range, jittering_range, landmarks.shape)\n","        landmarks += jittering\n","\n","        # Random rotation\n","        theta = np.radians(np.random.uniform(-rotation_range, rotation_range))\n","        rotation_matrix = np.array([\n","            [np.cos(theta), -np.sin(theta)],\n","            [np.sin(theta), np.cos(theta)]\n","        ])\n","        landmarks = np.dot(landmarks - centroid, rotation_matrix) + centroid\n","\n","        # Random translation\n","        max_translation = translation_range * (landmarks.max(axis=0) - landmarks.min(axis=0))\n","        translations = np.random.uniform(-max_translation, max_translation, size=landmarks.shape[1])\n","        landmarks += translations\n","\n","        # Random scaling\n","        scale = np.random.uniform(1 - scaling_range, 1 + scaling_range)\n","        landmarks = centroid + scale * (landmarks - centroid)\n","\n","        # Noise Injection\n","        noise = np.random.normal(0, noise_scale, landmarks.shape)\n","        landmarks += noise\n","\n","        # Mirroring (if applicable)\n","        if mirroring:\n","            landmarks[:, 0] = -landmarks[:, 0] + 2 * centroid[0]  # Reflect x-coordinates\n","\n","        # Update the landmarks in frame_data\n","        for i, landmark in enumerate(frame_data['landmarks']):\n","            landmark['x'], landmark['y'] = landmarks[i]\n","\n","        return frame_data\n","\n","\n","    def _create_graph_from_frame(self, sign_name, sign_data):\n","        graphs = []\n","\n","        for example in sign_data[\"examples\"]:\n","            all_features = []  # Combined list for landmarks, velocities, and accelerations\n","            edges = []\n","\n","            for frame in example[\"frames\"]:\n","                for landmark_data in frame[\"landmarks\"]:\n","                    # Extract spatial coordinates\n","                    landmark_features = [landmark_data[\"x\"], landmark_data[\"y\"]]\n","\n","                    # Extract temporal data (velocity and acceleration)\n","                    temporal_data = next((item for item in frame[\"temporal\"] if item[\"landmark\"] == landmark_data[\"landmark\"]), None)\n","                    if temporal_data:\n","                        velocity = [temporal_data[\"velocity\"][\"x\"], temporal_data[\"velocity\"][\"y\"]]\n","                        acceleration = [temporal_data[\"acceleration\"][\"x\"], temporal_data[\"acceleration\"][\"y\"]]\n","                    else:\n","                        velocity = [0, 0]\n","                        acceleration = [0, 0]\n","\n","                    # Combine spatial and temporal features\n","                    combined_features = landmark_features + velocity + acceleration\n","                    all_features.append(combined_features)\n","\n","                # Add spatial edges within the frame using natural connections\n","                for i in range(len(frame[\"landmarks\"])):\n","                    for j in range(len(frame[\"landmarks\"])):\n","                        if i != j:\n","                            connection = (frame[\"landmarks\"][i][\"landmark\"], frame[\"landmarks\"][j][\"landmark\"])\n","                            if connection in self.HAND_CONNECTIONS or \\\n","                              connection in self.POSE_CONNECTIONS or \\\n","                              connection in self.FACE_CONNECTIONS:\n","                                edges.append([len(all_features) - len(frame[\"landmarks\"]) + i,\n","                                              len(all_features) - len(frame[\"landmarks\"]) + j])\n","\n","            # Add temporal edges between frames within each example\n","            for i in range(len(example[\"frames\"]) - 1):\n","                for j in range(len(frame[\"landmarks\"])):\n","                    start_index = i * len(frame[\"landmarks\"]) + j\n","                    end_index = (i + 1) * len(frame[\"landmarks\"]) + j\n","                    edges.append([start_index, end_index])\n","\n","            # Create the graph\n","            x = torch.tensor(all_features, dtype=torch.float)\n","            edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n","            y = torch.tensor([self.sign_to_label[sign_name]], dtype=torch.long)\n","\n","            # Append the graph for the current example to the list of graphs\n","            graphs.append(Data(x=x, edge_index=edge_index, y=y))\n","\n","        return graphs\n","\n","    def get_dataset(self, augment=False):\n","        dataset = []\n","        file_count = 0\n","\n","        for filename in os.listdir(self.directory_path):\n","            if 0 <= self.max_files <= file_count:\n","                break  # Stop if max_files limit is reached\n","\n","            sign_name = os.path.splitext(filename)[0]\n","            file_path = os.path.join(self.directory_path, filename)\n","            sign_data = self._read_file_data(file_path)\n","            file_count += 1\n","\n","            if sign_data is None:\n","                print(f\"Skipping sign '{sign_name}' due to insufficient examples\")\n","                continue\n","\n","            # Retrieve a list of graphs, one for each example\n","            graphs = self._create_graph_from_frame(sign_name, sign_data)\n","\n","            # Debugging: Check the number of graphs created for the current sign\n","            print(f\"Sign '{sign_name}': Created {len(graphs)} graphs\")\n","\n","            dataset.extend(graphs)  # Extend the dataset with the list of graphs\n","\n","        return dataset\n","\n","    def number_of_classes(self):\n","        return len(self.sign_to_label)"],"metadata":{"id":"F2TjkBSGEs4f","executionInfo":{"status":"ok","timestamp":1702178364425,"user_tz":420,"elapsed":4,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xooW2sogtdL1"},"source":["### `ASLGraphClassifier` Class\n","\n","The `ASLGraphClassifier`, features deeper GCN layers and additional channels to capture intricate data patterns potentially. It takes a PyG `Data` object as input, and its forward pass emits class logits.\n","\n","**Methods**:\n","\n","- `forward`: Details the forward pass, accepting a PyG `Data` object. Two GCN layers with subsequent batch normalization and dropout layers process the input. Post global max-pooling, two linear layers coupled with dropout ensure final classification, leading to log-softmax outputs."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"RAslUK79VVV6","executionInfo":{"status":"ok","timestamp":1702178364425,"user_tz":420,"elapsed":4,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","from torch_geometric.nn import GATv2Conv, global_max_pool, BatchNorm, TopKPooling\n","\n","class ASLGraphClassifier(torch.nn.Module):\n","    def __init__(self, num_features, num_classes):\n","        super(ASLGraphClassifier, self).__init__()\n","        self.heads = 4  # Further reduced number of heads\n","        self.conv1 = GATv2Conv(num_features, 512, heads=self.heads, dropout=0.4)  # Reduced channels\n","        self.pool1 = TopKPooling(512 * self.heads, ratio=0.5)\n","        self.bn1 = BatchNorm(512 * self.heads)\n","        self.lin1 = torch.nn.Linear(512 * self.heads, 1024)\n","        self.lin2 = torch.nn.Linear(1024, num_classes)\n","        self.dropout = torch.nn.Dropout(p=0.5)\n","\n","    def forward(self, data):\n","        x, edge_index, batch = data.x, data.edge_index, data.batch\n","\n","        # First GATv2 layer and pooling\n","        x = F.elu(self.conv1(x, edge_index))\n","        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n","        x = self.bn1(x)\n","        x = self.dropout(x)\n","\n","        # Global pooling\n","        x = global_max_pool(x, batch)\n","\n","        # Fully connected layers\n","        x = F.relu(self.lin1(x))\n","        x = self.dropout(x)\n","        x = self.lin2(x)\n","\n","        return F.log_softmax(x, dim=1)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"RdBGsFveWcbF","executionInfo":{"status":"ok","timestamp":1702178365525,"user_tz":420,"elapsed":1103,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch_geometric.loader import DataLoader\n","from torch.cuda.amp import GradScaler, autocast\n","\n","EPOCHS = 100\n","LEARNING_RATE = 0.00001\n","WEIGHT_DECAY = 5e-4\n","BATCH_SIZE = 7\n","GRADIENT_ACCUM_STEPS = 4  # For gradient accumulation\n","WORKERS = 2\n","\n","def stratified_data_split(data_list, test_size=0.2):\n","    labels = [data.y.item() for data in data_list]\n","    train_data, test_data = train_test_split(data_list, test_size=test_size, stratify=labels, random_state=42)\n","    return train_data, test_data\n","\n","def validate(loader, model, device):\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","\n","    for data in loader:\n","        data = data.to(device)\n","        with torch.no_grad():\n","            out = model(data)\n","        pred = out.argmax(dim=1)\n","        all_preds.append(pred.cpu())\n","        all_labels.append(data.y.cpu())\n","\n","    all_preds = torch.cat(all_preds)\n","    all_labels = torch.cat(all_labels)\n","    accuracy = (all_preds == all_labels).float().mean().item()\n","\n","    metrics = {\n","        'accuracy': accuracy,\n","        'precision': precision_score(all_labels.numpy(), all_preds.numpy(), average='macro', zero_division=1),\n","        'recall': recall_score(all_labels.numpy(), all_preds.numpy(), average='macro', zero_division=1),\n","        'f1': f1_score(all_labels.numpy(), all_preds.numpy(), average='macro', zero_division=1)\n","    }\n","    return metrics\n","\n","def train(train_data, val_data, epochs=EPOCHS, learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY, patience=5):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    # Define the use_amp variable\n","    use_amp = torch.cuda.is_available()\n","\n","    num_features = train_data[0].num_node_features\n","    num_classes = len(set([data.y.item() for data in train_data]))\n","    model = ASLGraphClassifier(num_features=num_features, num_classes=num_classes).to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.7, patience=2, verbose=True)\n","    criterion = torch.nn.CrossEntropyLoss()\n","\n","    # Only instantiate the GradScaler if CUDA is available\n","    if use_amp:\n","        scaler = GradScaler()\n","    else:\n","        scaler = None  # or you could use a dummy scaler that doesn't do anything\n","\n","\n","    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS)\n","    train_loader = tqdm(train_loader, desc=\"Data Loading...\", leave=False)\n","    val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKERS)\n","\n","    best_val_accuracy = 0.0\n","    epochs_without_improvement = 0\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        total_loss = 0\n","        all_preds = []\n","        all_labels = []\n","        optimizer.zero_grad()\n","\n","        for idx, data in enumerate(train_loader):\n","            # print(f\"Batch {idx+1}\")\n","            # print(data.edge_index)\n","            # print(\"---------\")\n","            data = data.to(device)\n","\n","            # Conditional autocast based on use_amp\n","            if use_amp:\n","                with autocast():\n","                    out = model(data)\n","                    loss = criterion(out, data.y)\n","            else:\n","                out = model(data)\n","                loss = criterion(out, data.y)\n","\n","            # Conditional scaler step based on use_amp\n","            if use_amp:\n","                scaler.scale(loss / GRADIENT_ACCUM_STEPS).backward()\n","\n","                if (idx + 1) % GRADIENT_ACCUM_STEPS == 0:\n","                    scaler.step(optimizer)\n","                    scaler.update()\n","                    optimizer.zero_grad()\n","            else:\n","                loss.backward()\n","\n","                if (idx + 1) % GRADIENT_ACCUM_STEPS == 0:\n","                    optimizer.step()\n","                    optimizer.zero_grad()\n","\n","            preds = out.argmax(dim=1)\n","            all_preds.append(preds.cpu())\n","            all_labels.append(data.y.cpu())\n","            total_loss += loss.item()\n","\n","        all_preds = torch.cat(all_preds)\n","        all_labels = torch.cat(all_labels)\n","        train_accuracy = (all_preds == all_labels).float().mean().item()\n","\n","        avg_loss = total_loss / len(train_data)\n","\n","        val_metrics = validate(val_loader, model, device)\n","\n","        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_metrics['accuracy']:.4f}, Precision: {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}, F1: {val_metrics['f1']:.4f}\")\n","\n","        scheduler.step(val_metrics['accuracy'])\n","\n","        if val_metrics['accuracy'] > best_val_accuracy:\n","            best_val_accuracy = val_metrics['accuracy']\n","            epochs_without_improvement = 0\n","        else:\n","            epochs_without_improvement += 1\n","\n","        if epochs_without_improvement >= patience:\n","            print(f\"Early stopping triggered after {epochs_without_improvement} epochs without validation accuracy improvement.\")\n","            break\n","\n","        torch.cuda.empty_cache()\n","\n","    return model, all_preds, all_labels, train_accuracy"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"EU2sza6GtOUS","executionInfo":{"status":"ok","timestamp":1702178385991,"user_tz":420,"elapsed":20468,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"outputs":[],"source":["directory_path = \"/content/drive/MyDrive/Colab Notebooks/DGMD E-14 Project/Datasets/processed-40-500\"\n","# Create an instance of the ASLInMemoryDataset\n","dataset = ASLInMemoryDataset(root=directory_path)"]},{"cell_type":"code","source":["# Split the dataset into training and validation subsets\n","train_data, val_data = stratified_data_split(dataset)"],"metadata":{"id":"I1Sg12EwrK8u","executionInfo":{"status":"ok","timestamp":1702178387791,"user_tz":420,"elapsed":1802,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":425,"referenced_widgets":["5856ad785c334cd1b3e6785e5fb9b3a2","1b46f8d824234a62b7a8ef6330ac5564","a641a7cf8eb44ca5887c0fb3109e151a","a4e1df53b6f84bb68961b3b6467bfd47","93eb17d4712545688f43799f0edfb46f","a99ac8feae9c42e6b72f47d97ecd0436","36c518b00a5a4421895299b15f98ca67","838969a753604ec480b315a9aa0b894c","1ac02af04d7d443bac958c639280d026","c5f0c70cde974ccea19420e470a6f3a1","daaa29e8d69c4cf39b5e4a5c515c19e5"]},"id":"rr2PlwLy5M6H","executionInfo":{"status":"ok","timestamp":1702181823160,"user_tz":420,"elapsed":3435372,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}},"outputId":"cc6819ca-f422-43d1-cb19-fb7e15a7c86e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Data Loading...:   0%|          | 0/874 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5856ad785c334cd1b3e6785e5fb9b3a2"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1/100, Loss: 0.4659, Accuracy: 0.0479, Validation Accuracy: 0.0491, Precision: 0.6218, Recall: 0.0499, F1: 0.0124\n","Epoch 2/100, Loss: 0.4345, Accuracy: 0.0573, Validation Accuracy: 0.0451, Precision: 0.6727, Recall: 0.0459, F1: 0.0113\n","Epoch 3/100, Loss: 0.4297, Accuracy: 0.0535, Validation Accuracy: 0.0556, Precision: 0.8188, Recall: 0.0572, F1: 0.0120\n","Epoch 4/100, Loss: 0.4282, Accuracy: 0.0610, Validation Accuracy: 0.0517, Precision: 0.7669, Recall: 0.0547, F1: 0.0148\n","Epoch 5/100, Loss: 0.4266, Accuracy: 0.0707, Validation Accuracy: 0.0543, Precision: 0.7627, Recall: 0.0563, F1: 0.0171\n","Epoch 6/100, Loss: 0.4261, Accuracy: 0.0743, Validation Accuracy: 0.0536, Precision: 0.8102, Recall: 0.0560, F1: 0.0151\n","Epoch 00006: reducing learning rate of group 0 to 7.0000e-06.\n","Epoch 7/100, Loss: 0.4247, Accuracy: 0.0741, Validation Accuracy: 0.0549, Precision: 0.7604, Recall: 0.0577, F1: 0.0162\n","Epoch 8/100, Loss: 0.4240, Accuracy: 0.0785, Validation Accuracy: 0.0628, Precision: 0.7644, Recall: 0.0653, F1: 0.0217\n","Epoch 9/100, Loss: 0.4229, Accuracy: 0.0846, Validation Accuracy: 0.0647, Precision: 0.7643, Recall: 0.0667, F1: 0.0221\n","Epoch 10/100, Loss: 0.4226, Accuracy: 0.0867, Validation Accuracy: 0.0628, Precision: 0.7631, Recall: 0.0650, F1: 0.0207\n","Epoch 11/100, Loss: 0.4212, Accuracy: 0.0838, Validation Accuracy: 0.0589, Precision: 0.7128, Recall: 0.0611, F1: 0.0201\n","Epoch 12/100, Loss: 0.4201, Accuracy: 0.0921, Validation Accuracy: 0.0641, Precision: 0.8131, Recall: 0.0662, F1: 0.0215\n","Epoch 00012: reducing learning rate of group 0 to 4.9000e-06.\n","Epoch 13/100, Loss: 0.4190, Accuracy: 0.0938, Validation Accuracy: 0.0726, Precision: 0.8143, Recall: 0.0740, F1: 0.0240\n","Epoch 14/100, Loss: 0.4184, Accuracy: 0.0982, Validation Accuracy: 0.0765, Precision: 0.8152, Recall: 0.0777, F1: 0.0253\n","Epoch 15/100, Loss: 0.4173, Accuracy: 0.1018, Validation Accuracy: 0.0733, Precision: 0.7645, Recall: 0.0744, F1: 0.0238\n","Epoch 16/100, Loss: 0.4171, Accuracy: 0.0952, Validation Accuracy: 0.0726, Precision: 0.7641, Recall: 0.0737, F1: 0.0229\n","Epoch 17/100, Loss: 0.4165, Accuracy: 0.1003, Validation Accuracy: 0.0687, Precision: 0.8136, Recall: 0.0697, F1: 0.0217\n","Epoch 00017: reducing learning rate of group 0 to 3.4300e-06.\n","Epoch 18/100, Loss: 0.4153, Accuracy: 0.1036, Validation Accuracy: 0.0693, Precision: 0.8132, Recall: 0.0702, F1: 0.0200\n","Epoch 19/100, Loss: 0.4147, Accuracy: 0.1062, Validation Accuracy: 0.0674, Precision: 0.8122, Recall: 0.0681, F1: 0.0218\n","Early stopping triggered after 5 epochs without validation accuracy improvement.\n"]}],"source":["# Train the model using the datasets\n","model, all_preds, all_labels, accuracy = train(train_data, val_data, epochs=EPOCHS, learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"]},{"cell_type":"code","source":["dataset.sign_to_label()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":326},"id":"AlLngPoYqiDb","executionInfo":{"status":"error","timestamp":1702181823160,"user_tz":420,"elapsed":13,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}},"outputId":"ca4b29ba-5297-4c4b-f2f8-6a4314cf0205"},"execution_count":12,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-4d6169689395>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign_to_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-5-f399df4d4cf8>\u001b[0m in \u001b[0;36msign_to_label\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msign_to_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sign_to_label\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mASLDatasetLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sign_to_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign_to_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_data_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         raise AttributeError(f\"'{self.__class__.__name__}' object has no \"\n\u001b[0m\u001b[1;32m    303\u001b[0m                              f\"attribute '{key}'\")\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'ASLInMemoryDataset' object has no attribute '_sign_to_label'"]}]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"aborted","timestamp":1702181823160,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"},"user_tz":420},"id":"sMrYkTs89Q7O"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix, classification_report\n","import numpy as np\n","\n","# Convert lists to numpy arrays for compatibility with sklearn\n","y_true = np.array(all_labels)\n","y_pred = np.array(all_preds)\n","\n","# Calculate the confusion matrix\n","cm = confusion_matrix(y_true, y_pred)\n","\n","# Visualize the confusion matrix\n","plt.figure(figsize=(10, 7))\n","sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n","plt.title(\"Confusion Matrix\")\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"True\")\n","plt.show()\n","\n","# Ensure the class names are in the correct order for target_names\n","ordered_class_names = [name for name, num in sorted((dataset.sign_to_label()).items(), key=lambda item: item[1])]\n","\n","# Per-Class Accuracy\n","class_accuracy = cm.diagonal() / cm.sum(axis=1)\n","for i, acc in enumerate(class_accuracy):\n","    class_name = ordered_class_names[i]\n","    print(f\"Accuracy for class {i} ({class_name}): {acc*100:.2f}%\")\n","\n","# Detailed classification report\n","print(\"\\nClassification Report:\\n\")\n","print(classification_report(y_true, y_pred, target_names=ordered_class_names, zero_division=1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hl5tQmtN9Syt","executionInfo":{"status":"aborted","timestamp":1702181823160,"user_tz":420,"elapsed":3,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"outputs":[],"source":["from sklearn.metrics import classification_report\n","\n","def print_top_misclassified_classes(y_true, y_pred, sign_to_label, N=3, zero_division=1):\n","    \"\"\"\n","    Prints the top N classes that get misclassified the most.\n","\n","    Parameters:\n","    - y_true: Actual labels\n","    - y_pred: Predicted labels by the model\n","    - sign_to_label: Dictionary mapping class names to class numbers\n","    - N: Number of top misclassified classes to print\n","    - zero_division: Parameter for handling zero division in classification_report\n","\n","    Returns:\n","    None\n","    \"\"\"\n","\n","    # Ensure the class names are in the correct order for target_names\n","    ordered_class_names = [name for name, num in sorted(sign_to_label.items(), key=lambda item: item[1])]\n","\n","    # Generate and print classification report with class names\n","    print(\"\\nClassification Report:\\n\")\n","    print(classification_report(y_true, y_pred, target_names=ordered_class_names, zero_division=zero_division))\n","\n","    # Generate classification report as dict to find misclassified classes\n","    report = classification_report(y_true, y_pred, output_dict=True, zero_division=zero_division)\n","\n","    # Create a dictionary to store misclassification rates\n","    misclassification_rates = {}\n","\n","    # Iterate through each class in the report\n","    for class_num, metrics in report.items():\n","        if class_num.isdigit():\n","            class_name = [key for key, value in sign_to_label.items() if value == int(class_num)][0]\n","            misclassification_rates[class_name] = 1 - metrics['recall']\n","\n","    # Sort classes based on misclassification rate\n","    sorted_classes = sorted(misclassification_rates, key=misclassification_rates.get, reverse=True)\n","\n","    # Print top N misclassified classes\n","    print(f\"\\nTop {N} misclassified classes:\")\n","    for i in range(N):\n","        class_name = sorted_classes[i]\n","        print(f\"{i+1}. {class_name} - Misclassification rate: {misclassification_rates[class_name]:.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m4ZtqFxz9XlG","executionInfo":{"status":"aborted","timestamp":1702181823160,"user_tz":420,"elapsed":3,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"outputs":[],"source":["print_top_misclassified_classes(y_true, y_pred, dataset.sign_to_label(), N=10, zero_division=1)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1wIPH3B0fuuoSlPfXK5fGd9Em1LP-uQFJ","timestamp":1697014386160}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5856ad785c334cd1b3e6785e5fb9b3a2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1b46f8d824234a62b7a8ef6330ac5564","IPY_MODEL_a641a7cf8eb44ca5887c0fb3109e151a","IPY_MODEL_a4e1df53b6f84bb68961b3b6467bfd47"],"layout":"IPY_MODEL_93eb17d4712545688f43799f0edfb46f"}},"1b46f8d824234a62b7a8ef6330ac5564":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a99ac8feae9c42e6b72f47d97ecd0436","placeholder":"​","style":"IPY_MODEL_36c518b00a5a4421895299b15f98ca67","value":"Data Loading...: 100%"}},"a641a7cf8eb44ca5887c0fb3109e151a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_838969a753604ec480b315a9aa0b894c","max":874,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1ac02af04d7d443bac958c639280d026","value":874}},"a4e1df53b6f84bb68961b3b6467bfd47":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5f0c70cde974ccea19420e470a6f3a1","placeholder":"​","style":"IPY_MODEL_daaa29e8d69c4cf39b5e4a5c515c19e5","value":" 873/874 [02:44&lt;00:00,  5.35it/s]"}},"93eb17d4712545688f43799f0edfb46f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"a99ac8feae9c42e6b72f47d97ecd0436":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36c518b00a5a4421895299b15f98ca67":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"838969a753604ec480b315a9aa0b894c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ac02af04d7d443bac958c639280d026":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c5f0c70cde974ccea19420e470a6f3a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"daaa29e8d69c4cf39b5e4a5c515c19e5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}