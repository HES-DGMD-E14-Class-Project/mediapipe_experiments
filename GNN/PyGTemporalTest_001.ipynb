{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiSS4wuxZ01m",
        "outputId": "140232bf-7969-4dcd-83f2-5df0d29e6d5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n",
            "11.8\n",
            "Collecting torch-geometric-temporal\n",
            "  Downloading torch_geometric_temporal-0.54.0.tar.gz (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (4.4.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (2.0.1+cu118)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (3.0.2)\n",
            "Collecting pandas<=1.3.5 (from torch-geometric-temporal)\n",
            "  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse (from torch-geometric-temporal)\n",
            "  Using cached torch_sparse-0.6.17.tar.gz (209 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch_scatter (from torch-geometric-temporal)\n",
            "  Using cached torch_scatter-2.1.1.tar.gz (107 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch_geometric (from torch-geometric-temporal)\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (1.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.10/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2023.3.post1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torch-geometric-temporal) (3.27.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torch-geometric-temporal) (17.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (4.66.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (1.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torch-geometric-temporal) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (2023.7.22)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric->torch-geometric-temporal) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric->torch-geometric-temporal) (3.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torch-geometric-temporal) (1.3.0)\n",
            "Building wheels for collected packages: torch-geometric-temporal, torch_geometric, torch_scatter, torch_sparse\n",
            "  Building wheel for torch-geometric-temporal (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric-temporal: filename=torch_geometric_temporal-0.54.0-py3-none-any.whl size=86722 sha256=97af22abd717c2e9eefd4b19cc952aabd4cf0f8f74fed06034a1be134024c62b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/9b/b6/e15256e053f0cb49b1084a67a709db909d418386a231f0722c\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910454 sha256=fcd21c5641b699521a34756308e0eb1a367a790292debcb33e06a9a8bf3f01f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "  Building wheel for torch_scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_scatter: filename=torch_scatter-2.1.1-cp310-cp310-linux_x86_64.whl size=491866 sha256=2519bcaf8db2b366d850c13659da480a08d1081407b2941527df027e7390ec12\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/67/58/6566a3b61c6ec0f2ca0c2c324cd035ef2955601f0fb3197d5f\n",
            "  Building wheel for torch_sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_sparse: filename=torch_sparse-0.6.17-cp310-cp310-linux_x86_64.whl size=1053726 sha256=25a74b84023abc2b142367ebbdc5cefe034aa00a5b3dcc2ad57dea543346e681\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/25/e7/037b58fa47ba781444fd101a2f06c63a9d4e967ca6b910c53a\n",
            "Successfully built torch-geometric-temporal torch_geometric torch_scatter torch_sparse\n",
            "Installing collected packages: torch_scatter, torch_sparse, pandas, torch_geometric, torch-geometric-temporal\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 1.3.5 which is incompatible.\n",
            "plotnine 0.12.3 requires pandas>=1.5.0, but you have pandas 1.3.5 which is incompatible.\n",
            "xarray 2023.7.0 requires pandas>=1.4, but you have pandas 1.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-1.3.5 torch-geometric-temporal-0.54.0 torch_geometric-2.3.1 torch_scatter-2.1.1 torch_sparse-0.6.17\n"
          ]
        }
      ],
      "source": [
        "!python -c \"import torch; print(torch.__version__)\"\n",
        "!python -c \"import torch; print(torch.version.cuda)\"\n",
        "!pip install torch-geometric-temporal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric_temporal.nn.recurrent import A3TGCN2\n",
        "from torch_geometric_temporal.signal import temporal_signal_split\n",
        "\n",
        "# GPU support\n",
        "DEVICE = torch.device('cuda') # cuda\n",
        "shuffle=True\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "_9_1Hzq-pc6k"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from tqdm import tqdm\n",
        "except ImportError:\n",
        "    def tqdm(iterable):\n",
        "        return iterable\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric_temporal.nn.recurrent import TGCN\n",
        "\n",
        "from torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\n",
        "from torch_geometric_temporal.signal import temporal_signal_split\n",
        "\n",
        "loader = ChickenpoxDatasetLoader()\n",
        "\n",
        "dataset = loader.get_dataset()\n",
        "\n",
        "train_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n",
        "\n",
        "class RecurrentGCN(torch.nn.Module):\n",
        "    def __init__(self, node_features):\n",
        "        super(RecurrentGCN, self).__init__()\n",
        "        self.recurrent = TGCN(node_features, 32)\n",
        "        self.linear = torch.nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight, prev_hidden_state):\n",
        "        h = self.recurrent(x, edge_index, edge_weight, prev_hidden_state)\n",
        "        y = F.relu(h)\n",
        "        y = self.linear(y)\n",
        "        return y, h\n",
        "\n",
        "model = RecurrentGCN(node_features = 4)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in tqdm(range(50)):\n",
        "    cost = 0\n",
        "    hidden_state = None\n",
        "    for time, snapshot in enumerate(train_dataset):\n",
        "        y_hat, hidden_state = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr,hidden_state)\n",
        "        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n",
        "    cost = cost / (time+1)\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "model.eval()\n",
        "cost = 0\n",
        "hidden_state = None\n",
        "for time, snapshot in enumerate(test_dataset):\n",
        "    y_hat, hidden_state = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr, hidden_state)\n",
        "    cost = cost + torch.mean((y_hat-snapshot.y)**2)\n",
        "cost = cost / (time+1)\n",
        "cost = cost.item()\n",
        "print(\"MSE: {:.4f}\".format(cost))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdlYr4KnprOr",
        "outputId": "ad859943-17d2-4fa4-a867-4e5c2cd208d2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:20<00:00,  2.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 1.0027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: mount google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-ewruGCyaCn",
        "outputId": "6730d4cc-fdd0-4cee-88f3-aa503bc9fb7b"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "class ASLDatasetLoader:\n",
        "    def __init__(self, directory_path):\n",
        "        self.directory_path = directory_path\n",
        "        self.sign_to_label = self._create_sign_to_label_map()\n",
        "\n",
        "    def _create_sign_to_label_map(self):\n",
        "        signs = [os.path.splitext(filename)[0] for filename in os.listdir(self.directory_path)]\n",
        "        return {sign: i for i, sign in enumerate(signs)}\n",
        "\n",
        "    def _read_file_data(self, file_path):\n",
        "        with open(file_path, 'r') as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    def _augment_data(self, frame_data, rotation_range=10, translation_range=0.05, scaling_range=0.1):\n",
        "        \"\"\"\n",
        "        Augment the frame data with random rotation, translation, and scaling.\n",
        "\n",
        "        :param frame_data: Dictionary containing frame landmarks and deltas.\n",
        "        :param rotation_range: Maximum rotation angle in degrees.\n",
        "        :param translation_range: Maximum translation as a fraction of landmark range.\n",
        "        :param scaling_range: Maximum scaling factor.\n",
        "        :return: Augmented frame data.\n",
        "        \"\"\"\n",
        "        landmarks = np.array(frame_data[\"landmarks\"])\n",
        "        centroid = np.mean(landmarks, axis=0)\n",
        "\n",
        "        # Random rotation\n",
        "        theta = np.radians(np.random.uniform(-rotation_range, rotation_range))\n",
        "        rotation_matrix = np.array([\n",
        "            [np.cos(theta), -np.sin(theta)],\n",
        "            [np.sin(theta), np.cos(theta)]\n",
        "        ])\n",
        "        landmarks = np.dot(landmarks - centroid, rotation_matrix) + centroid\n",
        "\n",
        "        # Random translation\n",
        "        max_translation = translation_range * (landmarks.max(axis=0) - landmarks.min(axis=0))\n",
        "        translations = np.random.uniform(-max_translation, max_translation)\n",
        "        landmarks += translations\n",
        "\n",
        "        # Random scaling\n",
        "        scale = np.random.uniform(1 - scaling_range, 1 + scaling_range)\n",
        "        landmarks = centroid + scale * (landmarks - centroid)\n",
        "\n",
        "        frame_data[\"landmarks\"] = landmarks.tolist()\n",
        "        return frame_data\n",
        "\n",
        "\n",
        "    def _create_graph_from_frame(self, sign_name, frame_data):\n",
        "        landmarks = frame_data[\"landmarks\"]\n",
        "        deltas = frame_data[\"deltas\"]\n",
        "\n",
        "        # Create edges assuming landmarks are connected in sequence.\n",
        "        # For a more complex hand structure, this needs to be adapted.\n",
        "        edges = [[i, i+1] for i in range(len(landmarks) - 1)]\n",
        "\n",
        "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "        x = torch.tensor(np.hstack((landmarks, deltas)), dtype=torch.float)\n",
        "        y = torch.tensor([self.sign_to_label[sign_name]], dtype=torch.long)\n",
        "\n",
        "        return Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "    def get_dataset(self, augment=False):\n",
        "        dataset = []\n",
        "\n",
        "        for filename in os.listdir(self.directory_path):\n",
        "            sign_name = os.path.splitext(filename)[0]\n",
        "            file_path = os.path.join(self.directory_path, filename)\n",
        "            sign_data = self._read_file_data(file_path)\n",
        "\n",
        "            for frame_data in sign_data[\"frames\"]:\n",
        "                if augment:\n",
        "                  frame_data = self._augment_data(frame_data)\n",
        "                graph_data = self._create_graph_from_frame(sign_name, frame_data)\n",
        "\n",
        "                dataset.append(graph_data)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def number_of_classes(self):\n",
        "        return len(self.sign_to_label)"
      ],
      "metadata": {
        "id": "LifBvD3D4t4C"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool, BatchNorm\n",
        "\n",
        "class GraphClassifier(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, num_classes):\n",
        "        super(GraphClassifier, self).__init__()\n",
        "        self.conv1 = GCNConv(num_node_features, 128)\n",
        "        self.bn1 = BatchNorm(128)  # Batch Normalization after the first layer\n",
        "        self.conv2 = GCNConv(128, 64)\n",
        "        self.bn2 = BatchNorm(64)  # Batch Normalization after the second layer\n",
        "        self.fc = torch.nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        # First GCN layer with Batch Normalization\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "\n",
        "        # Second GCN layer with Batch Normalization\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Global pooling across nodes\n",
        "        x = global_mean_pool(x, data.batch)\n",
        "\n",
        "        # Final classification layer\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "metadata": {
        "id": "XpmOnloG40DP"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "\n",
        "def train():\n",
        "    directory_path = \"/content/drive/MyDrive/Colab Notebooks/DGMD E-14 Project/Datasets/ASL\"\n",
        "    loader = ASLDatasetLoader(directory_path)\n",
        "\n",
        "    # Create the entire dataset without augmentation\n",
        "    data_list = loader.get_dataset()\n",
        "\n",
        "    # Shuffle and split\n",
        "    random.shuffle(data_list)\n",
        "    train_size = int(0.8 * len(data_list))\n",
        "    train_dataset = data_list[:train_size]\n",
        "    test_dataset = data_list[train_size:]\n",
        "\n",
        "    # Now augment only the training dataset\n",
        "    augmented_train_dataset = loader.get_dataset(augment=True)\n",
        "    train_dataset = augmented_train_dataset[:train_size]  # Only take the initial portion corresponding to training\n",
        "\n",
        "    num_classes = loader.number_of_classes()\n",
        "\n",
        "    # Debugging code starts here\n",
        "    train_labels = [data.y.item() for data in train_dataset]\n",
        "    test_labels = [data.y.item() for data in test_dataset]\n",
        "\n",
        "    print(\"Training label distribution:\", Counter(train_labels))\n",
        "    print(\"Test label distribution:\", Counter(test_labels))\n",
        "    # Debugging code ends here\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # Adjust the number of node features based on your dataset\n",
        "    model = GraphClassifier(num_node_features=4, num_classes=num_classes).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(100):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            batch = batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(batch)\n",
        "            loss = F.nll_loss(out, batch.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in test_loader:\n",
        "        batch = batch.to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(batch).max(dim=1)[1]\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(batch.y.cpu().numpy())\n",
        "            correct += pred.eq(batch.y).sum().item()\n",
        "\n",
        "    print(f\"Accuracy: {correct / len(test_dataset)}\")\n",
        "\n",
        "    # Print a few predictions and true labels for inspection:\n",
        "    print(\"Sample predictions:\", all_preds[:20])\n",
        "    print(\"Sample true labels:\", all_labels[:20])"
      ],
      "metadata": {
        "id": "6QYi0OpiBuRK"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rr2PlwLy5M6H",
        "outputId": "8eaae0e9-6318-4242-b49d-b1149689d207"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training label distribution: Counter({0: 502, 3: 336, 5: 331, 6: 298, 2: 279, 4: 269, 1: 249})\n",
            "Test label distribution: Counter({0: 115, 5: 70, 6: 63, 7: 60, 2: 58, 3: 56, 8: 55, 1: 48, 4: 42})\n",
            "Epoch 0, Loss: 1.8325641306353286\n",
            "Epoch 1, Loss: 1.511253875745854\n",
            "Epoch 2, Loss: 1.3824609249410496\n",
            "Epoch 3, Loss: 1.2959549309502185\n",
            "Epoch 4, Loss: 1.2174891283814335\n",
            "Epoch 5, Loss: 1.1384455771513389\n",
            "Epoch 6, Loss: 1.114060253324643\n",
            "Epoch 7, Loss: 1.0601134963438545\n",
            "Epoch 8, Loss: 1.034084915275305\n",
            "Epoch 9, Loss: 1.0141520323887678\n",
            "Epoch 10, Loss: 0.9802168834377343\n",
            "Epoch 11, Loss: 0.9556333884386949\n",
            "Epoch 12, Loss: 0.9539522654573682\n",
            "Epoch 13, Loss: 0.9377628514464472\n",
            "Epoch 14, Loss: 0.9176177709874972\n",
            "Epoch 15, Loss: 0.9044702833806965\n",
            "Epoch 16, Loss: 0.9157127418988188\n",
            "Epoch 17, Loss: 0.8644599352084416\n",
            "Epoch 18, Loss: 0.8486766622100078\n",
            "Epoch 19, Loss: 0.8565347261831794\n",
            "Epoch 20, Loss: 0.8524264919925744\n",
            "Epoch 21, Loss: 0.8538088219266542\n",
            "Epoch 22, Loss: 0.8363908285826025\n",
            "Epoch 23, Loss: 0.8203699446060289\n",
            "Epoch 24, Loss: 0.8321447800582563\n",
            "Epoch 25, Loss: 0.8032791975518347\n",
            "Epoch 26, Loss: 0.8063233469573545\n",
            "Epoch 27, Loss: 0.8040533435176795\n",
            "Epoch 28, Loss: 0.8053558766841888\n",
            "Epoch 29, Loss: 0.7944158512941548\n",
            "Epoch 30, Loss: 0.803678143192345\n",
            "Epoch 31, Loss: 0.8049542845135004\n",
            "Epoch 32, Loss: 0.7524208149439852\n",
            "Epoch 33, Loss: 0.7767873108386993\n",
            "Epoch 34, Loss: 0.7649502829766609\n",
            "Epoch 35, Loss: 0.7631307504546474\n",
            "Epoch 36, Loss: 0.7693853008915001\n",
            "Epoch 37, Loss: 0.7435440249006513\n",
            "Epoch 38, Loss: 0.7808681189174383\n",
            "Epoch 39, Loss: 0.7719356279138109\n",
            "Epoch 40, Loss: 0.7631367205733984\n",
            "Epoch 41, Loss: 0.7381405544952607\n",
            "Epoch 42, Loss: 0.7346965662190612\n",
            "Epoch 43, Loss: 0.7429311636468054\n",
            "Epoch 44, Loss: 0.7259104596057409\n",
            "Epoch 45, Loss: 0.7168464543114246\n",
            "Epoch 46, Loss: 0.7175610455828654\n",
            "Epoch 47, Loss: 0.7031983447746492\n",
            "Epoch 48, Loss: 0.7183902272036378\n",
            "Epoch 49, Loss: 0.7323563841866775\n",
            "Epoch 50, Loss: 0.7041780415555121\n",
            "Epoch 51, Loss: 0.7403911545243061\n",
            "Epoch 52, Loss: 0.7156504354846309\n",
            "Epoch 53, Loss: 0.7122091050718872\n",
            "Epoch 54, Loss: 0.7291906362688038\n",
            "Epoch 55, Loss: 0.6999553865949872\n",
            "Epoch 56, Loss: 0.6965575415483662\n",
            "Epoch 57, Loss: 0.704484116023695\n",
            "Epoch 58, Loss: 0.7125298024063379\n",
            "Epoch 59, Loss: 0.7165110375679714\n",
            "Epoch 60, Loss: 0.688675409051734\n",
            "Epoch 61, Loss: 0.7014834843051265\n",
            "Epoch 62, Loss: 0.7081841184219844\n",
            "Epoch 63, Loss: 0.6873295718515423\n",
            "Epoch 64, Loss: 0.703158373144311\n",
            "Epoch 65, Loss: 0.6927023051490246\n",
            "Epoch 66, Loss: 0.6905721123789398\n",
            "Epoch 67, Loss: 0.6693717325237435\n",
            "Epoch 68, Loss: 0.6718103839478022\n",
            "Epoch 69, Loss: 0.6893554787400743\n",
            "Epoch 70, Loss: 0.6827897773662084\n",
            "Epoch 71, Loss: 0.6789266408329279\n",
            "Epoch 72, Loss: 0.6909371866306788\n",
            "Epoch 73, Loss: 0.6844991176900729\n",
            "Epoch 74, Loss: 0.6632487161058775\n",
            "Epoch 75, Loss: 0.6758232192254402\n",
            "Epoch 76, Loss: 0.6561445367168373\n",
            "Epoch 77, Loss: 0.6761202652689436\n",
            "Epoch 78, Loss: 0.6592043142923167\n",
            "Epoch 79, Loss: 0.6628712978161556\n",
            "Epoch 80, Loss: 0.6424416257462031\n",
            "Epoch 81, Loss: 0.6791480804832888\n",
            "Epoch 82, Loss: 0.6465770287412993\n",
            "Epoch 83, Loss: 0.66133743776402\n",
            "Epoch 84, Loss: 0.6672531479681042\n",
            "Epoch 85, Loss: 0.6701592416830466\n",
            "Epoch 86, Loss: 0.6637825693043184\n",
            "Epoch 87, Loss: 0.6557292514283892\n",
            "Epoch 88, Loss: 0.6611083330402912\n",
            "Epoch 89, Loss: 0.664629341011316\n",
            "Epoch 90, Loss: 0.659329955006989\n",
            "Epoch 91, Loss: 0.6711045474233762\n",
            "Epoch 92, Loss: 0.6790993469701686\n",
            "Epoch 93, Loss: 0.6652410970607274\n",
            "Epoch 94, Loss: 0.6635524920174773\n",
            "Epoch 95, Loss: 0.6531144279829213\n",
            "Epoch 96, Loss: 0.6449861866487584\n",
            "Epoch 97, Loss: 0.6512191761547411\n",
            "Epoch 98, Loss: 0.6661906183605463\n",
            "Epoch 99, Loss: 0.6450382966390797\n",
            "Accuracy: 0.7231040564373897\n",
            "Sample predictions: [2, 5, 3, 0, 0, 1, 4, 4, 1, 5, 4, 6, 4, 0, 5, 2, 3, 2, 3, 3]\n",
            "Sample true labels: [2, 4, 3, 0, 0, 1, 8, 8, 7, 5, 8, 6, 4, 0, 5, 2, 3, 2, 3, 3]\n"
          ]
        }
      ]
    }
  ]
}