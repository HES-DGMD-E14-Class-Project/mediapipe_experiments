{"cells":[{"cell_type":"markdown","metadata":{"id":"lLqJy6EZtrZB"},"source":["### Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"7-ewruGCyaCn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702269857115,"user_tz":420,"elapsed":20147,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}},"outputId":"d2d2919c-a879-404d-c6de-58c831c9b29d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"yLRvBRGgt5Zk"},"source":["#### Install Pytorch Geometric Temporal"]},{"cell_type":"code","source":["!pip install torch_geometric"],"metadata":{"id":"QRkM-wvTVBMw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702269864215,"user_tz":420,"elapsed":7102,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}},"outputId":"594fe82d-0e1f-4249-ea12-c92d0d61a09e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch_geometric\n","  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.11.17)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n","Installing collected packages: torch_geometric\n","Successfully installed torch_geometric-2.4.0\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"oiSS4wuxZ01m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702269877525,"user_tz":420,"elapsed":13318,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}},"outputId":"a5e8e77d-7eb6-4994-ed25-6f344edc3fd3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://data.pyg.org/whl/torch-2.1.0+cu118.html\n","Collecting pyg_lib\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/pyg_lib-0.3.1%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (2.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_scatter\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_scatter-2.1.2%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_sparse\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_sparse-0.6.18%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_cluster\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_cluster-1.6.3%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_spline_conv\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_spline_conv-1.2.2%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (887 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.8/887.8 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_sparse) (1.11.4)\n","Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch_sparse) (1.23.5)\n","Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n","Successfully installed pyg_lib-0.3.1+pt21cu118 torch_cluster-1.6.3+pt21cu118 torch_scatter-2.1.2+pt21cu118 torch_sparse-0.6.18+pt21cu118 torch_spline_conv-1.2.2+pt21cu118\n"]}],"source":["import torch\n","\n","TORCH = torch.__version__.split('+')[0]\n","CUDA = 'cu' + torch.version.cuda.replace('.', '')\n","\n","# Construct the installation command\n","install_command = f\"pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\"\n","\n","# Execute the command\n","!{install_command}"]},{"cell_type":"code","source":["import numpy as np\n","import os\n","\n","# Set a random seed for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""],"metadata":{"id":"upFSRna0XWWE","executionInfo":{"status":"ok","timestamp":1702269877747,"user_tz":420,"elapsed":227,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zi7c7EMpsCBX"},"source":["### `ASLDatasetLoader` Class\n","\n","The `ASLDatasetLoader` class is designed for loading and processing the ASL dataset. Given a directory, it reads sign language data from JSON files and constructs graph representations suitable for graph-based neural networks. Crucially, the class converts JSON data into PyTorch Geometric (PyG) `Data` objects comprising `x` (node features), `edge_index` (graph connectivity), and `y' (labels) attributes.\n","\n","**Methods**:\n","\n","- `_create_sign_to_label_map`: Generates a mapping from sign names to unique labels.\n","\n","- `_read_file_data`: Reads data from a given JSON file.\n","\n","- `_augment_data`: Implements data augmentation by applying random rotation, translation, and scaling to landmarks, which can enhance the model's robustness.\n","\n","- `_create_graph_from_frame`: Constructs a PyG `Data` object from frame data, concentrating on hand and face landmarks. Edges are created between consecutive landmarks and between left and right hand landmarks. Additional features, like hand-to-face distances, are also computed.\n","\n","- `get_dataset`: Assembles the dataset, optionally incorporating data augmentation. The function outputs a list of PyG `Data` objects ready for graph neural network processing."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"LifBvD3D4t4C","executionInfo":{"status":"ok","timestamp":1702269881170,"user_tz":420,"elapsed":3425,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"outputs":[],"source":["import json\n","from torch_geometric.data import Data\n","\n","class ASLDatasetLoader:\n","    # Define natural connections as class attributes\n","    HAND_CONNECTIONS = frozenset([\n","      # Left hand palm\n","      (\"left_hand-0\", \"left_hand-1\"),\n","      (\"left_hand-0\", \"left_hand-5\"),\n","      (\"left_hand-9\", \"left_hand-13\"),\n","      (\"left_hand-13\", \"left_hand-17\"),\n","      (\"left_hand-5\", \"left_hand-9\"),\n","      (\"left_hand-0\", \"left_hand-17\"),\n","      # Left hand thumb\n","      (\"left_hand-1\", \"left_hand-2\"),\n","      (\"left_hand-2\", \"left_hand-3\"),\n","      (\"left_hand-3\", \"left_hand-4\"),\n","      # Left hand index finger\n","      (\"left_hand-5\", \"left_hand-6\"),\n","      (\"left_hand-6\", \"left_hand-7\"),\n","      (\"left_hand-7\", \"left_hand-8\"),\n","      # Left hand middle finger\n","      (\"left_hand-9\", \"left_hand-10\"),\n","      (\"left_hand-10\", \"left_hand-11\"),\n","      (\"left_hand-11\", \"left_hand-12\"),\n","      # Left hand ring finger\n","      (\"left_hand-13\", \"left_hand-14\"),\n","      (\"left_hand-14\", \"left_hand-15\"),\n","      (\"left_hand-15\", \"left_hand-16\"),\n","      # Left hand pinky\n","      (\"left_hand-17\", \"left_hand-18\"),\n","      (\"left_hand-18\", \"left_hand-19\"),\n","      (\"left_hand-19\", \"left_hand-20\"),\n","      # Right hand palm\n","      (\"right_hand-0\", \"right_hand-1\"),\n","      (\"right_hand-0\", \"right_hand-5\"),\n","      (\"right_hand-9\", \"right_hand-13\"),\n","      (\"right_hand-13\", \"right_hand-17\"),\n","      (\"right_hand-5\", \"right_hand-9\"),\n","      (\"right_hand-0\", \"right_hand-17\"),\n","      # Right hand thumb\n","      (\"right_hand-1\", \"right_hand-2\"),\n","      (\"right_hand-2\", \"right_hand-3\"),\n","      (\"right_hand-3\", \"right_hand-4\"),\n","      # Right hand index finger\n","      (\"right_hand-5\", \"right_hand-6\"),\n","      (\"right_hand-6\", \"right_hand-7\"),\n","      (\"right_hand-7\", \"right_hand-8\"),\n","      # Right hand middle finger\n","      (\"right_hand-9\", \"right_hand-10\"),\n","      (\"right_hand-10\", \"right_hand-11\"),\n","      (\"right_hand-11\", \"right_hand-12\"),\n","      # Right hand ring finger\n","      (\"right_hand-13\", \"right_hand-14\"),\n","      (\"right_hand-14\", \"right_hand-15\"),\n","      (\"right_hand-15\", \"right_hand-16\"),\n","      # Right hand pinky\n","      (\"right_hand-17\", \"right_hand-18\"),\n","      (\"right_hand-18\", \"right_hand-19\"),\n","      (\"right_hand-19\", \"right_hand-20\"),\n","    ])\n","\n","    POSE_CONNECTIONS = frozenset([\n","      (\"pose-0\", \"pose-1\"),\n","      (\"pose-1\", \"pose-2\"),\n","      (\"pose-2\", \"pose-3\"),\n","      (\"pose-3\", \"pose-7\"),\n","      (\"pose-0\", \"pose-4\"),\n","      (\"pose-4\", \"pose-5\"),\n","      (\"pose-5\", \"pose-6\"),\n","      (\"pose-6\", \"pose-8\"),\n","      (\"pose-9\", \"pose-10\"),\n","      (\"pose-11\", \"pose-12\"),\n","      (\"pose-11\", \"pose-13\"),\n","      (\"pose-13\", \"pose-15\"),\n","      (\"pose-15\", \"pose-17\"),\n","      (\"pose-12\", \"pose-14\"),\n","      (\"pose-14\", \"pose-16\"),\n","      (\"pose-16\", \"pose-18\"),\n","      (\"pose-11\", \"pose-23\"),\n","      (\"pose-12\", \"pose-24\"),\n","      (\"pose-23\", \"pose-24\"),\n","    ])\n","\n","    FACE_CONNECTIONS = frozenset([\n","      # Connections for FACEMESH_LIPS using available landmarks\n","      (\"face-61\", \"face-146\"), (\"face-146\", \"face-91\"), (\"face-91\", \"face-181\"),\n","      (\"face-181\", \"face-84\"), (\"face-84\", \"face-17\"), (\"face-17\", \"face-314\"),\n","      (\"face-314\", \"face-405\"), (\"face-405\", \"face-321\"), (\"face-321\", \"face-375\"),\n","      (\"face-375\", \"face-291\"), (\"face-78\", \"face-95\"), (\"face-95\", \"face-88\"),\n","      (\"face-88\", \"face-178\"), (\"face-178\", \"face-87\"), (\"face-87\", \"face-14\"),\n","      (\"face-14\", \"face-317\"), (\"face-317\", \"face-402\"), (\"face-402\", \"face-318\"),\n","      (\"face-318\", \"face-324\"), (\"face-324\", \"face-308\"),\n","\n","      # Connections for FACEMESH_LEFT_EYE using available landmarks\n","      (\"face-263\", \"face-249\"), (\"face-388\", \"face-387\"), (\"face-387\", \"face-386\"),\n","      (\"face-386\", \"face-385\"), (\"face-385\", \"face-384\"), (\"face-384\", \"face-398\"),\n","\n","      # Connections for FACEMESH_LEFT_EYEBROW using available landmarks\n","      (\"face-276\", \"face-283\"), (\"face-300\", \"face-293\"), (\"face-293\", \"face-334\"),\n","      (\"face-334\", \"face-296\"), (\"face-296\", \"face-336\"),\n","\n","      # Connections for FACEMESH_RIGHT_EYE using available landmarks\n","      (\"face-33\", \"face-7\"), (\"face-246\", \"face-161\"), (\"face-161\", \"face-160\"),\n","      (\"face-160\", \"face-159\"), (\"face-159\", \"face-158\"), (\"face-158\", \"face-157\"),\n","      (\"face-157\", \"face-173\"),\n","\n","      # Connections for FACEMESH_RIGHT_EYEBROW using available landmarks\n","      (\"face-46\", \"face-53\"), (\"face-70\", \"face-63\"), (\"face-63\", \"face-105\"),\n","      (\"face-105\", \"face-66\"), (\"face-66\", \"face-107\"),\n","\n","      # Connections for FACEMESH_FACE_OVAL using available landmarks\n","      (\"face-10\", \"face-338\"), (\"face-338\", \"face-297\"), (\"face-297\", \"face-332\"),\n","      (\"face-332\", \"face-284\"), (\"face-284\", \"face-251\"), (\"face-251\", \"face-389\"),\n","      (\"face-389\", \"face-356\"), (\"face-356\", \"face-454\"), (\"face-454\", \"face-323\"),\n","      (\"face-323\", \"face-361\"), (\"face-361\", \"face-288\"), (\"face-288\", \"face-397\"),\n","      (\"face-397\", \"face-365\"), (\"face-365\", \"face-379\"), (\"face-379\", \"face-378\"),\n","      (\"face-378\", \"face-400\"), (\"face-400\", \"face-377\"), (\"face-377\", \"face-152\"),\n","      (\"face-152\", \"face-148\"), (\"face-148\", \"face-176\"), (\"face-176\", \"face-149\"),\n","      (\"face-149\", \"face-150\"), (\"face-150\", \"face-136\"), (\"face-136\", \"face-172\"),\n","      (\"face-172\", \"face-58\"), (\"face-58\", \"face-132\"), (\"face-132\", \"face-93\"),\n","      (\"face-93\", \"face-234\"), (\"face-234\", \"face-127\"), (\"face-127\", \"face-162\"),\n","      (\"face-162\", \"face-21\"), (\"face-21\", \"face-54\"), (\"face-54\", \"face-103\"),\n","      (\"face-103\", \"face-67\"), (\"face-67\", \"face-109\"), (\"face-109\", \"face-10\"),\n","    ])\n","\n","    def __init__(self, directory_path, min_examples_per_class=2, max_files=-1):\n","        self.directory_path = directory_path\n","        self.min_examples_per_class = min_examples_per_class\n","        self.max_files = max_files\n","        self.sign_to_label = self._create_sign_to_label_map()\n","\n","    def _create_sign_to_label_map(self):\n","        signs = [os.path.splitext(filename)[0] for filename in os.listdir(self.directory_path)]\n","        return {sign: i for i, sign in enumerate(signs)}\n","\n","    def _read_file_data(self, file_path):\n","        with open(file_path, 'r') as f:\n","            data = json.load(f)\n","\n","        sign_name = data[\"sign\"]\n","        num_examples = len(data[\"examples\"])\n","        print(f\"Loaded sign '{sign_name}' with {num_examples} examples\")\n","\n","        return data if num_examples >= self.min_examples_per_class else None\n","\n","    def _augment_data(self, frame_data, rotation_range=10, translation_range=0.05, scaling_range=0.1, jittering_range=0.01, noise_scale=0.01, mirroring=False):\n","        \"\"\"\n","        Augment the frame data with various techniques including rotation, translation, scaling, jittering, noise injection, and mirroring.\n","\n","        :param frame_data: Dictionary containing frame landmarks and deltas.\n","        :param rotation_range: Maximum rotation angle in degrees.\n","        :param translation_range: Maximum translation as a fraction of landmark range.\n","        :param scaling_range: Maximum scaling factor.\n","        :param jittering_range: Range for jittering.\n","        :param noise_scale: Scale of the random noise to be added.\n","        :param mirroring: Whether to mirror the landmarks (simulate opposite hand).\n","        :return: Augmented frame data.\n","        \"\"\"\n","        # Extract landmarks\n","        landmarks = np.array([[landmark['x'], landmark['y']] for landmark in frame_data['landmarks']])\n","        centroid = np.mean(landmarks, axis=0)\n","\n","        # Jittering\n","        jittering = np.random.uniform(-jittering_range, jittering_range, landmarks.shape)\n","        landmarks += jittering\n","\n","        # Random rotation\n","        theta = np.radians(np.random.uniform(-rotation_range, rotation_range))\n","        rotation_matrix = np.array([\n","            [np.cos(theta), -np.sin(theta)],\n","            [np.sin(theta), np.cos(theta)]\n","        ])\n","        landmarks = np.dot(landmarks - centroid, rotation_matrix) + centroid\n","\n","        # Random translation\n","        max_translation = translation_range * (landmarks.max(axis=0) - landmarks.min(axis=0))\n","        translations = np.random.uniform(-max_translation, max_translation, size=landmarks.shape[1])\n","        landmarks += translations\n","\n","        # Random scaling\n","        scale = np.random.uniform(1 - scaling_range, 1 + scaling_range)\n","        landmarks = centroid + scale * (landmarks - centroid)\n","\n","        # Noise Injection\n","        noise = np.random.normal(0, noise_scale, landmarks.shape)\n","        landmarks += noise\n","\n","        # Mirroring (if applicable)\n","        if mirroring:\n","            landmarks[:, 0] = -landmarks[:, 0] + 2 * centroid[0]  # Reflect x-coordinates\n","\n","        # Update the landmarks in frame_data\n","        for i, landmark in enumerate(frame_data['landmarks']):\n","            landmark['x'], landmark['y'] = landmarks[i]\n","\n","        return frame_data\n","\n","\n","    def _create_graph_from_frame(self, sign_name, sign_data):\n","        graphs = []\n","\n","        for example in sign_data[\"examples\"]:\n","            all_features = []  # Combined list for landmarks, velocities, and accelerations\n","            edges = []\n","\n","            for frame in example[\"frames\"]:\n","                for landmark_data in frame[\"landmarks\"]:\n","                    # Extract spatial coordinates\n","                    landmark_features = [landmark_data[\"x\"], landmark_data[\"y\"]]\n","\n","                    # Extract temporal data (velocity and acceleration)\n","                    temporal_data = next((item for item in frame[\"temporal\"] if item[\"landmark\"] == landmark_data[\"landmark\"]), None)\n","                    if temporal_data:\n","                        velocity = [temporal_data[\"velocity\"][\"x\"], temporal_data[\"velocity\"][\"y\"]]\n","                        acceleration = [temporal_data[\"acceleration\"][\"x\"], temporal_data[\"acceleration\"][\"y\"]]\n","                    else:\n","                        velocity = [0, 0]\n","                        acceleration = [0, 0]\n","\n","                    # Combine spatial and temporal features\n","                    combined_features = landmark_features + velocity + acceleration\n","                    all_features.append(combined_features)\n","\n","                # Add spatial edges within the frame using natural connections\n","                for i in range(len(frame[\"landmarks\"])):\n","                    for j in range(len(frame[\"landmarks\"])):\n","                        if i != j:\n","                            connection = (frame[\"landmarks\"][i][\"landmark\"], frame[\"landmarks\"][j][\"landmark\"])\n","                            if connection in self.HAND_CONNECTIONS or \\\n","                              connection in self.POSE_CONNECTIONS or \\\n","                              connection in self.FACE_CONNECTIONS:\n","                                edges.append([len(all_features) - len(frame[\"landmarks\"]) + i,\n","                                              len(all_features) - len(frame[\"landmarks\"]) + j])\n","\n","            # Add temporal edges between frames within each example\n","            for i in range(len(example[\"frames\"]) - 1):\n","                for j in range(len(frame[\"landmarks\"])):\n","                    start_index = i * len(frame[\"landmarks\"]) + j\n","                    end_index = (i + 1) * len(frame[\"landmarks\"]) + j\n","                    edges.append([start_index, end_index])\n","\n","            # Create the graph\n","            x = torch.tensor(all_features, dtype=torch.float)\n","            edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n","            y = torch.tensor([self.sign_to_label[sign_name]], dtype=torch.long)\n","\n","            # Append the graph for the current example to the list of graphs\n","            graphs.append(Data(x=x, edge_index=edge_index, y=y))\n","\n","        return graphs\n","\n","    def get_dataset(self, augment=False):\n","        dataset = []\n","        file_count = 0\n","\n","        for filename in os.listdir(self.directory_path):\n","            if 0 <= self.max_files <= file_count:\n","                break  # Stop if max_files limit is reached\n","\n","            sign_name = os.path.splitext(filename)[0]\n","            file_path = os.path.join(self.directory_path, filename)\n","            sign_data = self._read_file_data(file_path)\n","            file_count += 1\n","\n","            if sign_data is None:\n","                print(f\"Skipping sign '{sign_name}' due to insufficient examples\")\n","                continue\n","\n","            # Retrieve a list of graphs, one for each example\n","            graphs = self._create_graph_from_frame(sign_name, sign_data)\n","\n","            # Debugging: Check the number of graphs created for the current sign\n","            print(f\"Sign '{sign_name}': Created {len(graphs)} graphs\")\n","\n","            dataset.extend(graphs)  # Extend the dataset with the list of graphs\n","\n","        return dataset\n","\n","    def number_of_classes(self):\n","        return len(self.sign_to_label)"]},{"cell_type":"markdown","metadata":{"id":"xooW2sogtdL1"},"source":["### `ASLGraphClassifier` Class\n","\n","The `ASLGraphClassifier`, features deeper GCN layers and additional channels to capture intricate data patterns potentially. It takes a PyG `Data` object as input, and its forward pass emits class logits.\n","\n","**Methods**:\n","\n","- `forward`: Details the forward pass, accepting a PyG `Data` object. Two GCN layers with subsequent batch normalization and dropout layers process the input. Post global max-pooling, two linear layers coupled with dropout ensure final classification, leading to log-softmax outputs."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"RAslUK79VVV6","executionInfo":{"status":"ok","timestamp":1702269881171,"user_tz":420,"elapsed":4,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"outputs":[],"source":["import torch.nn.functional as F\n","from torch_geometric.nn import GCNConv, BatchNorm, global_max_pool, LayerNorm\n","\n","class ASLGraphClassifier(torch.nn.Module):\n","    def __init__(self, num_features, num_classes):\n","        super(ASLGraphClassifier, self).__init__()\n","        self.conv1 = GCNConv(num_features, 512)\n","        self.bn1 = BatchNorm(512)\n","        self.conv2 = GCNConv(512, 1024)\n","        self.bn2 = BatchNorm(1024)\n","        self.ln1 = LayerNorm(1024)  # Layer normalization\n","        self.lin1 = torch.nn.Linear(1024, 512)\n","        self.ln2 = LayerNorm(512)  # Layer normalization\n","        self.lin2 = torch.nn.Linear(512, num_classes)\n","\n","        self.dropout = torch.nn.Dropout(p=0.7)\n","\n","    def forward(self, data):\n","        x, edge_index, batch = data.x, data.edge_index, data.batch\n","\n","        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n","        x = self.dropout(x)\n","        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n","        x = self.ln1(x)  # Apply layer normalization\n","        x = self.dropout(x)\n","\n","        x = global_max_pool(x, batch)\n","\n","        x = F.relu(self.lin1(x))\n","        x = self.ln2(x)  # Apply layer normalization\n","        x = self.dropout(x)\n","        x = self.lin2(x)\n","        return F.log_softmax(x, dim=1)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"RdBGsFveWcbF","executionInfo":{"status":"ok","timestamp":1702269881980,"user_tz":420,"elapsed":812,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from sklearn.model_selection import train_test_split\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch_geometric.loader import DataLoader\n","from collections import Counter\n","\n","def stratified_data_split(data_list, test_size=0.2):\n","    labels = [data.y.item() for data in data_list]\n","    train_data, test_data = train_test_split(data_list, test_size=test_size, stratify=labels, random_state=42)\n","    return train_data, test_data\n","\n","def validate(loader, model, device):\n","    model.eval()\n","    correct = 0\n","    for data in loader:\n","        data = data.to(device)\n","        with torch.no_grad():\n","            out = model(data)\n","        pred = out.argmax(dim=1)\n","        correct += int((pred == data.y).sum())\n","    return correct / len(loader.dataset)\n","\n","def train(loader, EPOCHS=150, LEARNING_RATE=0.0001):\n","    # Stratified split of the data\n","    data_list = loader.get_dataset()\n","    train_dataset, test_dataset = stratified_data_split(data_list, test_size=0.2)\n","    augmented_train_dataset = loader.get_dataset(augment=True)\n","\n","    num_classes = loader.number_of_classes()\n","    train_labels = [data.y.item() for data in train_dataset]\n","    test_labels = [data.y.item() for data in test_dataset]\n","\n","    print(\"Training label distribution:\", Counter(train_labels))\n","    print(\"Test label distribution:\", Counter(test_labels))\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    num_features = train_dataset[0].x.size(1)\n","    model = ASLGraphClassifier(num_features=num_features, num_classes=num_classes).to(device)\n","\n","    initial_batch_size = 64\n","    final_batch_size = 16\n","    warmup_epochs = 5\n","    initial_lr = 0.00001\n","    validate_every_n_batches = 60\n","    patience=15\n","    weight_decay=1e-5\n","    reduction_factor=0.8\n","\n","    current_batch_size = initial_batch_size\n","    train_loader = DataLoader(train_dataset, batch_size=current_batch_size, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=weight_decay)\n","    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.7, patience=patience, verbose=True)\n","\n","    l1_lambda = 1e-6\n","    l2_lambda = 1e-5\n","\n","    best_val_accuracy = 0\n","    epochs_without_improvement = 0\n","    max_epochs_without_improvement = 20\n","\n","    model.train()\n","    for epoch in range(EPOCHS):\n","        total_loss = 0\n","        correct_train = 0\n","        total_train = 0\n","        batch_count = 0\n","\n","        for batch in train_loader:\n","            batch = batch.to(device)\n","            optimizer.zero_grad()\n","            out = model(batch)\n","\n","            l1_reg = sum(param.abs().sum() for param in model.parameters())\n","            l2_reg = sum(param.pow(2).sum() for param in model.parameters())\n","            loss = F.nll_loss(out, batch.y) + l1_lambda * l1_reg + l2_lambda * l2_reg\n","\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","            pred_train = out.argmax(dim=1)\n","            correct_train += int((pred_train == batch.y).sum())\n","            total_train += batch.y.size(0)\n","\n","            batch_count += 1\n","            if batch_count % validate_every_n_batches == 0:\n","                val_accuracy = validate(test_loader, model, device)\n","                print(f\"Validation Accuracy after {batch_count} batches: {val_accuracy:.4f}\")\n","\n","        if epoch < warmup_epochs:\n","            lr = initial_lr + (LEARNING_RATE - initial_lr) * (epoch / warmup_epochs)\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] = lr\n","\n","        if epoch % 10 == 0 and current_batch_size > final_batch_size:\n","            current_batch_size //= 2\n","            train_loader = DataLoader(train_dataset, batch_size=current_batch_size, shuffle=True)\n","\n","        avg_loss = total_loss / len(train_loader)\n","        train_accuracy = correct_train / total_train\n","        val_accuracy = validate(test_loader, model, device)\n","\n","        print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n","        scheduler.step(val_accuracy)\n","\n","        if val_accuracy > best_val_accuracy:\n","            best_val_accuracy = val_accuracy\n","            epochs_without_improvement = 0\n","        else:\n","            epochs_without_improvement += 1\n","\n","        if epochs_without_improvement >= max_epochs_without_improvement:\n","            print(\"Early stopping triggered.\")\n","            break\n","\n","    model.eval()\n","    correct = 0\n","    all_preds = []\n","    all_labels = []\n","\n","    for batch in test_loader:\n","        batch = batch.to(device)\n","        with torch.no_grad():\n","            pred = model(batch).max(dim=1)[1]\n","            all_preds.extend(pred.cpu().numpy())\n","            all_labels.extend(batch.y.cpu().numpy())\n","            correct += pred.eq(batch.y).sum().item()\n","\n","    accuracy = correct / len(test_dataset)\n","    print(f\"Accuracy: {accuracy}\")\n","    print(\"Sample predictions:\", all_preds[:20])\n","    print(\"Sample true labels:\", all_labels[:20])\n","\n","    return model, all_preds, all_labels, accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rr2PlwLy5M6H","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7b4bf944-9e27-40fd-8ff5-15cc07253504"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded sign 'TV' with 385 examples\n","Sign 'TV': Created 385 graphs\n","Loaded sign 'after' with 347 examples\n","Sign 'after': Created 347 graphs\n","Loaded sign 'airplane' with 393 examples\n","Sign 'airplane': Created 393 graphs\n","Loaded sign 'all' with 386 examples\n","Sign 'all': Created 386 graphs\n","Loaded sign 'alligator' with 390 examples\n","Sign 'alligator': Created 390 graphs\n","Loaded sign 'bird' with 404 examples\n","Sign 'bird': Created 404 graphs\n","Loaded sign 'callonphone' with 385 examples\n","Sign 'callonphone': Created 385 graphs\n","Loaded sign 'cry' with 390 examples\n","Sign 'cry': Created 390 graphs\n","Loaded sign 'dad' with 378 examples\n","Sign 'dad': Created 378 graphs\n","Loaded sign 'dance' with 312 examples\n","Sign 'dance': Created 312 graphs\n","Loaded sign 'dog' with 380 examples\n","Sign 'dog': Created 380 graphs\n","Loaded sign 'drink' with 400 examples\n","Sign 'drink': Created 400 graphs\n","Loaded sign 'duck' with 405 examples\n","Sign 'duck': Created 405 graphs\n","Loaded sign 'elephant' with 382 examples\n","Sign 'elephant': Created 382 graphs\n","Loaded sign 'eye' with 395 examples\n","Sign 'eye': Created 395 graphs\n","Loaded sign 'feet' with 372 examples\n","Sign 'feet': Created 372 graphs\n","Loaded sign 'finger' with 379 examples\n","Sign 'finger': Created 379 graphs\n","Loaded sign 'flower' with 396 examples\n","Sign 'flower': Created 396 graphs\n","Loaded sign 'food' with 392 examples\n","Sign 'food': Created 392 graphs\n","Loaded sign 'face' with 370 examples\n","Sign 'face': Created 370 graphs\n","Loaded sign 'TV' with 385 examples\n","Sign 'TV': Created 385 graphs\n","Loaded sign 'after' with 347 examples\n","Sign 'after': Created 347 graphs\n","Loaded sign 'airplane' with 393 examples\n","Sign 'airplane': Created 393 graphs\n","Loaded sign 'all' with 386 examples\n","Sign 'all': Created 386 graphs\n","Loaded sign 'alligator' with 390 examples\n","Sign 'alligator': Created 390 graphs\n","Loaded sign 'bird' with 404 examples\n","Sign 'bird': Created 404 graphs\n","Loaded sign 'callonphone' with 385 examples\n","Sign 'callonphone': Created 385 graphs\n","Loaded sign 'cry' with 390 examples\n","Sign 'cry': Created 390 graphs\n","Loaded sign 'dad' with 378 examples\n","Sign 'dad': Created 378 graphs\n","Loaded sign 'dance' with 312 examples\n","Sign 'dance': Created 312 graphs\n","Loaded sign 'dog' with 380 examples\n","Sign 'dog': Created 380 graphs\n","Loaded sign 'drink' with 400 examples\n","Sign 'drink': Created 400 graphs\n","Loaded sign 'duck' with 405 examples\n","Sign 'duck': Created 405 graphs\n","Loaded sign 'elephant' with 382 examples\n","Sign 'elephant': Created 382 graphs\n","Loaded sign 'eye' with 395 examples\n","Sign 'eye': Created 395 graphs\n","Loaded sign 'feet' with 372 examples\n","Sign 'feet': Created 372 graphs\n","Loaded sign 'finger' with 379 examples\n","Sign 'finger': Created 379 graphs\n","Loaded sign 'flower' with 396 examples\n","Sign 'flower': Created 396 graphs\n","Loaded sign 'food' with 392 examples\n","Sign 'food': Created 392 graphs\n","Loaded sign 'face' with 370 examples\n","Sign 'face': Created 370 graphs\n","Training label distribution: Counter({12: 324, 5: 323, 11: 320, 17: 317, 14: 316, 2: 314, 18: 313, 4: 312, 7: 312, 3: 309, 6: 308, 0: 308, 13: 305, 10: 304, 16: 303, 8: 302, 15: 298, 19: 296, 1: 278, 9: 250})\n","Test label distribution: Counter({12: 81, 5: 81, 11: 80, 17: 79, 18: 79, 14: 79, 2: 79, 4: 78, 7: 78, 6: 77, 0: 77, 3: 77, 13: 77, 10: 76, 16: 76, 8: 76, 19: 74, 15: 74, 1: 69, 9: 62})\n","Validation Accuracy after 60 batches: 0.0556\n","Epoch 0, Loss: 1.6589, Training Accuracy: 0.0553, Validation Accuracy: 0.0621\n","Validation Accuracy after 60 batches: 0.0961\n","Validation Accuracy after 120 batches: 0.1033\n","Validation Accuracy after 180 batches: 0.0981\n","Epoch 1, Loss: 3.0292, Training Accuracy: 0.0893, Validation Accuracy: 0.0831\n","Validation Accuracy after 60 batches: 0.0935\n","Validation Accuracy after 120 batches: 0.0889\n","Validation Accuracy after 180 batches: 0.1302\n","Epoch 2, Loss: 2.9888, Training Accuracy: 0.1049, Validation Accuracy: 0.1118\n","Validation Accuracy after 60 batches: 0.1302\n","Validation Accuracy after 120 batches: 0.1334\n","Validation Accuracy after 180 batches: 0.1596\n","Epoch 3, Loss: 2.8512, Training Accuracy: 0.1360, Validation Accuracy: 0.1733\n","Validation Accuracy after 60 batches: 0.1759\n","Validation Accuracy after 120 batches: 0.1929\n","Validation Accuracy after 180 batches: 0.2034\n","Epoch 4, Loss: 2.6682, Training Accuracy: 0.1980, Validation Accuracy: 0.2001\n","Validation Accuracy after 60 batches: 0.2263\n","Validation Accuracy after 120 batches: 0.2472\n","Validation Accuracy after 180 batches: 0.2518\n","Epoch 5, Loss: 2.5610, Training Accuracy: 0.2217, Validation Accuracy: 0.2629\n","Validation Accuracy after 60 batches: 0.2780\n","Validation Accuracy after 120 batches: 0.2610\n","Validation Accuracy after 180 batches: 0.2649\n","Epoch 6, Loss: 2.4374, Training Accuracy: 0.2665, Validation Accuracy: 0.2603\n","Validation Accuracy after 60 batches: 0.2825\n","Validation Accuracy after 120 batches: 0.2740\n","Validation Accuracy after 180 batches: 0.2950\n","Epoch 7, Loss: 2.3368, Training Accuracy: 0.3001, Validation Accuracy: 0.2923\n","Validation Accuracy after 60 batches: 0.3048\n","Validation Accuracy after 120 batches: 0.3094\n","Validation Accuracy after 180 batches: 0.3165\n","Epoch 8, Loss: 2.2358, Training Accuracy: 0.3406, Validation Accuracy: 0.3165\n","Validation Accuracy after 60 batches: 0.3146\n","Validation Accuracy after 120 batches: 0.3479\n","Validation Accuracy after 180 batches: 0.3506\n","Epoch 9, Loss: 2.1629, Training Accuracy: 0.3650, Validation Accuracy: 0.3434\n","Validation Accuracy after 60 batches: 0.3499\n","Validation Accuracy after 120 batches: 0.3623\n","Validation Accuracy after 180 batches: 0.3695\n","Epoch 10, Loss: 1.0464, Training Accuracy: 0.3848, Validation Accuracy: 0.3492\n","Validation Accuracy after 60 batches: 0.3224\n","Validation Accuracy after 120 batches: 0.3499\n","Validation Accuracy after 180 batches: 0.3316\n","Validation Accuracy after 240 batches: 0.3506\n","Validation Accuracy after 300 batches: 0.3087\n","Validation Accuracy after 360 batches: 0.3120\n","Epoch 11, Loss: 2.1043, Training Accuracy: 0.3770, Validation Accuracy: 0.3617\n","Validation Accuracy after 60 batches: 0.3584\n","Validation Accuracy after 120 batches: 0.3571\n","Validation Accuracy after 180 batches: 0.3748\n","Validation Accuracy after 240 batches: 0.3957\n","Validation Accuracy after 300 batches: 0.3695\n","Validation Accuracy after 360 batches: 0.3721\n","Epoch 12, Loss: 2.0126, Training Accuracy: 0.4002, Validation Accuracy: 0.3492\n","Validation Accuracy after 60 batches: 0.3813\n","Validation Accuracy after 120 batches: 0.3604\n","Validation Accuracy after 180 batches: 0.3702\n","Validation Accuracy after 240 batches: 0.3774\n","Validation Accuracy after 300 batches: 0.3787\n","Validation Accuracy after 360 batches: 0.4055\n","Epoch 13, Loss: 1.9538, Training Accuracy: 0.4221, Validation Accuracy: 0.3342\n","Validation Accuracy after 60 batches: 0.4173\n","Validation Accuracy after 120 batches: 0.3990\n","Validation Accuracy after 180 batches: 0.3931\n","Validation Accuracy after 240 batches: 0.3813\n","Validation Accuracy after 300 batches: 0.4022\n","Validation Accuracy after 360 batches: 0.3819\n","Epoch 14, Loss: 1.8738, Training Accuracy: 0.4416, Validation Accuracy: 0.3963\n","Validation Accuracy after 60 batches: 0.4042\n","Validation Accuracy after 120 batches: 0.4232\n","Validation Accuracy after 180 batches: 0.4120\n","Validation Accuracy after 240 batches: 0.4042\n","Validation Accuracy after 300 batches: 0.4277\n","Validation Accuracy after 360 batches: 0.4232\n","Epoch 15, Loss: 1.8303, Training Accuracy: 0.4598, Validation Accuracy: 0.4205\n","Validation Accuracy after 60 batches: 0.4160\n","Validation Accuracy after 120 batches: 0.4173\n","Validation Accuracy after 180 batches: 0.4388\n","Validation Accuracy after 240 batches: 0.4192\n","Validation Accuracy after 300 batches: 0.4199\n","Validation Accuracy after 360 batches: 0.4264\n","Epoch 16, Loss: 1.7677, Training Accuracy: 0.4786, Validation Accuracy: 0.4179\n","Validation Accuracy after 60 batches: 0.4081\n","Validation Accuracy after 120 batches: 0.4356\n","Validation Accuracy after 180 batches: 0.4186\n","Validation Accuracy after 240 batches: 0.4107\n","Validation Accuracy after 300 batches: 0.4310\n","Validation Accuracy after 360 batches: 0.4336\n","Epoch 17, Loss: 1.7105, Training Accuracy: 0.4900, Validation Accuracy: 0.4336\n","Validation Accuracy after 60 batches: 0.4356\n","Validation Accuracy after 120 batches: 0.4388\n","Validation Accuracy after 180 batches: 0.4186\n","Validation Accuracy after 240 batches: 0.4395\n","Validation Accuracy after 300 batches: 0.4408\n","Validation Accuracy after 360 batches: 0.4559\n","Epoch 18, Loss: 1.6572, Training Accuracy: 0.5074, Validation Accuracy: 0.4375\n","Validation Accuracy after 60 batches: 0.4598\n","Validation Accuracy after 120 batches: 0.4395\n","Validation Accuracy after 180 batches: 0.4493\n","Validation Accuracy after 240 batches: 0.4441\n","Validation Accuracy after 300 batches: 0.4330\n","Validation Accuracy after 360 batches: 0.4421\n","Epoch 19, Loss: 1.6099, Training Accuracy: 0.5245, Validation Accuracy: 0.4330\n","Validation Accuracy after 60 batches: 0.4434\n","Validation Accuracy after 120 batches: 0.4506\n","Validation Accuracy after 180 batches: 0.4487\n","Validation Accuracy after 240 batches: 0.4480\n","Validation Accuracy after 300 batches: 0.4637\n","Validation Accuracy after 360 batches: 0.4218\n","Epoch 20, Loss: 1.5598, Training Accuracy: 0.5416, Validation Accuracy: 0.4519\n","Validation Accuracy after 60 batches: 0.4572\n","Validation Accuracy after 120 batches: 0.4716\n","Validation Accuracy after 180 batches: 0.4604\n","Validation Accuracy after 240 batches: 0.4447\n","Validation Accuracy after 300 batches: 0.4611\n","Validation Accuracy after 360 batches: 0.4611\n","Epoch 21, Loss: 1.5087, Training Accuracy: 0.5533, Validation Accuracy: 0.4683\n","Validation Accuracy after 60 batches: 0.4735\n","Validation Accuracy after 120 batches: 0.4912\n","Validation Accuracy after 180 batches: 0.4722\n","Validation Accuracy after 240 batches: 0.4604\n","Validation Accuracy after 300 batches: 0.4846\n","Validation Accuracy after 360 batches: 0.4500\n","Epoch 22, Loss: 1.4605, Training Accuracy: 0.5659, Validation Accuracy: 0.4866\n","Validation Accuracy after 60 batches: 0.4513\n","Validation Accuracy after 120 batches: 0.4559\n","Validation Accuracy after 180 batches: 0.4833\n","Validation Accuracy after 240 batches: 0.4755\n","Validation Accuracy after 300 batches: 0.4774\n","Validation Accuracy after 360 batches: 0.4591\n","Epoch 23, Loss: 1.4259, Training Accuracy: 0.5762, Validation Accuracy: 0.4820\n","Validation Accuracy after 60 batches: 0.4604\n","Validation Accuracy after 120 batches: 0.4755\n","Validation Accuracy after 180 batches: 0.4833\n","Validation Accuracy after 240 batches: 0.4918\n","Validation Accuracy after 300 batches: 0.5088\n","Validation Accuracy after 360 batches: 0.4650\n","Epoch 24, Loss: 1.3929, Training Accuracy: 0.5861, Validation Accuracy: 0.4853\n","Validation Accuracy after 60 batches: 0.4683\n","Validation Accuracy after 120 batches: 0.4768\n","Validation Accuracy after 180 batches: 0.4716\n","Validation Accuracy after 240 batches: 0.4781\n","Validation Accuracy after 300 batches: 0.4820\n","Validation Accuracy after 360 batches: 0.4938\n","Epoch 25, Loss: 1.3455, Training Accuracy: 0.6021, Validation Accuracy: 0.4702\n","Validation Accuracy after 60 batches: 0.4840\n","Validation Accuracy after 120 batches: 0.5003\n","Validation Accuracy after 180 batches: 0.4644\n","Validation Accuracy after 240 batches: 0.4957\n","Validation Accuracy after 300 batches: 0.4957\n","Validation Accuracy after 360 batches: 0.4840\n","Epoch 26, Loss: 1.3114, Training Accuracy: 0.6098, Validation Accuracy: 0.4879\n","Validation Accuracy after 60 batches: 0.4918\n","Validation Accuracy after 120 batches: 0.4977\n","Validation Accuracy after 180 batches: 0.4912\n","Validation Accuracy after 240 batches: 0.4912\n","Validation Accuracy after 300 batches: 0.4872\n","Validation Accuracy after 360 batches: 0.4925\n","Epoch 27, Loss: 1.2479, Training Accuracy: 0.6309, Validation Accuracy: 0.4931\n","Validation Accuracy after 60 batches: 0.5036\n","Validation Accuracy after 120 batches: 0.5016\n","Validation Accuracy after 180 batches: 0.4912\n","Validation Accuracy after 240 batches: 0.5108\n","Validation Accuracy after 300 batches: 0.4872\n","Validation Accuracy after 360 batches: 0.4814\n","Epoch 28, Loss: 1.2213, Training Accuracy: 0.6337, Validation Accuracy: 0.4977\n","Validation Accuracy after 60 batches: 0.4997\n","Validation Accuracy after 120 batches: 0.4827\n","Validation Accuracy after 180 batches: 0.4957\n","Validation Accuracy after 240 batches: 0.5141\n","Validation Accuracy after 300 batches: 0.5023\n","Validation Accuracy after 360 batches: 0.4964\n","Epoch 29, Loss: 1.1861, Training Accuracy: 0.6492, Validation Accuracy: 0.4905\n","Validation Accuracy after 60 batches: 0.5056\n","Validation Accuracy after 120 batches: 0.5199\n","Validation Accuracy after 180 batches: 0.5023\n","Validation Accuracy after 240 batches: 0.5075\n","Validation Accuracy after 300 batches: 0.5075\n","Validation Accuracy after 360 batches: 0.4971\n","Epoch 30, Loss: 1.1391, Training Accuracy: 0.6672, Validation Accuracy: 0.4977\n","Validation Accuracy after 60 batches: 0.5010\n","Validation Accuracy after 120 batches: 0.5147\n","Validation Accuracy after 180 batches: 0.5121\n","Validation Accuracy after 240 batches: 0.5128\n","Validation Accuracy after 300 batches: 0.5056\n","Validation Accuracy after 360 batches: 0.4872\n","Epoch 31, Loss: 1.1027, Training Accuracy: 0.6760, Validation Accuracy: 0.5036\n","Validation Accuracy after 60 batches: 0.4977\n","Validation Accuracy after 120 batches: 0.5003\n","Validation Accuracy after 180 batches: 0.4689\n","Validation Accuracy after 240 batches: 0.5088\n","Validation Accuracy after 300 batches: 0.4853\n","Validation Accuracy after 360 batches: 0.5245\n","Epoch 32, Loss: 1.0760, Training Accuracy: 0.6836, Validation Accuracy: 0.5252\n","Validation Accuracy after 60 batches: 0.5023\n"]}],"source":["MAX_FILES = 20\n","directory_path = \"/content/drive/MyDrive/Colab Notebooks/DGMD E-14 Project/Datasets/processed-40-500\"\n","loader = ASLDatasetLoader(directory_path, max_files=MAX_FILES)\n","\n","model, all_preds, all_labels, accuracy = train(loader)"]},{"cell_type":"code","source":["torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/DGMD E-14 Project/SavedModels/best-gnn-weights.pth')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":196},"id":"I9aZrYomnTuq","executionInfo":{"status":"error","timestamp":1702331559805,"user_tz":420,"elapsed":167,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}},"outputId":"d524a91c-a147-4340-9876-446c2db63748"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-17c86c1115a9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/DGMD E-14 Project/SavedModels/best-gnn-weights.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sMrYkTs89Q7O"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix, classification_report\n","import numpy as np\n","\n","# Convert lists to numpy arrays for compatibility with sklearn\n","y_true = np.array(all_labels)\n","y_pred = np.array(all_preds)\n","\n","# Calculate the confusion matrix\n","cm = confusion_matrix(y_true, y_pred)\n","\n","# Visualize the confusion matrix\n","plt.figure(figsize=(10, 7))\n","sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n","plt.title(\"Confusion Matrix\")\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"True\")\n","plt.show()\n","\n","# Ensure the class names are in the correct order for target_names\n","ordered_class_names = [name for name, num in sorted(loader.sign_to_label.items(), key=lambda item: item[1])]\n","\n","# Per-Class Accuracy\n","class_accuracy = cm.diagonal() / cm.sum(axis=1)\n","for i, acc in enumerate(class_accuracy):\n","    class_name = ordered_class_names[i]\n","    print(f\"Accuracy for class {i} ({class_name}): {acc*100:.2f}%\")\n","\n","# Detailed classification report\n","print(\"\\nClassification Report:\\n\")\n","print(classification_report(y_true, y_pred, target_names=ordered_class_names, zero_division=1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hl5tQmtN9Syt"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","\n","def print_top_misclassified_classes(y_true, y_pred, sign_to_label, N=3, zero_division=1):\n","    \"\"\"\n","    Prints the top N classes that get misclassified the most.\n","\n","    Parameters:\n","    - y_true: Actual labels\n","    - y_pred: Predicted labels by the model\n","    - sign_to_label: Dictionary mapping class names to class numbers\n","    - N: Number of top misclassified classes to print\n","    - zero_division: Parameter for handling zero division in classification_report\n","\n","    Returns:\n","    None\n","    \"\"\"\n","\n","    # Ensure the class names are in the correct order for target_names\n","    ordered_class_names = [name for name, num in sorted(sign_to_label.items(), key=lambda item: item[1])]\n","\n","    # Generate and print classification report with class names\n","    print(\"\\nClassification Report:\\n\")\n","    print(classification_report(y_true, y_pred, target_names=ordered_class_names, zero_division=zero_division))\n","\n","    # Generate classification report as dict to find misclassified classes\n","    report = classification_report(y_true, y_pred, output_dict=True, zero_division=zero_division)\n","\n","    # Create a dictionary to store misclassification rates\n","    misclassification_rates = {}\n","\n","    # Iterate through each class in the report\n","    for class_num, metrics in report.items():\n","        if class_num.isdigit():\n","            class_name = [key for key, value in sign_to_label.items() if value == int(class_num)][0]\n","            misclassification_rates[class_name] = 1 - metrics['recall']\n","\n","    # Sort classes based on misclassification rate\n","    sorted_classes = sorted(misclassification_rates, key=misclassification_rates.get, reverse=True)\n","\n","    # Print top N misclassified classes\n","    print(f\"\\nTop {N} misclassified classes:\")\n","    for i in range(N):\n","        class_name = sorted_classes[i]\n","        print(f\"{i+1}. {class_name} - Misclassification rate: {misclassification_rates[class_name]:.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m4ZtqFxz9XlG"},"outputs":[],"source":["N = 10 if MAX_FILES > 50 else MAX_FILES\n","print_top_misclassified_classes(y_true, y_pred, loader.sign_to_label, N=N, zero_division=1)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1wIPH3B0fuuoSlPfXK5fGd9Em1LP-uQFJ","timestamp":1697014386160}],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":0}