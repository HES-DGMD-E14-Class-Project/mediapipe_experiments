{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiSS4wuxZ01m",
        "outputId": "c48e9145-17b7-437a-d5e4-99f2a5b95547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n",
            "11.8\n",
            "Collecting torch-geometric-temporal\n",
            "  Downloading torch_geometric_temporal-0.54.0.tar.gz (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (4.4.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (2.0.1+cu118)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (3.0.2)\n",
            "Collecting pandas<=1.3.5 (from torch-geometric-temporal)\n",
            "  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse (from torch-geometric-temporal)\n",
            "  Downloading torch_sparse-0.6.17.tar.gz (209 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch_scatter (from torch-geometric-temporal)\n",
            "  Downloading torch_scatter-2.1.1.tar.gz (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.6/107.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch_geometric (from torch-geometric-temporal)\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (1.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.10/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2023.3.post1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torch-geometric-temporal) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torch-geometric-temporal) (17.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (4.66.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (1.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torch-geometric-temporal) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (2023.7.22)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric->torch-geometric-temporal) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric->torch-geometric-temporal) (3.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torch-geometric-temporal) (1.3.0)\n",
            "Building wheels for collected packages: torch-geometric-temporal, torch_geometric, torch_scatter, torch_sparse\n",
            "  Building wheel for torch-geometric-temporal (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric-temporal: filename=torch_geometric_temporal-0.54.0-py3-none-any.whl size=86722 sha256=66dbca53e08bdaf22077873383a9ade9653c677d3305df62c26ac30fac4d951d\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/9b/b6/e15256e053f0cb49b1084a67a709db909d418386a231f0722c\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910454 sha256=986c22cd1d9bb8d178cb514611cb34c2a7b52cb3d420988cc822a2090955ad81\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "  Building wheel for torch_scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_scatter: filename=torch_scatter-2.1.1-cp310-cp310-linux_x86_64.whl size=491866 sha256=1d8be5e5ecb1f25d8344767f684234017944d406be9705b69385e171d9c172ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/67/58/6566a3b61c6ec0f2ca0c2c324cd035ef2955601f0fb3197d5f\n",
            "  Building wheel for torch_sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_sparse: filename=torch_sparse-0.6.17-cp310-cp310-linux_x86_64.whl size=1053726 sha256=cfb07c12426a62e04cf3a88dbd82bc23aee93a2a1eb20885e78c553f81b61803\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/25/e7/037b58fa47ba781444fd101a2f06c63a9d4e967ca6b910c53a\n",
            "Successfully built torch-geometric-temporal torch_geometric torch_scatter torch_sparse\n",
            "Installing collected packages: torch_scatter, torch_sparse, pandas, torch_geometric, torch-geometric-temporal\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 1.3.5 which is incompatible.\n",
            "plotnine 0.12.3 requires pandas>=1.5.0, but you have pandas 1.3.5 which is incompatible.\n",
            "xarray 2023.7.0 requires pandas>=1.4, but you have pandas 1.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-1.3.5 torch-geometric-temporal-0.54.0 torch_geometric-2.3.1 torch_scatter-2.1.1 torch_sparse-0.6.17\n"
          ]
        }
      ],
      "source": [
        "!python -c \"import torch; print(torch.__version__)\"\n",
        "!python -c \"import torch; print(torch.version.cuda)\"\n",
        "!pip install torch-geometric-temporal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: mount google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-ewruGCyaCn",
        "outputId": "c40d3574-aca7-43e0-f9c1-a8f23cf281c2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "HAND_TO_FACE_THRESHOLD = 0.05\n",
        "\n",
        "class ASLDatasetLoader:\n",
        "    def __init__(self, directory_path):\n",
        "        self.directory_path = directory_path\n",
        "        self.sign_to_label = self._create_sign_to_label_map()\n",
        "\n",
        "    def _create_sign_to_label_map(self):\n",
        "        signs = [os.path.splitext(filename)[0] for filename in os.listdir(self.directory_path)]\n",
        "        return {sign: i for i, sign in enumerate(signs)}\n",
        "\n",
        "    def _read_file_data(self, file_path):\n",
        "        with open(file_path, 'r') as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    def _augment_data(self, frame_data, rotation_range=10, translation_range=0.05, scaling_range=0.1):\n",
        "        \"\"\"\n",
        "        Augment the frame data with random rotation, translation, and scaling.\n",
        "\n",
        "        :param frame_data: Dictionary containing frame landmarks and deltas.\n",
        "        :param rotation_range: Maximum rotation angle in degrees.\n",
        "        :param translation_range: Maximum translation as a fraction of landmark range.\n",
        "        :param scaling_range: Maximum scaling factor.\n",
        "        :return: Augmented frame data.\n",
        "        \"\"\"\n",
        "        landmarks = np.array(frame_data[\"landmarks\"])\n",
        "        centroid = np.mean(landmarks, axis=0)\n",
        "\n",
        "        # Random rotation\n",
        "        theta = np.radians(np.random.uniform(-rotation_range, rotation_range))\n",
        "        rotation_matrix = np.array([\n",
        "            [np.cos(theta), -np.sin(theta)],\n",
        "            [np.sin(theta), np.cos(theta)]\n",
        "        ])\n",
        "        landmarks = np.dot(landmarks - centroid, rotation_matrix) + centroid\n",
        "\n",
        "        # Random translation\n",
        "        max_translation = translation_range * (landmarks.max(axis=0) - landmarks.min(axis=0))\n",
        "        translations = np.random.uniform(-max_translation, max_translation)\n",
        "        landmarks += translations\n",
        "\n",
        "        # Random scaling\n",
        "        scale = np.random.uniform(1 - scaling_range, 1 + scaling_range)\n",
        "        landmarks = centroid + scale * (landmarks - centroid)\n",
        "\n",
        "        frame_data[\"landmarks\"] = landmarks.tolist()\n",
        "        return frame_data\n",
        "\n",
        "    def _create_graph_from_frame(self, sign_name, frame_data, landmark_types):\n",
        "        left_hand_indices = [i for i, t in enumerate(landmark_types) if t == \"L\"]\n",
        "        right_hand_indices = [i for i, t in enumerate(landmark_types) if t == \"R\"]\n",
        "        face_indices = [i for i, t in enumerate(landmark_types) if t == \"F\"]\n",
        "\n",
        "        landmarks = np.array(frame_data[\"landmarks\"])\n",
        "        deltas = np.array(frame_data[\"deltas\"])\n",
        "\n",
        "        # Create weights based on landmark importance\n",
        "        weights = [2 if t == \"L\" or t == \"R\" else 1 for t in landmark_types]\n",
        "\n",
        "        # Create edges based on the number of available landmarks (or nodes)\n",
        "        edges = [[i, i + 1] for i in range(len(landmarks) - 1)]\n",
        "\n",
        "        # Add edges between the left and right hand landmarks\n",
        "        for i in left_hand_indices:\n",
        "            for j in right_hand_indices:\n",
        "                edges.append([i, j])\n",
        "\n",
        "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "\n",
        "        # Compute additional features like hand-to-face and hand-to-body distances\n",
        "        hand_to_face_contact = []\n",
        "        for idx, ltype in enumerate(landmark_types):\n",
        "            if ltype in [\"L\", \"R\"] and any(t == \"F\" for t in landmark_types):\n",
        "                min_distance = min([np.linalg.norm(landmarks[idx] - landmarks[j]) for j, t in enumerate(landmark_types) if t == \"F\"])\n",
        "                hand_to_face_contact.append(1 if min_distance < HAND_TO_FACE_THRESHOLD else 0)\n",
        "            else:\n",
        "                hand_to_face_contact.append(0)\n",
        "\n",
        "        # Reshape the 1D arrays to 2D for concatenation\n",
        "        weights_2d = np.array(weights)[:, np.newaxis]\n",
        "        hand_to_face_contact_2d = np.array(hand_to_face_contact)[:, np.newaxis]\n",
        "\n",
        "        # Concatenate landmarks, deltas, importance weights, and hand-to-face contact features\n",
        "        x = torch.tensor(np.hstack((landmarks, deltas, weights_2d, hand_to_face_contact_2d)), dtype=torch.float)\n",
        "        y = torch.tensor([self.sign_to_label[sign_name]], dtype=torch.long)\n",
        "\n",
        "        return Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "\n",
        "    def get_dataset(self, augment=False):\n",
        "        dataset = []\n",
        "\n",
        "        for filename in os.listdir(self.directory_path):\n",
        "            sign_name = os.path.splitext(filename)[0]\n",
        "            file_path = os.path.join(self.directory_path, filename)\n",
        "            sign_data = self._read_file_data(file_path)\n",
        "\n",
        "            for frame_data in sign_data[\"frames\"]:\n",
        "                landmark_types = sign_data.get(\"landmark_types\", [\"F\", \"L\", \"P\", \"R\"])  # defaulting to all types\n",
        "\n",
        "                if augment:\n",
        "                  frame_data = self._augment_data(frame_data)\n",
        "                graph_data = self._create_graph_from_frame(sign_name, frame_data, landmark_types)\n",
        "\n",
        "                dataset.append(graph_data)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def number_of_classes(self):\n",
        "        return len(self.sign_to_label)"
      ],
      "metadata": {
        "id": "LifBvD3D4t4C"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GCNConv, global_max_pool, BatchNorm  # Notice the change in the import\n",
        "\n",
        "class GraphClassifier(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, num_classes, dropout_rate=0.5):\n",
        "        super(GraphClassifier, self).__init__()\n",
        "        self.conv1 = GCNConv(num_node_features, 128)\n",
        "        self.bn1 = BatchNorm(128)\n",
        "        self.dropout1 = torch.nn.Dropout(dropout_rate)  # Dropout after first layer\n",
        "        self.conv2 = GCNConv(128, 64)\n",
        "        self.bn2 = BatchNorm(64)\n",
        "        self.dropout2 = torch.nn.Dropout(dropout_rate)  # Dropout after second layer\n",
        "        self.fc = torch.nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        # First GCN layer\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.leaky_relu(x)  # Use LeakyReLU\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # Second GCN layer\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Global pooling across nodes\n",
        "        x = global_max_pool(x, data.batch)  # Here's the change from mean pooling to max pooling\n",
        "\n",
        "        # Final classification layer\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "XpmOnloG40DP"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_max_pool, global_mean_pool\n",
        "\n",
        "class ExtendedGraphClassifier(torch.nn.Module):\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super(ExtendedGraphClassifier, self).__init__()\n",
        "        self.conv1 = GCNConv(num_features, 256)  # Increased channels\n",
        "        self.bn1 = torch.nn.BatchNorm1d(256)    # Batch normalization layer\n",
        "        self.conv2 = GCNConv(256, 512)          # Increased channels\n",
        "        self.bn2 = torch.nn.BatchNorm1d(512)    # Batch normalization layer\n",
        "        self.lin1 = torch.nn.Linear(512, 256)\n",
        "        self.lin2 = torch.nn.Linear(256, num_classes)\n",
        "        self.dropout = torch.nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
        "        x = self.dropout(x)\n",
        "        x = global_max_pool(x, batch)\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.lin2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "RAslUK79VVV6"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch_geometric.loader import DataLoader\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "EPOCHS = 100\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "\n",
        "def stratified_data_split(data_list, test_size=0.2):\n",
        "    # Extract labels from data list\n",
        "    labels = [data.y.item() for data in data_list]\n",
        "\n",
        "    # Use sklearn's train_test_split with stratify option\n",
        "    train_data, test_data = train_test_split(data_list, test_size=test_size, stratify=labels, random_state=42)\n",
        "\n",
        "    return train_data, test_data\n",
        "\n",
        "\n",
        "def validate(loader, model, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        with torch.no_grad():\n",
        "            out = model(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += int((pred == data.y).sum())\n",
        "    return correct / len(loader.dataset)\n",
        "\n",
        "def train():\n",
        "    directory_path = \"/content/drive/MyDrive/Colab Notebooks/DGMD E-14 Project/Datasets/ASL\"\n",
        "    loader = ASLDatasetLoader(directory_path)\n",
        "\n",
        "    # Create the entire dataset without augmentation and then perform stratified split\n",
        "    data_list = loader.get_dataset()\n",
        "    train_dataset, test_dataset = stratified_data_split(data_list, test_size=0.2)\n",
        "\n",
        "    # Now augment only the training dataset\n",
        "    augmented_train_dataset = loader.get_dataset(augment=True)\n",
        "\n",
        "    num_classes = loader.number_of_classes()\n",
        "\n",
        "    train_labels = [data.y.item() for data in train_dataset]\n",
        "    test_labels = [data.y.item() for data in test_dataset]\n",
        "\n",
        "    print(\"Training label distribution:\", Counter(train_labels))\n",
        "    print(\"Test label distribution:\", Counter(test_labels))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    num_features = train_dataset[0].x.size(1)\n",
        "    model = ExtendedGraphClassifier(num_features=num_features, num_classes=num_classes).to(device)\n",
        "\n",
        "    #model = ExtendedGraphClassifier(num_features=4, num_classes=num_classes).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=5e-4)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.7, patience=5, verbose=True)\n",
        "\n",
        "    max_epochs_without_improvement = 20\n",
        "    epochs_without_improvement = 0\n",
        "    best_val_accuracy = 0\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(EPOCHS):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            batch = batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(batch)\n",
        "            loss = F.nll_loss(out, batch.y)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Check for NaN loss\n",
        "            if np.isnan(loss.item()):\n",
        "                print(\"Warning: NaN loss detected!\")\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch}, Loss: {avg_loss}\")\n",
        "\n",
        "        val_accuracy = validate(test_loader, model, device)\n",
        "        scheduler.step(val_accuracy)\n",
        "\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "\n",
        "        if epochs_without_improvement >= max_epochs_without_improvement:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in test_loader:\n",
        "        batch = batch.to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(batch).max(dim=1)[1]\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(batch.y.cpu().numpy())\n",
        "            correct += pred.eq(batch.y).sum().item()\n",
        "\n",
        "    print(f\"Accuracy: {correct / len(test_dataset)}\")\n",
        "    print(\"Sample predictions:\", all_preds[:20])\n",
        "    print(\"Sample true labels:\", all_labels[:20])"
      ],
      "metadata": {
        "id": "RdBGsFveWcbF"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rr2PlwLy5M6H",
        "outputId": "9ffa269d-22bd-4582-f487-05107dfbb8bd"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training label distribution: Counter({15: 402, 7: 363, 9: 298, 3: 282, 13: 269, 16: 265, 19: 258, 2: 247, 8: 246, 4: 243, 5: 243, 10: 236, 14: 223, 18: 220, 17: 215, 12: 214, 1: 210, 0: 203, 11: 199, 6: 199})\n",
            "Test label distribution: Counter({15: 100, 7: 91, 9: 75, 3: 70, 13: 67, 16: 66, 19: 65, 2: 62, 8: 62, 4: 61, 5: 60, 10: 59, 14: 56, 18: 55, 17: 54, 12: 53, 1: 52, 0: 51, 6: 50, 11: 50})\n",
            "Epoch 0, Loss: 2.7034658178498474\n",
            "Epoch 1, Loss: 2.227446574953538\n",
            "Epoch 2, Loss: 1.9220618017112152\n",
            "Epoch 3, Loss: 1.708973925324935\n",
            "Epoch 4, Loss: 1.568981236294855\n",
            "Epoch 5, Loss: 1.4559599579889564\n",
            "Epoch 6, Loss: 1.3368036660212506\n",
            "Epoch 7, Loss: 1.3242413526094412\n",
            "Epoch 8, Loss: 1.2776312152796154\n",
            "Epoch 9, Loss: 1.2143087224869789\n",
            "Epoch 10, Loss: 1.175213598375079\n",
            "Epoch 11, Loss: 1.1082643964622594\n",
            "Epoch 12, Loss: 1.0993853513198564\n",
            "Epoch 13, Loss: 1.0614747574812249\n",
            "Epoch 14, Loss: 1.0249428920730759\n",
            "Epoch 15, Loss: 0.9787604231623155\n",
            "Epoch 16, Loss: 0.9762025109574765\n",
            "Epoch 17, Loss: 0.9188091630422617\n",
            "Epoch 18, Loss: 0.9121678282942953\n",
            "Epoch 19, Loss: 0.9176758018475545\n",
            "Epoch 20, Loss: 0.8825642391096188\n",
            "Epoch 21, Loss: 0.8687682212153568\n",
            "Epoch 22, Loss: 0.854126003535488\n",
            "Epoch 23, Loss: 0.8293341850932641\n",
            "Epoch 24, Loss: 0.8578757572023175\n",
            "Epoch 25, Loss: 0.8128642115411879\n",
            "Epoch 26, Loss: 0.7903649842814554\n",
            "Epoch 27, Loss: 0.8015656641012505\n",
            "Epoch 28, Loss: 0.7931290470723864\n",
            "Epoch 29, Loss: 0.7813419003652621\n",
            "Epoch 30, Loss: 0.7583324211307719\n",
            "Epoch 31, Loss: 0.7550383483684515\n",
            "Epoch 32, Loss: 0.7199713227492345\n",
            "Epoch 33, Loss: 0.7076912218634086\n",
            "Epoch 34, Loss: 0.7248554348568373\n",
            "Epoch 35, Loss: 0.6926729518778717\n",
            "Epoch 36, Loss: 0.6985080711826493\n",
            "Epoch 37, Loss: 0.6977466806957994\n",
            "Epoch 38, Loss: 0.7016091335423386\n",
            "Epoch 39, Loss: 0.6812183649479588\n",
            "Epoch 40, Loss: 0.6837187503712087\n",
            "Epoch 41, Loss: 0.6905987796526921\n",
            "Epoch 42, Loss: 0.6628050972012025\n",
            "Epoch 43, Loss: 0.6581710022461565\n",
            "Epoch 44, Loss: 0.6413164368913143\n",
            "Epoch 45, Loss: 0.6682126139160953\n",
            "Epoch 46, Loss: 0.6389230065330674\n",
            "Epoch 47, Loss: 0.6566889131370979\n",
            "Epoch 48, Loss: 0.6349172079110448\n",
            "Epoch 49, Loss: 0.6428572533628608\n",
            "Epoch 50, Loss: 0.6180578468343879\n",
            "Epoch 51, Loss: 0.6154778413003004\n",
            "Epoch 52, Loss: 0.6136209142358997\n",
            "Epoch 53, Loss: 0.6162919375715377\n",
            "Epoch 54, Loss: 0.6205078942489021\n",
            "Epoch 55, Loss: 0.6021282380517525\n",
            "Epoch 56, Loss: 0.6021124348987507\n",
            "Epoch 00057: reducing learning rate of group 0 to 7.0000e-04.\n",
            "Epoch 57, Loss: 0.5698698112104512\n",
            "Epoch 58, Loss: 0.5585728182068354\n",
            "Epoch 59, Loss: 0.5601583411799201\n",
            "Epoch 60, Loss: 0.5495744245716289\n",
            "Epoch 61, Loss: 0.5388270733477194\n",
            "Epoch 62, Loss: 0.5373435164742832\n",
            "Epoch 63, Loss: 0.5234026039325739\n",
            "Epoch 64, Loss: 0.528045018262501\n",
            "Epoch 65, Loss: 0.5237505781122401\n",
            "Epoch 66, Loss: 0.5242148488759995\n",
            "Epoch 67, Loss: 0.5204364972401269\n",
            "Epoch 68, Loss: 0.5111417408230938\n",
            "Epoch 69, Loss: 0.5031199537311928\n",
            "Epoch 70, Loss: 0.5032513490583324\n",
            "Epoch 71, Loss: 0.5091113016197953\n",
            "Epoch 72, Loss: 0.5067480433779427\n",
            "Epoch 73, Loss: 0.49514555638726754\n",
            "Epoch 74, Loss: 0.5080462988418869\n",
            "Epoch 75, Loss: 0.49224419090189514\n",
            "Epoch 76, Loss: 0.4911795341515843\n",
            "Epoch 77, Loss: 0.4910639216062389\n",
            "Epoch 78, Loss: 0.4860084602915788\n",
            "Epoch 79, Loss: 0.4840043940687481\n",
            "Epoch 80, Loss: 0.47962732216979886\n",
            "Epoch 81, Loss: 0.47683348874502546\n",
            "Epoch 82, Loss: 0.4666248624083362\n",
            "Epoch 83, Loss: 0.4835249727851228\n",
            "Epoch 84, Loss: 0.4799153176666815\n",
            "Epoch 85, Loss: 0.4716766038084332\n",
            "Epoch 86, Loss: 0.46442086211865463\n",
            "Epoch 87, Loss: 0.4713288117435914\n",
            "Epoch 88, Loss: 0.4601966102100626\n",
            "Epoch 89, Loss: 0.47303120820205424\n",
            "Epoch 90, Loss: 0.4689169366714321\n",
            "Epoch 91, Loss: 0.4565833279792267\n",
            "Epoch 92, Loss: 0.47233082797331144\n",
            "Epoch 93, Loss: 0.46193567616275594\n",
            "Epoch 94, Loss: 0.45105193442181696\n",
            "Epoch 95, Loss: 0.4458263328746904\n",
            "Epoch 96, Loss: 0.44665629426135295\n",
            "Epoch 97, Loss: 0.4546740605861326\n",
            "Epoch 98, Loss: 0.4419841851237454\n",
            "Epoch 99, Loss: 0.4526063738555848\n",
            "Epoch 00100: reducing learning rate of group 0 to 4.9000e-04.\n",
            "Accuracy: 0.8482922954725973\n",
            "Sample predictions: [9, 16, 9, 19, 16, 12, 0, 13, 15, 2, 6, 3, 6, 6, 12, 3, 8, 13, 2, 15]\n",
            "Sample true labels: [9, 16, 9, 19, 0, 12, 16, 13, 15, 2, 6, 2, 3, 6, 12, 3, 8, 13, 2, 15]\n"
          ]
        }
      ]
    }
  ]
}