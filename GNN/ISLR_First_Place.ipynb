{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyPB1XpbdXt7GvjRq7EDt9oM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T0IvPbKkQ_R4","executionInfo":{"status":"ok","timestamp":1702340712924,"user_tz":420,"elapsed":4134,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}},"outputId":"654a173e-1f8d-4b9d-d483-79d7999c6c99"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","source":["import torch\n","\n","TORCH = torch.__version__.split('+')[0]\n","CUDA = 'cu' + torch.version.cuda.replace('.', '')\n","\n","# Construct the installation command\n","install_command = f\"pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\"\n","\n","# Execute the command\n","!{install_command}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LG23i2jsVTxv","executionInfo":{"status":"ok","timestamp":1702340722684,"user_tz":420,"elapsed":9761,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}},"outputId":"e5b65c55-2fae-4adf-a750-5eb80f5e3f95"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://data.pyg.org/whl/torch-2.1.0+cu118.html\n","Requirement already satisfied: pyg_lib in /usr/local/lib/python3.10/dist-packages (0.3.1+pt21cu118)\n","Requirement already satisfied: torch_scatter in /usr/local/lib/python3.10/dist-packages (2.1.2+pt21cu118)\n","Requirement already satisfied: torch_sparse in /usr/local/lib/python3.10/dist-packages (0.6.18+pt21cu118)\n","Requirement already satisfied: torch_cluster in /usr/local/lib/python3.10/dist-packages (1.6.3+pt21cu118)\n","Requirement already satisfied: torch_spline_conv in /usr/local/lib/python3.10/dist-packages (1.2.2+pt21cu118)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_sparse) (1.11.4)\n","Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch_sparse) (1.23.5)\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","import numpy as np\n","\n","# Set a random seed for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""],"metadata":{"id":"BFN_nVMvgrFm","executionInfo":{"status":"ok","timestamp":1702340722684,"user_tz":420,"elapsed":5,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","DATA_DIR = \"/content/drive/MyDrive/Colab Notebooks/DGMD E-14 Project/Datasets/asl-signs\"\n","\n","# Train DataFrame\n","train_df = pd.read_csv(f\"{DATA_DIR}/train.csv\")\n","\n","# Define the list of signs to include\n","selected_signs = [\n","    'TV', 'after', 'airplane', 'all', 'alligator', 'bird', 'callonphone', 'cry',\n","    'dad', 'dance', 'dog', 'drink', 'duck', 'elephant', 'eye', 'feet', 'finger',\n","    'flower', 'food', 'face'\n","]\n","\n","# Filter the DataFrame to include only the selected signs\n","train_df = train_df[train_df['sign'].isin(selected_signs)]\n","\n","\n","display(train_df.head())\n","display(train_df.info())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"7vd3WNmRhQYB","executionInfo":{"status":"ok","timestamp":1702340723132,"user_tz":420,"elapsed":453,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}},"outputId":"a0eab1a4-97ce-4f00-b517-c3165e1017d3"},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":["                                             path  participant_id  \\\n","3   train_landmark_files/25571/1000210073.parquet           25571   \n","5   train_landmark_files/26734/1000241583.parquet           26734   \n","8    train_landmark_files/37055/100035691.parquet           37055   \n","39  train_landmark_files/53618/1001896056.parquet           53618   \n","41   train_landmark_files/53618/100190623.parquet           53618   \n","\n","    sequence_id    sign  \n","3    1000210073    bird  \n","5    1000241583    duck  \n","8     100035691  flower  \n","39   1001896056  finger  \n","41    100190623     cry  "],"text/html":["\n","  <div id=\"df-812e8d26-d22e-43b3-9e61-9c25a330aafc\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>path</th>\n","      <th>participant_id</th>\n","      <th>sequence_id</th>\n","      <th>sign</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3</th>\n","      <td>train_landmark_files/25571/1000210073.parquet</td>\n","      <td>25571</td>\n","      <td>1000210073</td>\n","      <td>bird</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>train_landmark_files/26734/1000241583.parquet</td>\n","      <td>26734</td>\n","      <td>1000241583</td>\n","      <td>duck</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>train_landmark_files/37055/100035691.parquet</td>\n","      <td>37055</td>\n","      <td>100035691</td>\n","      <td>flower</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>train_landmark_files/53618/1001896056.parquet</td>\n","      <td>53618</td>\n","      <td>1001896056</td>\n","      <td>finger</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>train_landmark_files/53618/100190623.parquet</td>\n","      <td>53618</td>\n","      <td>100190623</td>\n","      <td>cry</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-812e8d26-d22e-43b3-9e61-9c25a330aafc')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-812e8d26-d22e-43b3-9e61-9c25a330aafc button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-812e8d26-d22e-43b3-9e61-9c25a330aafc');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-57da7bc6-ce10-4507-a38d-c1d6e0c3ac58\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-57da7bc6-ce10-4507-a38d-c1d6e0c3ac58')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-57da7bc6-ce10-4507-a38d-c1d6e0c3ac58 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 7641 entries, 3 to 94474\n","Data columns (total 4 columns):\n"," #   Column          Non-Null Count  Dtype \n","---  ------          --------------  ----- \n"," 0   path            7641 non-null   object\n"," 1   participant_id  7641 non-null   int64 \n"," 2   sequence_id     7641 non-null   int64 \n"," 3   sign            7641 non-null   object\n","dtypes: int64(2), object(2)\n","memory usage: 298.5+ KB\n"]},{"output_type":"display_data","data":{"text/plain":["None"]},"metadata":{}}]},{"cell_type":"code","source":["# Getting unique 'sign' values and their counts\n","CLASSES = train_df['sign'].unique()\n","NUMBER_OF_CLASSES = len(CLASSES)\n","# Count the occurrences of each sign in the dataset\n","sign_counts = train_df['sign'].value_counts()\n","\n","\n","\n","# Calculate min, max, and average counts\n","min_sign_count = sign_counts.min()\n","max_sign_count = sign_counts.max()\n","average_sign_count = sign_counts.mean()\n","\n","print(\"Count of unique signs:\", NUMBER_OF_CLASSES)\n","print(\"Minimum number of examples per sign:\", min_sign_count)\n","print(\"Maximum number of examples per sign:\", max_sign_count)\n","print(\"Average number of examples per sign:\", average_sign_count)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R19rx_56f_fn","executionInfo":{"status":"ok","timestamp":1702340723132,"user_tz":420,"elapsed":6,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}},"outputId":"f0c764cd-b7c0-4962-8331-3b06def81369"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Count of unique signs: 20\n","Minimum number of examples per sign: 312\n","Maximum number of examples per sign: 405\n","Average number of examples per sign: 382.05\n"]}]},{"cell_type":"code","source":["import random\n","import math\n","import pandas as pd\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import pyarrow.parquet as pq\n","from sklearn.preprocessing import LabelEncoder\n","\n","class ASLSignsDataset(Dataset):\n","    def __init__(self, dataframe, root_dir, rows_per_frame=543):\n","        self.dataframe = dataframe\n","        self.root_dir = root_dir\n","        self.label_encoder = LabelEncoder()\n","        self.dataframe['encoded_labels'] = self.label_encoder.fit_transform(dataframe['sign'])\n","        self.rows_per_frame = rows_per_frame\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        file_path = os.path.join(self.root_dir, self.dataframe.iloc[idx]['path'])\n","        landmarks_df = pq.read_table(file_path).to_pandas()\n","\n","        # Load relevant data subset and reshape\n","        landmarks_tensor = self._load_relevant_data_subset(landmarks_df, MAX_LEN)\n","\n","        # Print the shape of the data for debugging\n","        # print(\"Original landmarks shape:\", landmarks_tensor.shape)\n","\n","        # Preprocess the data\n","        landmarks_tensor = self._preprocess(landmarks_tensor)\n","        # print(f\"Shape after preprocessing: {landmarks_tensor.shape}\")\n","\n","        # Debugging: Print the shape after preprocessing\n","        # print(\"Post-preprocessing shape:\", landmarks_tensor.shape)\n","\n","        # Apply data augmentation (if needed)\n","        # print(\"Pre-augmentation shape:\", landmarks_tensor.shape)\n","        # augmented_landmarks = self._augment(landmarks_tensor)\n","        # print(\"Post-augmentation shape:\", augmented_landmarks.shape)\n","\n","        # # Debugging: Print the shape after augmentation\n","        # print(\"Post-augmentation shape:\", augmented_landmarks.shape)\n","\n","        # Encode labels for the filtered signs\n","        label = self.label_encoder.transform([self.dataframe.iloc[idx]['sign']])[0]\n","\n","        return landmarks_tensor, label\n","\n","    def _load_relevant_data_subset(self, df, max_len):\n","        n_frames = int(len(df) / self.rows_per_frame)\n","        landmarks = df[['x', 'y', 'z']].values.reshape(n_frames, self.rows_per_frame, -1)\n","\n","        # Convert landmarks to a PyTorch tensor\n","        landmarks = torch.tensor(landmarks, dtype=torch.float32)\n","\n","        # Padding logic\n","        if n_frames < max_len:\n","            pad_size = max_len - n_frames\n","            padding = torch.zeros((pad_size, self.rows_per_frame, landmarks.shape[2]))\n","            landmarks = torch.cat([landmarks, padding], dim=0)\n","\n","        return landmarks\n","\n","    def _preprocess(self, landmarks):\n","        # Assuming landmarks shape: [sequence_length, ROWS_PER_FRAME, channels]\n","        sequence_length, num_landmarks, channels = landmarks.shape\n","\n","        # Normalization around the nose landmark\n","        nose_mean = landmarks[:, 17, :2].mean(dim=1, keepdim=True)\n","        nose_mean = nose_mean.unsqueeze(1).expand(-1, num_landmarks, -1)\n","        landmarks.sub_(nose_mean)  # In-place subtraction\n","\n","        # Select specific landmarks based on POINT_LANDMARKS\n","        selected_landmarks = landmarks[:, POINT_LANDMARKS, :]\n","\n","        # Calculate first and second derivatives\n","        dx = torch.diff(selected_landmarks, dim=1, prepend=selected_landmarks[:, :1, :])\n","        dx2 = torch.diff(dx, dim=1, prepend=dx[:, :1, :])\n","\n","        # Concatenation of original, dx, and dx2\n","        processed_landmarks = torch.cat([selected_landmarks, dx, dx2], dim=-1)\n","\n","        # Handle NaN values\n","        processed_landmarks = torch.nan_to_num(processed_landmarks)\n","\n","        # Flattening the last two dimensions\n","        processed_landmarks = processed_landmarks.view(sequence_length, -1)\n","        return processed_landmarks\n","\n","    def _augment(self, landmarks, max_len=None):\n","        print(f\"Augment: Before - {landmarks.shape}\")\n","        # Apply augmentations with random chance\n","\n","        # Debugging: Print the shape before resampling\n","        # print(\"Pre-resample shape:\", landmarks.shape)\n","\n","        # Apply resampling with a probability of 0.8\n","        if random.random() < 0.8:\n","            landmarks = self._resample(landmarks)\n","\n","        # Debugging: Print the shape after resampling\n","        # print(\"Post-resample shape:\", landmarks.shape)\n","\n","        # Apply flip_lr with a probability of 0.5\n","        if random.random() < 0.5:\n","            landmarks = self._flip_lr(landmarks)\n","\n","        # Apply spatial_random_affine with a probability of 0.75\n","        if random.random() < 0.75:\n","            landmarks = self._spatial_random_affine(landmarks)\n","\n","        # Apply temporal_crop if max_len is provided\n","        if max_len is not None:\n","            landmarks = self._temporal_crop(landmarks, max_len=max_len)\n","\n","        # Apply temporal_mask with a probability of 0.5\n","        if random.random() < 0.5:\n","            landmarks = self._temporal_mask(landmarks)\n","\n","        # Apply spatial_mask with a probability of 0.5\n","        if random.random() < 0.5:\n","            landmarks = self._spatial_mask(landmarks)\n","\n","        print(f\"Augment: After - {landmarks.shape}\")\n","\n","        return landmarks\n","\n","    def _resample(self, landmarks):\n","        # Determine the current and target sequence lengths\n","        current_length = landmarks.size(0)\n","        rate = random.uniform(0.8, 1.2)\n","        target_length = int(current_length * rate)\n","\n","        # Reshape the landmarks for interpolation\n","        # Change shape to [batch, channels, sequence_length]\n","        # Here channels will be ROWS_PER_FRAME * 3\n","        landmarks = landmarks.reshape(1, -1, current_length)\n","\n","        # Apply interpolation\n","        landmarks = F.interpolate(landmarks, size=(target_length,), mode='nearest')\n","\n","        # Reshape back to the original format [sequence_length, ROWS_PER_FRAME, 3]\n","        landmarks = landmarks.reshape(target_length, -1, 3)\n","\n","        return landmarks\n","\n","    def _flip_lr(self, landmarks):\n","        # Reshape the tensor from [sequence_length, 1629] to [sequence_length, 543, 3]\n","        landmarks = landmarks.view(landmarks.size(0), -1, 3)\n","\n","        # Flip the x-coordinate\n","        landmarks[:, :, 0] = 1 - landmarks[:, :, 0]\n","\n","        # Swap specific landmarks\n","        landmarks = self._swap_landmarks(landmarks, LHAND, RHAND)\n","        landmarks = self._swap_landmarks(landmarks, LLIP, RLIP)\n","        landmarks = self._swap_landmarks(landmarks, LPOSE, RPOSE)\n","        landmarks = self._swap_landmarks(landmarks, LEYE, REYE)\n","        landmarks = self._swap_landmarks(landmarks, LNOSE, RNOSE)\n","\n","        # Reshape the tensor back to [sequence_length, 1629]\n","        landmarks = landmarks.view(landmarks.size(0), -1)\n","\n","        return landmarks\n","\n","    def _swap_landmarks(self, landmarks, set1, set2):\n","        # Swap two sets of landmarks\n","        # Note: Adjust the indices in set1 and set2 based on the new landmarks structure\n","        # after preprocessing. Make sure they are within the bounds of the reshaped tensor.\n","        for l_index, r_index in zip(set1, set2):\n","            if l_index < landmarks.shape[1] and r_index < landmarks.shape[1]:\n","                temp = landmarks[:, l_index].clone()\n","                landmarks[:, l_index] = landmarks[:, r_index]\n","                landmarks[:, r_index] = temp\n","            else:\n","                # Handle out-of-bounds indices or adjust the logic to match the new structure\n","                print(f\"Index out of bounds: l_index={l_index}, r_index={r_index}\")\n","        return landmarks\n","\n","\n","    def _spatial_random_affine(self, landmarks):\n","        # Reshape the tensor from [sequence_length, 1629] to [sequence_length, 543, 3]\n","        landmarks = landmarks.view(landmarks.size(0), -1, 3)\n","\n","        # Define the center for rotation\n","        center = torch.tensor([0.5, 0.5])\n","\n","        for i in range(landmarks.shape[0]):  # Iterate through each frame\n","            frame = landmarks[i, :, :2]  # Select x and y coordinates\n","\n","            # Apply scaling\n","            scale = random.uniform(0.8, 1.2)\n","            frame = frame * scale\n","\n","            # Apply shearing\n","            shear_x = shear_y = random.uniform(-0.15, 0.15)\n","            if random.random() < 0.5:\n","                shear_x = 0.\n","            else:\n","                shear_y = 0.\n","            shear_mat = torch.tensor([[1., shear_x], [shear_y, 1.]])\n","            frame = torch.matmul(frame, shear_mat)\n","            center += torch.tensor([shear_y, shear_x])\n","\n","            # Apply rotation\n","            degree = random.uniform(-30, 30)\n","            radian = degree * math.pi / 180\n","            cos_val, sin_val = math.cos(radian), math.sin(radian)\n","            rotate_mat = torch.tensor([[cos_val, -sin_val], [sin_val, cos_val]])\n","            frame = frame - center\n","            frame = torch.matmul(frame, rotate_mat)\n","            frame = frame + center\n","\n","            # Apply translation\n","            shift_val = random.uniform(-0.1, 0.1)\n","            frame += shift_val\n","\n","            landmarks[i, :, :2] = frame  # Update the transformed frame\n","\n","        # Reshape the tensor back to [sequence_length, 1629]\n","        landmarks = landmarks.view(landmarks.size(0), -1)\n","\n","        return landmarks\n","\n","\n","    def _temporal_crop(self, landmarks, max_len=384):\n","        # Temporal cropping of the data\n","        # landmarks shape: [sequence_length, ROWS_PER_FRAME, 3]\n","\n","        sequence_length = landmarks.shape[0]\n","\n","        # If the sequence is shorter than the max_len, return it as is\n","        if sequence_length <= max_len:\n","            return landmarks\n","\n","        # Randomly choose a start point for the crop\n","        start = random.randint(0, sequence_length - max_len)\n","        end = start + max_len\n","\n","        # Crop the sequence\n","        cropped_landmarks = landmarks[start:end, :, :]\n","\n","        return cropped_landmarks\n","\n","    def _temporal_mask(self, landmarks, mask_size_range=(0.2, 0.4), mask_value=float('nan')):\n","        # Temporal masking of the data\n","        # landmarks shape after preprocessing and augmentation: [sequence_length, channels]\n","        # We need to ensure the sequence_length dimension is correctly handled.\n","\n","        # Handle cases where landmarks might have an additional dimension (like after augmentation)\n","        if landmarks.ndim > 2:\n","            sequence_length = landmarks.size(0)\n","        else:\n","            sequence_length, _ = landmarks.shape\n","\n","        # Randomly determine the size of the mask\n","        mask_size_fraction = random.uniform(*mask_size_range)\n","        mask_size = int(sequence_length * mask_size_fraction)\n","\n","        # Randomly choose a start point for the mask\n","        if sequence_length - mask_size > 0:\n","            start = random.randint(0, sequence_length - mask_size)\n","        else:\n","            start = 0\n","\n","        # Apply the mask\n","        if landmarks.ndim > 2:\n","            landmarks[start:start + mask_size, :, :] = mask_value\n","        else:\n","            landmarks[start:start + mask_size, :] = mask_value\n","\n","        return landmarks\n","\n","    def _spatial_mask(self, landmarks, size=(0.2, 0.4), mask_value=float('nan')):\n","        # landmarks shape after preprocessing and augmentation: [sequence_length, channels]\n","        if landmarks.ndim > 2:\n","            # Additional reshaping logic if the tensor has more than 2 dimensions\n","            landmarks = landmarks.view(landmarks.size(0), -1)\n","\n","        sequence_length, channels = landmarks.shape\n","        num_landmarks = channels // 3  # Assuming each landmark has x, y, z coordinates\n","\n","        # Randomly determine the size and offset of the mask\n","        mask_size = random.uniform(*size)\n","        mask_offset_x = random.uniform(0, 1 - mask_size)\n","        mask_offset_y = random.uniform(0, 1 - mask_size)\n","\n","        # Apply the mask\n","        for i in range(sequence_length):  # Iterate through each frame\n","            for j in range(num_landmarks):  # Iterate through each landmark\n","                x_index = j * 3  # Index for x-coordinate\n","                y_index = x_index + 1  # Index for y-coordinate\n","\n","                # Check if the landmark is within the mask bounds\n","                x_in_mask = mask_offset_x < landmarks[i, x_index] < mask_offset_x + mask_size\n","                y_in_mask = mask_offset_y < landmarks[i, y_index] < mask_offset_y + mask_size\n","\n","                if x_in_mask and y_in_mask:\n","                    # Mask this landmark\n","                    landmarks[i, x_index:x_index + 3] = mask_value  # Mask x, y, z coordinates\n","\n","        return landmarks\n"],"metadata":{"id":"8u0IEapm1SgC","executionInfo":{"status":"ok","timestamp":1702340724630,"user_tz":420,"elapsed":794,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Instantiate the dataset\n","dataset = ASLSignsDataset(train_df, DATA_DIR)\n","\n","# Check an example\n","example_landmarks, example_label = dataset[0]\n","print(\"Tensor size:\", example_landmarks.size(), \"Label:\", example_label)\n","\n","# Statistics about the dataset\n","total_items = len(dataset)\n","unique_labels = len(set(dataset.dataframe['encoded_labels']))\n","\n","print(\"Total items in the dataset:\", total_items)\n","print(\"Number of unique labels:\", unique_labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aPDPs0Ed1dyb","executionInfo":{"status":"ok","timestamp":1702340724630,"user_tz":420,"elapsed":2,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}},"outputId":"4b9ada50-8f0f-48b9-c914-ec86e6647c56"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor size: torch.Size([384, 3186]) Label: 5\n","Total items in the dataset: 7641\n","Number of unique labels: 20\n"]}]},{"cell_type":"code","source":["def collate_fn(batch):\n","    max_len = max([item[0].shape[0] for item in batch])\n","    # print(f\"Max sequence length for padding: {max_len}\")\n","\n","    # Pad each sequence to the max_len\n","    padded_batch = []\n","    for x, y in batch:\n","        # print(f\"Shape before padding: {x.shape}\")\n","        padded_len = max_len - x.shape[0]\n","        padded_x = F.pad(x, (0, 0, padded_len, 0))  # Pad at the end of the sequence\n","        # print(f\"Shape after padding: {padded_x.shape}\")\n","        padded_batch.append((padded_x, y))\n","\n","    # Stack all padded tensors\n","    batch_x = torch.stack([item[0] for item in padded_batch])\n","    batch_y = torch.tensor([item[1] for item in padded_batch])\n","    # print(f\"Batch shape after padding and stacking: {batch_x.shape}\")\n","\n","    return batch_x, batch_y"],"metadata":{"id":"r9RufhnZVG4v","executionInfo":{"status":"ok","timestamp":1702340724630,"user_tz":420,"elapsed":2,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from sklearn.model_selection import KFold\n","\n","class CausalDWConv1D(nn.Module):\n","    def __init__(self, channels, kernel_size, dilation_rate):\n","        super(CausalDWConv1D, self).__init__()\n","        self.padding = (kernel_size - 1) * dilation_rate\n","        self.dw_conv = nn.Conv1d(channels, channels, kernel_size, stride=1, padding=self.padding, groups=channels, dilation=dilation_rate, bias=False)\n","\n","    def forward(self, x):\n","        x = self.dw_conv(x)\n","        # Remove the padding added for causality\n","        return x[:, :, :-self.padding] if self.padding != 0 else x\n"],"metadata":{"id":"auWfmiYB_3fk","executionInfo":{"status":"ok","timestamp":1702340724630,"user_tz":420,"elapsed":2,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["class ECA(nn.Module):\n","    def __init__(self, kernel_size=5):\n","        super(ECA, self).__init__()\n","        self.kernel_size = kernel_size\n","        self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=(kernel_size - 1) // 2, bias=False)\n","\n","    def forward(self, x):\n","        # Global Average Pooling along the spatial dimensions\n","        y = x.mean(dim=-1, keepdim=True)\n","\n","        # Adjust the dimensions for 1D convolution\n","        y = y.permute(0, 2, 1)  # Swap the channel and the spatial dimension\n","\n","        # Convolution\n","        y = self.conv(y)\n","\n","        # Activation\n","        y = torch.sigmoid(y)\n","\n","        # Reverse the earlier permutation to match the input tensor's shape\n","        y = y.permute(0, 2, 1)\n","\n","        return x * y"],"metadata":{"id":"Ml_trtUTJ3zx","executionInfo":{"status":"ok","timestamp":1702340724630,"user_tz":420,"elapsed":2,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["class Swish(nn.Module):\n","    def forward(self, x):\n","        return x * torch.sigmoid(x)"],"metadata":{"id":"rnZiJChz2Rnp","executionInfo":{"status":"ok","timestamp":1702340725176,"user_tz":420,"elapsed":547,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["class Conv1DBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, dilation_rate=1, drop_rate=0.0, expand_ratio=2, se_ratio=0.25, activation='relu'):\n","        super(Conv1DBlock, self).__init__()\n","\n","        # Expansion phase (if expand_ratio is set to a value greater than 1)\n","        self.use_expansion = expand_ratio > 1\n","        if self.use_expansion:\n","            self.expansion_conv = nn.Conv1d(in_channels, in_channels * expand_ratio, kernel_size=1)\n","            self.expansion_bn = nn.BatchNorm1d(in_channels * expand_ratio)\n","            if activation == 'swish':\n","                self.expansion_activation = Swish()\n","            else:\n","                self.expansion_activation = getattr(F, activation)\n","\n","        # Depthwise separable convolution\n","        self.dwconv = CausalDWConv1D(in_channels * expand_ratio if self.use_expansion else in_channels, kernel_size, dilation_rate)\n","        self.norm = nn.BatchNorm1d(in_channels * expand_ratio if self.use_expansion else in_channels)\n","\n","        # Squeeze and Excitation\n","        self.use_se = 0 < se_ratio <= 1\n","        if self.use_se:\n","            num_squeezed_channels = max(1, int(in_channels * se_ratio))\n","            self.se = nn.Sequential(\n","                nn.AdaptiveAvgPool1d(1),\n","                nn.Conv1d(in_channels * expand_ratio if self.use_expansion else in_channels, num_squeezed_channels, kernel_size=1),\n","                nn.ReLU(),\n","                nn.Conv1d(num_squeezed_channels, in_channels * expand_ratio if self.use_expansion else in_channels, kernel_size=1),\n","                nn.Sigmoid()\n","            )\n","\n","        # Output phase\n","        self.project_conv = nn.Conv1d(in_channels * expand_ratio if self.use_expansion else in_channels, out_channels, kernel_size=1)\n","        self.project_bn = nn.BatchNorm1d(out_channels)\n","\n","        self.drop_rate = drop_rate\n","        if activation == 'swish':\n","            self.activation = Swish()\n","        else:\n","            self.activation = getattr(F, activation)\n","\n","    def forward(self, x):\n","        identity = x\n","\n","        # Expansion\n","        if self.use_expansion:\n","            x = self.expansion_conv(x)\n","            x = self.expansion_bn(x)\n","            x = self.expansion_activation(x) if self.expansion_activation else x\n","\n","        # Depthwise convolution\n","        x = self.dwconv(x)\n","        x = self.norm(x)\n","\n","        # Squeeze and Excitation\n","        if self.use_se:\n","            x = x * self.se(x)\n","\n","        # Projection\n","        x = self.project_conv(x)\n","        x = self.project_bn(x)\n","\n","        # Skip connection and drop\n","        if self.drop_rate > 0:\n","            x = F.dropout(x, p=self.drop_rate, training=self.training)\n","        x += identity  # skip connection\n","        x = self.activation(x) if self.activation else x\n","        return x\n"],"metadata":{"id":"qVHSu5LOJoNH","executionInfo":{"status":"ok","timestamp":1702340725176,"user_tz":420,"elapsed":3,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["class MultiHeadSelfAttention(nn.Module):\n","    def __init__(self, dim, num_heads):\n","        super(MultiHeadSelfAttention, self).__init__()\n","        self.dim = dim\n","        self.num_heads = num_heads\n","        assert dim % num_heads == 0, \"dimension must be divisible by the number of heads\"\n","\n","        self.depth = dim // num_heads\n","\n","        self.wq = nn.Linear(dim, dim)\n","        self.wk = nn.Linear(dim, dim)\n","        self.wv = nn.Linear(dim, dim)\n","\n","        self.dense = nn.Linear(dim, dim)\n","        self.scale = torch.sqrt(torch.tensor(self.depth, dtype=torch.float32))\n","\n","    def split_heads(self, x, batch_size):\n","        # Split the last dimension into (num_heads, depth)\n","        x = x.view(batch_size, -1, self.num_heads, self.depth)\n","        # Transpose for the shape (batch_size, num_heads, seq_len, depth)\n","        return x.permute(0, 2, 1, 3)\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","\n","        q = self.split_heads(self.wq(x), batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n","        k = self.split_heads(self.wk(x), batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n","        v = self.split_heads(self.wv(x), batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n","\n","        # Scaled dot-product attention\n","        matmul_qk = torch.matmul(q, k.transpose(-2, -1))  # (batch_size, num_heads, seq_len_q, seq_len_k)\n","        scaled_attention_logits = matmul_qk / self.scale\n","\n","        attention_weights = F.softmax(scaled_attention_logits, dim=-1)  # (batch_size, num_heads, seq_len_q, seq_len_k)\n","        output = torch.matmul(attention_weights, v)  # (batch_size, num_heads, seq_len_q, depth)\n","\n","        output = output.permute(0, 2, 1, 3).contiguous()  # (batch_size, seq_len_q, num_heads, depth)\n","        output = output.view(batch_size, -1, self.dim)  # (batch_size, seq_len_q, dim)\n","\n","        return self.dense(output)\n"],"metadata":{"id":"Fp54ei23n2ry","executionInfo":{"status":"ok","timestamp":1702340725176,"user_tz":420,"elapsed":3,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["class TransformerBlock(nn.Module):\n","    def __init__(self, dim, num_heads, expand_ratio, drop_rate, activation):\n","        super().__init__()\n","        self.attention = MultiHeadSelfAttention(dim, num_heads)\n","        self.norm1 = nn.LayerNorm(dim)\n","        self.norm2 = nn.LayerNorm(dim)\n","\n","        # Use activation function as a layer, not as a function\n","        if activation == 'swish':\n","            activation_layer = Swish()\n","        else:\n","            activation_layer = getattr(nn, activation)()\n","\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(dim, dim * expand_ratio),\n","            activation_layer,\n","            nn.Linear(dim * expand_ratio, dim),\n","            nn.Dropout(drop_rate)\n","        )\n","\n","    def forward(self, x):\n","        attn_output = self.attention(x)\n","        x = self.norm1(x + attn_output)\n","\n","        ff_output = self.feed_forward(x)\n","        x = self.norm2(x + ff_output)\n","\n","        return x\n"],"metadata":{"id":"_5WYa1VPpQTl","executionInfo":{"status":"ok","timestamp":1702340725176,"user_tz":420,"elapsed":3,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["class ASLClassifierModel(nn.Module):\n","    def __init__(self, num_classes=NUMBER_OF_CLASSES, channels=96, dim=48):  # Reduced channels and dim\n","        super(ASLClassifierModel, self).__init__()\n","\n","        self.dim = dim\n","        self.seq_len = 384\n","\n","        self.stem_conv = nn.Conv1d(channels, dim, kernel_size=1)\n","        self.stem_bn = nn.BatchNorm1d(dim)\n","\n","        # Reduced number of blocks\n","        self.blocks = nn.Sequential(\n","            Conv1DBlock(dim, dim, 17, dilation_rate=1, drop_rate=0.1, expand_ratio=1, se_ratio=0.25, activation='swish'),\n","            TransformerBlock(dim, num_heads=2, expand_ratio=2, drop_rate=0.1, activation='swish'),\n","            Conv1DBlock(dim, dim, 17, dilation_rate=1, drop_rate=0.1, expand_ratio=1, se_ratio=0.25, activation='swish'),\n","        )\n","        # # Simplified model structure\n","        # self.blocks = nn.Sequential(\n","        #     Conv1DBlock(dim, dim, 17, dilation_rate=1, drop_rate=0.05, expand_ratio=1, se_ratio=0.25, activation='swish'),\n","        #     TransformerBlock(dim, num_heads=2, expand_ratio=2, drop_rate=0.05, activation='swish')\n","        # )\n","\n","        self.pre_attention_linear = nn.Linear(dim * 3186, self.seq_len * dim)  # Adjusted feature size\n","        self.top_conv = nn.Linear(dim * 2, dim * 2, bias=False)\n","        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n","        self.late_dropout = nn.Dropout(0.5)  # Reduced dropout\n","        self.classifier = nn.Linear(dim * 2, num_classes)\n","\n","    def forward_blocks(self, blocks, x):\n","        for block in blocks:\n","            x = checkpoint.checkpoint(block, x)\n","        return x\n","\n","    def forward(self, x):\n","        # Initial convolution and batch normalization\n","        x = self.stem_conv(x)\n","        x = self.stem_bn(x)\n","        x.relu_()  # In-place ReLU activation\n","\n","        # Apply checkpointing to sequential blocks\n","        x = self.forward_blocks(self.blocks, x)\n","\n","        # Flatten the tensor along sequence length and feature size, keeping batch size separate\n","        x = x.view(x.size(0), -1)  # Flatten to [batch_size, seq_len * num_features]\n","\n","        # Transform the flattened tensor to the shape expected by the attention mechanism\n","        x = self.pre_attention_linear(x)  # Transform to [batch_size, self.seq_len * self.dim]\n","\n","        # Reshape the output to match the attention mechanism's expected input shape\n","        x = x.view(x.size(0), self.seq_len, self.dim)\n","\n","        # Apply top convolution, global average pooling, late dropout, and classifier\n","        x = self.top_conv(x)\n","        x = self.global_avg_pool(x).squeeze(-1)\n","        x.late_dropout_(p=0.8)  # In-place dropout\n","        x = self.classifier(x)\n","\n","        return x"],"metadata":{"id":"ROT6D9t-Js-y","executionInfo":{"status":"ok","timestamp":1702340725176,"user_tz":420,"elapsed":3,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import KFold\n","from torch.utils.data import Subset, DataLoader\n","from torch.cuda.amp import GradScaler, autocast\n","import torch\n","import os\n","\n","# Function to create DataLoader\n","def get_data_loader(dataset, batch_size=4, shuffle=True, num_workers=4, pin_memory=True):\n","    return DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        num_workers=num_workers,\n","        pin_memory=pin_memory,\n","        collate_fn=collate_fn\n","    )\n","\n","# Function to get DataLoaders for training and validation\n","def get_dataloaders(full_dataset, train_idx, valid_idx, batch_size):\n","    train_subset = Subset(full_dataset, train_idx)\n","    valid_subset = Subset(full_dataset, valid_idx)\n","\n","    train_loader = get_data_loader(train_subset, batch_size=batch_size, shuffle=True)\n","    valid_loader = get_data_loader(valid_subset, batch_size=batch_size, shuffle=False)\n","\n","    return train_loader, valid_loader\n","\n","# Function to train for one epoch\n","def train_one_epoch(model, dataloader, criterion, optimizer):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, labels in dataloader:\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","        # Clear GPU cache\n","        if torch.cuda.is_available():\n","            torch.cuda.empty_cache()\n","\n","    return running_loss / len(dataloader)\n","\n","# Function to validate the model\n","def validate(model, dataloader, criterion):\n","    model.eval()\n","    running_loss = 0.0\n","    with torch.no_grad():\n","        for inputs, labels in dataloader:\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            running_loss += loss.item()\n","\n","            # Clear GPU cache\n","            if torch.cuda.is_available():\n","                torch.cuda.empty_cache()\n","\n","    return running_loss / len(dataloader)\n","\n","# Training loop with checkpointing and early stopping\n","def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience, fold):\n","    scaler = GradScaler()\n","    best_val_loss = float('inf')\n","    early_stopping_counter = 0\n","\n","    for epoch in range(num_epochs):\n","        # Training phase\n","        model.train()\n","        running_loss = 0.0\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","\n","            # Mixed precision training\n","            with autocast():\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","\n","            # Backpropagation with scaled loss\n","            scaler.scale(loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","\n","            running_loss += loss.item()\n","\n","            # Clear GPU cache if needed\n","            if torch.cuda.is_available():\n","                torch.cuda.empty_cache()\n","\n","        avg_train_loss = running_loss / len(train_loader)\n","\n","        # Validation phase\n","        val_loss = validate(model, val_loader, criterion)\n","\n","        print(f'Epoch {epoch}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n","\n","        # Checkpointing\n","        current_checkpoint_path = os.path.join(CHECKPOINT_DIR, f'checkpoint_fold_{fold}_epoch_{epoch}.pth')\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_checkpoint_path = os.path.join(CHECKPOINT_DIR, f'best_model_checkpoint_fold_{fold}.pth')\n","            torch.save(model.state_dict(), best_checkpoint_path)\n","            early_stopping_counter = 0\n","        else:\n","            early_stopping_counter += 1\n","\n","        if early_stopping_counter >= patience:\n","            print(\"Early stopping triggered\")\n","            break\n","\n","    # Load the best model state for this fold\n","    load_best_checkpoint(model, fold)"],"metadata":{"id":"PD5jFEyUBsvJ","executionInfo":{"status":"ok","timestamp":1702340725176,"user_tz":420,"elapsed":3,"user":{"displayName":"Brian Sam-Bodden","userId":"17727095235669976107"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","from torch.utils.data import DataLoader, Subset\n","from sklearn.model_selection import KFold\n","\n","CHECKPOINT_DIR = '/content/drive/MyDrive/Colab Notebooks/DGMD E-14 Project/Datasets/asl-signs/CheckPoints/'\n","\n","# Function to save a checkpoint\n","def save_checkpoint(model, optimizer, epoch, fold, path=CHECKPOINT_DIR):\n","    checkpoint_filename = f'checkpoint_fold_{fold}_epoch_{epoch}.pth'\n","    checkpoint_path = os.path.join(path, checkpoint_filename)\n","    torch.save({\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict()\n","    }, checkpoint_path)\n","\n","# Function to load the best checkpoint for a given fold\n","def load_best_checkpoint(model, fold, path=CHECKPOINT_DIR):\n","    checkpoint_filename = f'best_model_checkpoint_fold_{fold}.pth'\n","    checkpoint_path = os.path.join(path, checkpoint_filename)\n","    if os.path.isfile(checkpoint_path):\n","        checkpoint = torch.load(checkpoint_path)\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        print(f\"Loaded best checkpoint for fold {fold} from {checkpoint_path}\")\n","    else:\n","        print(f\"No best checkpoint found for fold {fold} at {checkpoint_path}\")\n","\n","\n","# Configuration\n","n_splits = 5\n","seed = 42\n","batch_size = 4  # Adjust as needed\n","\n","# KFold split\n","kfold = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n","\n","# Path to save the checkpoint\n","checkpoint_path = \"/content/drive/MyDrive/Colab Notebooks/DGMD E-14 Project/ModelCheckpoints/ISLR_First_Place_Check_Point.pth\"\n","\n","# KFold training loop\n","for fold, (train_idx, valid_idx) in enumerate(kfold.split(range(len(dataset)))):\n","    print(f\"Training on fold {fold+1}\")\n","\n","    train_loader, val_loader = get_dataloaders(dataset, train_idx, valid_idx, batch_size)\n","\n","    # Initialize model, criterion, optimizer for each fold\n","    model = ASLClassifierModel(num_classes=NUMBER_OF_CLASSES, channels=384, dim=192)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","    # Training loop for the current fold\n","    train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, patience=5, fold=fold)\n","\n","    # Load best checkpoint after training is done\n","    load_best_checkpoint(model, fold)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zMFLykAZ1iNM","outputId":"0557601e-dbd8-467b-942e-0ed6445d7b61"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training on fold 1\n"]}]}]}